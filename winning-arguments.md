# Winning Arguments

**A system of meta-philosophy**

[Audio version available on YouTube.](https://www.youtube.com/playlist?list=PLMG5V0T2bhudVl601quvjHk2lga2L7F5l)

Under construction. Pardon the dust!

## Table of Contents

* [Acknowledgements](#acknowledgements)
* [How to use this text](#how-to-use-this-text)
* [Introduction: arguments and debates](#introduction-arguments-and-debates)
* [Games](#games)
* [Rules and laws](#rules-and-laws)
* [Winning](#winning)
* [Winning debates](#winning-debates)
* [Winning arguments](#winning-arguments-1)
* [Language](#language)
* [Statements](#statements)
* [Speech acts](#speech-acts)
* [Truth](#truth)
* [Epistemology](#epistemology)
* [Meaning](#meaning)
* [Rationality](#rationality)
* [Reasons](#reasons)
* [Logic](#logic)
   * [Set theory](#set-theory)
   * [Model theory](#model-theory)
   * [Vagueness](#vagueness)
   * [Intuitionism](#intuitionism)
   * [Paradoxes](#paradoxes)
   * [Conclusion](#conclusion)
* [Fallacies](#fallacies)
* [Biases](#biases)
* [Uncertainty](#uncertainty)
* [Beliefs](#beliefs)
* [Persuasion](#persuasion)
* [Math](#math)
* [Paradoxes](#paradoxes)
* [The Law of One](#the-law-of-one)
* [Faith](#faith)

## Acknowledgements

Morgan Thomas is the author. I am indebted to everybody whose work I have drawn upon here, who has done philosophy with me, whose philosophy I have read, or whose example has inspired me philosophically. They are too many to name, but they include (in no particular order) my parents Amy and Spencer Thomas; my sibling Esty; Lao Tzu; [JR](http://www.crazywisdomjournal.com/featuredstories/2014/8/22/the-crazy-wisdom-interview-with-jim-robert-of-pioneer-high-school); my friends Arian, Alexis, Thoi, Chloe, Becca, Graeme, Daniel S., Eric A., Khayree Billingslea, Isaac S.; Eliezer Yudkowsky; [Carla L. Rueckert, Don Elkins, and James McCarty](http://www.llresearch.org/aboutus.aspx); Ra; Aleister Crowley; various great Eastern mystics; the members of the Zen Buddhist Temples in Ann Arbor, MI and Tempe, AZ around 2010--2013; Crispin Wright; Graham Priest; Jc Beall; Michael Dummett; Nathan Kellen; Andrew Parisi; Hanna Gunn; Madiha Hamdi; Junyeol Kim; Teresa Allen; David Baldwin; Emma Björngard; Rasa Davidaviciute; Joel Hamkins; Alycia LaGuardia-LoBianco; Joseph Lurie; Colin McCollough-Benner; Tom Meagher; Dana Francisco Miranda; Jordan Ochs; David Pruitt; Nate Sheff; Andrew Tedder; Daniel Silvermint; Donald Baxter; Suzy Killmister; Mitch Green; Hallie Liberto; Bill Lycan; Michael Lynch; Edmund Husserl; Dave Ripley; Marcus Rossberg; Susan Schneider; Stewart Shapiro; Daniel Silvermint; Keith Simmons; Samuel Wheeler; Ruth Millikan; Immanuel Kant; Jean-Paul Sartre; George Soros; Jordan Peterson; Paul Grice; Martin Heidegger; Ludwig Wittgenstein; Gottlob Frege; Bertrand Russell; Kurt Gödel; Damir Dzhafarov; Reed Solomon; Paul Grice; Paul Horwich; Alfred Tarski; Kurt Gödel; Douglas Hofstadter; Alan Turing; Joseph P. Foy; Michel Foucault; Alvin Plantinga; Clare Levijoki; Manolo Lago; Edmund Gettier; Grace Paterson; Gregg J.; Bethany B.; Jeremy W.; Kate S.; Luis Anderson; Richard Newton; Socrates; Plato; Aristotle; Stefan Molyneux; Sargon of Akkad; Tim Pool; Hal Kierstead; Brad Armendt; John Devlin; [Philosophical Overdose podcast](https://www.youtube.com/user/soultorment27); [The Partially Examined Life podcast](https://www.youtube.com/user/shinobirastafari); and many others, too many to name.

## How to use this text

You can use this text as a practical guide to winning arguments, to reasoning about any subject, to seeking the truth about any subject, and to misleading people about any subject.

The text is a philosophical study of language, reasoning, argumentation, and related topics, with the practical aim stated above. The same theories that explain how to win arguments on any subject also explain how to reason about any subject, how to seek the truth about any subject, and how to mislead people about any subject. The same knowledge can be used for any of these purposes, and it's up to the person who has it to use it responsibly.

The author advises the reader against the use of any techniques to deliberately mislead people, as the author believes misleading people almost invariably leads, in the long haul, to consequences more deplorable than what would have resulted if one told the truth.

The text contains exercises (TODO: not true yet), which the author feels will be likely to further the reader's learning, and which the reader is encouraged to do when they feel they will further their learning. The greatest exercise, of course, is life, where you can fruitfully exercise the skills of reasoning, critical thinking, and argumentation you can learn in this text in almost any context of life.

This text is meant to be read from start to finish without skipping around. It is cumulative; each piece builds upon preceding pieces. In general you shouldn't expect to fully understand a part of the text unless you understand the text up to that point. 

For those who have not read much philosophy before, I apologize for the abstract, hair-splitting, and sometimes exhaustingly detailed and compact nature of the text, which will probably be a challenge for some people new to philosophy. I would like to offer some words of advice on how to read the text.

* Start out by reading one sentence at a time. Make sure you understand what each sentence is saying, before moving on to the next sentence. You can relax this rule if you're comfortable that you're following the text.
* If you understand what's being said but not why it's being said, you can just plow forward and imagine it will eventually become clear. I considered it too complex and distracting to try to explain, before each thing that's said, why it's being said. Therefore I ask for the reader's trust that they are being led on a purposeful path towards my understanding of winning arguments. If we take a shortcut through a strange, dark tunnel where the reader can only see two inches in front of their face, the reader is asked to take courage and keep their attention on those two inches.
* If there's a sentence or a section that's giving you too much difficulty, you can skip it.
* If you get totally lost and stop being able to follow what's being said, you can go back to the last point in the text up to which you're comfortable you've understood things, and re-read from there.
* If you come across a word you don't know, whose meaning you can't infer from context, then you should look up a definition for the word.
* If you come across a word whose meaning you don't understand in the particular context it's used, then look for a standard dictionary definition of the word and see if that fits.

To those who are new to philosophy, I recommend patience with the speed of one's understanding if it proves to try one's patience. Philosophical thoughts are usually developed by going over the same problems again and again over the course of years, and understanding philosophical writing often requires reading it multiple times and thinking about it over time.

In this text we will engage in an activity we call "conceptual analysis." Conceptual analysis is analysis of the meaning of words or concepts with the aim of clarifying their meaning and understanding it more deeply. In general, and in this text, conceptual analysis falls somewhere between the two poles of purely *descriptive* analysis, describing how various people have used a word or concept, and purely *prescriptive* analysis, defining a meaning for the term which the author uses and argues that readers should use.

Descriptive conceptual analysis is useful for uncovering the many layers of meaning that are often present in everyday terms. Prescriptive conceptual analysis, used judiciously, is useful for establishing clear communication and laying linguistic foundations that can be built on. A gentle and careful blending of descriptive and prescriptive conceptual analysis provides, in my experience, a powerful and versatile ingredient for philosophical argumentation. This text is or aims to be, among other things, a study in the method of conceptual analysis, and a large proportion of the text is conceptual analysis.

## Introduction: arguments and debates

Winning arguments is what I aim to show how to do in this text. I will show how this can be done by constructing winning arguments. As just illustrated, the phrase "winning arguments" has at least two important senses.

First, "winning arguments" can be interpreted to refer to an activity, the activity of winning arguments, where "argument" here is used in the sense of "debate."

Second, "winning arguments" can be interpreted as a noun phrase, where "argument" is used in the sense of "a series of statements designed to provide reason to believe some conclusion(s)." In this interpretation of the phrase, "winning" is an adjective, presumably meaning something like "persuasive."

A great deal of importance has been introduced in the preceding three paragraphs, so let's unpack the ideas further.

The first item of importance is the distinction between two senses of the word "argument."

Arguments in the first sense of the word are debates. By "debates," I mean exchanges of communication where participants discuss with each other the merits and demerits of some claims that they are mutually interested in and believe themselves to disagree about or are uncertain about.

Arguments in the second sense of the term are series of statements designed to provide reason to believe some conclusion(s). Henceforth, in this text I will consistently use the term "argument" to refer to arguments in this second sense, and I will use the term "debate" in order to retire the first sense of the term "argument."

Here is an example of an argument:

**(Question Science)** You shouldn't uncritically accept the conclusions of all scientific studies you come across. [Ioannidis (2005)](http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124) is a scientific study which claims that most published research findings are false. If you should uncritically accept the conclusions of all scientific studies you come across, then you should accept Ioannidis (2005). But if you uncritically accept Ioannidis (2005), then you accept that most scientific studies draw false conclusions. That means you shouldn't uncritically accept the conclusions of all scientific studies you come across, because if you do, you can expect to be adopting a lot of false beliefs.

Let's analyze this argument, which I will refer to as Question Science. Question Science seeks to prove that you should not uncritically accept the conclusions of all scientific studies you come across. It proceeds by assuming for the sake of argument the opposite of what it seeks to prove, and deriving an absurd result. It assumes for the sake of argument that we should uncritically accept the conclusions of all scientific studies we come across, and ends by deriving the absurd conclusion that you should do something which will lead to you adopting a lot of false beliefs.

Let's step through the argument. We begin with the assumption for the sake of argument that you should uncritically accept all scientific studies you come across. The next move is based on the observation that if somebody thinks that way, you can make them believe anything that's said in a published scientific paper. The next move is to point out a paper, Ioannidis (2005), which will make us believe, under our sake-of-argument assumption, that most scientific studies conclude things that are false. This tells us that the thing we assumed we should do will lead us to hold many false beliefs. With the further assumption that we should not uncritically accept information that's likely to be false, we can refute the statement that we started out by assuming. In other words we can conclude that we should not uncritically accept the conclusions of all scientific studies we come across.

Question Science is an example of a *reductio ad absurdum* argument. A *reductio ad absurdum* argument, in a strict sense, is one that proceeds by assuming the negation of its conclusion, and deriving a logical contradiction thereby. In a looser sense, a *reductio ad absurdum* argument can derive, instead of a logical contradiction, an absurd consequence that will probably be unacceptable to the audience of the argument, leading the audience to reject the assumption that leads to that consequence.

Question Science is a *reductio ad absurdum* argument in the looser sense. In Question Science, we first assume the view we wish to refute, and with a modest amount of information and background assumptions, we get to the conclusion that our assumption will likely lead us to accept many false things as true, something which makes most people want to back out of the assumption.

Question Science is an example of a winning argument. I expect the reader agrees: it is hard to disagree with the reasoning, or the conclusion. I haven't encountered anybody who finds this argument unpersuasive. One reason the argument is so strong is the simplicity and effectiveness of its structure, which is based on the *reductio ad absurdum* idea that is the basis of many good arguments stretching back to ancient times.

Let's round out this discussion with an example of an argument which is a *reductio ad absurdum* in the strict sense: that is, an argument which proceeds by assuming the opposite of that which it seeks to prove, and deriving a logical contradiction from that assumption. As our example, we will do an informal version of the classic proof, due to Euclid, that there are infinitely many prime numbers.

**(Infinitude of Primes)** Recall that a prime number is a whole number greater than one which is divisible only by itself and one. Suppose, towards reaching a contradiction, that there are finitely many prime numbers. Let *x* be the number resulting from multiplying together all the prime numbers and adding one. Since *x* is evidently larger than all prime numbers, *x* is not prime. The fact that *x* is not prime means, by definition, that *x* is divisible by at least one prime number. Pick a prime number that *x* is divisible by, and call it *p*. We know that the remainder of dividing *x* by *p* is 0, since that's what it means for *x* to be divisible by *p*. However, we also know that the remainder of dividing *x* by *p* is 1, because of how we produced *x*. This is a contradiction, demonstrating that our original assumption was false, or in other words that there are infinitely many prime numbers.

Why is it true (as we said at the second to last sentence of the argument) that the remainder of dividing *x* by *p* is 1? We produced *x* by multiplying together all the (finitely many) prime numbers and adding one. Therefore *p* goes into (*x*-1) evenly, i.e. the remainder of the division (*x*-1)/*p* is zero, and therefore the remainder of the division *x*/*p* is 1.

Infinitude of Primes is another example of a winning argument. Mathematical theorems, like the theorem that there are infinitely many prime numbers, are extraordinarily uncontroversial. This can be explained at least in part by the statement that math is a game that has simple and essentially agreed upon rules. By describing math as a game, I do not mean to imply that math is not an activity of seeking truth. I don't mean to take any position in this text on whether or not math is an activity of seeking the truth.

Nonetheless, correct mathematical proofs are some of the best examples we have of winning arguments, due to their seemingly very reliable persuasive power.

We've dug a bit into the concept of arguments, in the sense of series of statements designed to provide reason to believe some conclusion(s). Let's now return to the distinction we made between debates and arguments.

Debates almost always involve the participants making arguments. Can arguments occur anywhere except in the context of debates? 

One place where you can find arguments is in books, especially non-fiction books. One can argue that an argument in a book is not in the context of any particular debate, because a book is an inanimate object that can exist in many copies in different places and times. Books are objects which are incapable of participating reciprocally in debates. You can, in a certain sense, let a book make arguments to you by reading it. You can also respond to the book's arguments, mentally, in your journal, to other people, to the author, etc. But the book cannot respond to your responses to its arguments. The author might be able to, if they're still alive, but that's another issue. This is what I mean when I say that books cannot participate reciprocally in debates. For these and many other reasons, you might say that arguments in books do not necessarily occur in the context of any debate, or perhaps that they never do.

Nonetheless, you might argue that arguments in books usually do occur in the context of some debate or another. This idea is palatable if we accept, as examples of debates, ongoing public debates. An ongoing public debate on a question, roughly, is a situation where lots of people make arguments on and have debates on the question, across a significant slice of space and time, probably across a region or the world for years. If you accept ongoing public debates as examples of debates, then you will probably accept that most arguments in most books occur in the context of some debate.

A possible exception to this generalization is the (hypothetical) case where somebody writes a book which makes arguments on a topic nobody else has discussed, and then nobody reads the book. In this case, you could for the sake of uniformity say that the book occurs in the context of a debate involving one person, the author. However, this might be odd to say because there seems to be something inherently interactive, social, multi-personal about debates. I think most people are going to be inclined to say that the arguments in this book do not occur in the context of any debate.

The possibility of such an exception to the generalization that arguments in books occur in the context of some debate makes more sense when we consider the plausibility of the idea that an argument in a book can occur in the context of multiple debates. For example, a single book about climate change can be involved in the overall public debate on climate change, while simultaneously being involved in many particular interpersonal debates about climate change, each occurring between a specific group of people together in one location in physical space or cyberspace. It seems reasonable to talk in this way and to say that the book is involved in multiple debates on one subject. If the number of debates a book is involved in can vary in number, then it seems only natural that the number could in principle equal zero, though that probably isn't common in reality.

So far we've reached three generalizations about the relationship between arguments and debates:

* Arguments usually occur in the context of debates.
* Debates usually contain arguments.
* Arguments can, in principle, occur outside the context of debates.

In my estimation, all of these points are important. That arguments usually occur in the context of debates, and debates usually contain arguments, tells us that arguments and debates are importantly related to each other and we need to understand them together. That arguments can, in principle, occur outside the context of debates tells us that arguments can be understood independently from debates (though it doesn't tell us whether that's the most useful way to understand them). It's probably not the case, on the other hand, that debates can be understood independently from arguments, since skilled debates are almost always in essence exchanges of arguments.

We will take both of the available approaches to understanding arguments. One can understand arguments either within the context of debates, or independently from debates. We will do both. Studying arguments outside the context of debates helps us focus in on arguments' intrinsic properties, and thereby understand arguments themselves much better. However, the perspective is unbalanced if we don't also study arguments in the practical context of debates as they occur in reality.

Hopefully that discussion of the relationships between debates and arguments has clarified for the reader the distinction between debates and arguments, as well as helping to clarify each concept individually.

Let's return to our original task of clarifying the meaning of the first three paragraphs of this Introduction. So far we've clarified the meanings of two critical words, "debate" and "argument." Yet many words of nebulous, dubious, or unclear meaning remain. First and foremost, "winning," but also (with the evident nebulousness, dubiousness or unclarity depending in part on where and how deep you've traveled into philosophy) "true," "reason," "believe," "persuasive," and "statement." Let's analyze the meanings of these words, starting with "winning."

What is winning? For starters, winning is most often something that happens to people in the context of games (chess, baseball, etc.).

Question: is winning something that ever happens to people *except* in the context of some game?

To make a start on what's going to be a highly roundabout answer to this question, let's consider the phrase "winning at life." Winning at life is a form of winning, at least on a surface inspection of the phrase. One can also hypothesize that "winning at life" is in some usages a phrase with its own particular meaning that doesn't decompose as winning in the context of life, in the same way that a "close shave" doesn't always refer to a shave that was close.

If winning at life is winning in the context of life, and winning only ever happens in the context of some game, then it follows that life is a game. So, then, is life a game? 

"Life is a game" is a truism, a statement that people are liable to spout as a form of shallow wisdom, perhaps without much reflection on how true it really is. How true is the truism? To answer that, we need to ask, what does the truism mean? To answer that, we need to ask, what do the words in the truism "life is a game" mean? "Life" seems to refer to human life, and probably it's clear enough what we're referring to there. What about "game?"

## Games

What is a game? Let's try to make a start on this question by identifying some things that are games, and also some things that are not games, which fail to be games in ways that are interesting and tell us something about games. A lot of the judgment as to which is which is going to be subjective, but the judgments on particular cases aren't in the end the important thing, but rather what's important is the general picture of games that emerges from the analysis.

Popular and typical examples of games include soccer, chess, poker, dice, solitaire, World of Warcraft, and slot machines. Childrens' games include [peekaboo](https://en.wikipedia.org/wiki/Peekaboo), [tag](https://en.wikipedia.org/wiki/Tag_(game)), [ring around the rosie](https://en.wikipedia.org/wiki/Ring_a_Ring_o'_Roses), and two people sitting on the ground and rolling a ball back and forth between each other. Unusual examples of games include [war games](https://en.wikipedia.org/wiki/Military_exercise), the [Olympic Games](https://en.wikipedia.org/wiki/Olympic_Games), [Roman gladiatorial combat](https://en.wikipedia.org/wiki/Gladiator), TV game or reality shows such as [The Celebrity Apprentice](https://en.wikipedia.org/wiki/The_Celebrity_Apprentice), [Chopped](https://en.wikipedia.org/wiki/Chopped_(TV_series)#Format), and [Lost](https://en.wikipedia.org/wiki/Lost_(game_show)), and abstract theoretical games like the [prisoner's dilemma](https://en.wikipedia.org/wiki/Prisoner's_dilemma) which are studied in [game theory](https://en.wikipedia.org/wiki/Game_theory). There's a whole category of more analogical usages of "game," such as the game of life, the game of capitalism, the game of courtship and romantic love, and the game of war.

By inspecting this list we can immediately state some generalizations that are not true about all games. Not all games are frivolous in purpose; some, such as war games, are serious in purpose. Even games that are superficially frivolous in purpose can be games where you're competing for your own life, as shown by the example of Roman gladiatorial combat. Non-lethal games can involve considerable stakes of money and glory, as exemplified in professional poker and in the Olympic Games.

Roman gladiatorial combat was a game played to entertain people. In that sense it was frivolous in purpose. But, it was gruesome, violent, lethal, and expensive, so presumably there was some significant reason people went to the trouble and difficulty of putting on the games.

One hypothesis I've heard is the following. Rome contained a large class of poor people who lived on welfare and therefore were not occupied with useful work, and it was found to be necessary to entertain them to prevent them from engaging in crime and disruption. If entertaining poor unemployed people to prevent them from causing disruption was the primary purpose of the games for some of the people who funded them, then certainly there was nothing frivolous about the games for those funders. For the people watching the games, the main purpose seems to have been entertainment.

Another hypothesis one can put forth is that the gladiatorial games embodied the Roman values of violence, domination, and heroism, and like most civilizations the Romans felt it was a worthwhile effort to put on events which could stand as monuments to their values. If we think of this as the primary purpose of the games, then one's inclined to say that the purpose is not frivolous, either for the funders or for the audience members.

If you think of the funders and the watchers of the Roman gladiatorial games as different sets of players in the game (expanding the conceptual boundaries of the game beyond the combat ring itself), one can venture that for the funders, the purpose of the games was often not frivolous, whereas for the watchers, the purpose of the games was most often the frivolous purpose of entertainment.

I am not asserting that any of these interpretations of the purposes of the Roman gladiatorial games are correct. However, each of these interpretations could (from a perspective of ignorance of relevant historical facts) be correct, and each one is a conceptually possible example of the purposes of a game.

The example of the Roman games illustrates the principle that a game can have multiple purposes, some explicit and some implicit, and different players in a game can have different purposes. The explicit purpose(s) of a game, being explicit, are typically apparent to all. In contrast, probably in most cases it's unclear to all or almost all observers of a game what all of the implicit purposes of the game are. This is because implicit purposes are, by definition, not stated, and since we can't read each other's thoughts we have no reliable way of knowing each other's unstated motivations.

Furthermore, people can be unaware of their own motivations. It is widely agreed, particularly among experts in psychology, that most of people's mental activity is unconscious, i.e. not witnessed or observed by their conscious minds. This view holds that among people's motivations for things they do, there are their unconscious motivations. Some argue that most or all things we do are influenced at least in part, or even mostly, by unconscious motivations.

This is not a psychology text, and we won't try to answer the question of in what ways and to what extent people's behavior is influenced by unconscious motivations. However, if the reader agrees that unconscious motivations exist, then we can say that a game may have an implicit purpose of which no observers have any awareness.

Given our limited access to our own minds, and our near lack of access to others' minds, it's clear why probably in most cases it's unclear to all or almost all observers of a game what all of the implicit purposes of the game are.

Let's now switch gears and poke at one of the more dubious examples I gave of a game, to see what we can learn about games in the process.

TODO: Explain prisoner's dilemma and address whether it's a game

Let's now switch gears again. Let's give some examples of things that are not games, which fail to be games in interesting ways that might tell us something about games.

Here's an example of a non-game: making toast. If you are a native English speaker: does it sound natural to describe making toast as a game? For me, the answer is that it does not sound natural. Making toast is not a game. It's some other kind of activity. Food preparation.

Here are some more examples of activities that are not usually called games: fixing your toaster; driving to work; working; sleeping; having sex; thinking; conversation; serving on a jury in a court of law; voting. People don't usually call any of these things games, and yet many of them have certain similarities to games. Serving on a jury and voting both have game-like qualities, being group activities governed by socially agreed upon systems of rules within which people make decisions in order to fulfill the objective of the activity (making a fair decision on the case being presented, or electing representatives and resolving plebiscites, respectively). Conversation, and language use in general, have many game-like qualities, being subject to many elaborate, largely unspoken rules. Game-like activities can be found strewn throughout life if you look for them.

One social norm that seems to apply across societies in general is that even if you are not playing a game that others are playing, you shouldn't violate the rules of their game. For example, if two people are playing chess, it would be against social norms probably anywhere in the world to move the pieces on their board, as a non-player in the game. One can say that games like chess are embedded within a larger meta-game or gamelike activity, the game of society, which has meta-rules like "don't break the rules of other people's games."

We have collected some examples and non-examples of games, and we have a substantial middle ground of things that arguably have both gamelike and non-gamelike qualities, including voting, serving on a jury, and, usually, working. Let's use this data to evaluate some possible generalizations about games.

**Do games always have rules?** Can we think of an example of a game that has no rules? Conversation, one might argue, is a game with no rules. In some sense this is true; people are free to say whatever they want. However, conversation does have many unstated norms that are usually not violated.

Paul Grice argued that participants in conversations usually follow certain [maxims](https://www.sas.upenn.edu/~haroldfs/dravling/grice.html). They are (to quote):

1. **The maxim of quantity**, where one tries to be as informative as one possibly can, and gives as much information as is needed, and no more.
2. **The maxim of quality**, where one tries to be truthful, and does not give information that is false or that is not supported by evidence.
3. **The maxim of relation**, where one tries to be relevant, and says things that are pertinent to the discussion.
4. **The maxim of manner**, when one tries to be as clear, as brief, and as orderly as one can in what one says, and where one avoids obscurity and ambiguity.

Obviously these rules are violated by speakers on a regular basis, but other speakers who perceive the violations of these rules tend to perceive them as social violations or imperfections in some sense. If one agrees with that, then one agrees in this sense that Grice's maxims are rules that speakers in conversations follow.

Every conversation will follow rules of grammar and prononunciation particular to the language(s) being used in the conversation. Further examples of kinds of rules conversations usually follow are rules of etiquette, rules about what things it is and is not appropriate to talk about in a particular context, rules about what words are unacceptable to say, and so forth. Therefore we can say, as a generalization, that conversations follow rules. Therefore we can say the game of conversation is not a counterexample to the claim that games always have rules.

Other possible counterexamples, so far as I'm aware, can be disposed of by thinking along similar lines. It looks to me like any appearance of a gamelike activity lacking rules is due to failure to look sufficiently broadly for rules governing the activity. In interpreting this statement, it should be understood that rules need not be inviolable to be rules.

When speaking English, people usually follow rules of grammar. Sometimes people violate the rules of grammar they normally follow. Different English speakers sometimes follow different rules of grammar. For example, African American vernacular English (AAVE) has different rules of grammar than standard American English. "He be comin" is grammatically valid AAVE, but not grammatically valid standard American English. People sometimes follow different linguistic rules in different contexts, as for example when a black person speaks AAVE with their family and standard American English at work. Following different linguistic rules in different contexts is called "code-switching."

Everybody has their own unique linguistic habits. Everybody pronounces words slightly differently, understands the meanings of words slightly differently, and so forth. Mostly these differences are too tiny to identify, but the fact that everybody differs slightly in how they produce and process language is illustrated for example by the fact that it is possible to identify people's voices and people's handwriting. Therefore one can say there are as many **idiolects** (variants specific to an individual) of a natural language as there are speakers of that language.

The foregoing illustrates that there is a great deal of complexity and inconsistency when it comes to rules of English grammar. We can still say as a generalization that English speakers, and speakers of every other language, follow the rules of grammar particular to their dialects most of the time when speaking. This illustrates the nuance that can apply when discussing the rules of gamelike activities. Rules of gamelike activities can be very flexible, granular, fluid, and invisible.

So far as our discussion has led us to be able to see, the generalization is true that **games and gamelike activities follow rules**. If there are exceptions to this generalization, they seem to be obscure.

The nature of rules is well worth exploring in more depth. However, for the moment I will delay that discussion. Let us now explore other possible generalizations about games.

**Are games always competitive?** The answer appears to be **no**. For some childrens' games, such as [peekaboo](https://en.wikipedia.org/wiki/Peekaboo) and [ring around the rosie](https://en.wikipedia.org/wiki/Ring_a_Ring_o'_Roses), it's difficult to see any sense in which they are competitive. However, most games are competitive in some sense. Even solitary games usually involve self-competition, trying to learn and do better.

**Are games always cooperative?** Any game which involves multiple people is cooperative in the sense that playing a game requires the players to agree to play the game (as opposed to doing anything else), to put effort towards its execution, and to abide by the rules of the game. Therefore we can conclude that multi-player games are always cooperative on some level, and usually competitive on another level.

Jordan Peterson made this point in one of his lectures for his course Maps of Meaning. He gave an example much like what follows. In a game of hockey, the members of each team cooperate with each other in their competition with the other team. In addition, the members of each team compete with their team members for position in the meritocracy of the team. Finally, all the players cooperate with each other in their agreement to play the game and in their execution of gameplay according to the rules. There are a number of different levels of cooperation and competition in this case.

In solitary games the obvious comment would be that there is no cooperation because cooperation requires multiple people. This is true in an obvious sense. However, it's also false in a less obvious sense, in that playing a game requires internal coordination on the part of a person. In order to play a game, a person has to coordinate many different parts of their mind and body, orchestrating their activities in such a way as to accomplish a single task. In the same sense, every activity whatsoever done by a person involves self-cooperation. One could fairly object that the concept of self-cooperation is an undue extension of the term "cooperation," and I will not press the point.

What conclusion are we led to about whether games are always cooperative? In this case we're led to **no single conclusion**. What we've said indicates that if self-cooperation is a form of cooperation, then games are always cooperative (as is every human activity), and on the other hand, if self-cooperation is not a form of cooperation, then games are not always cooperative. In addition, what we've said indicates that multi-player games are always cooperative.

What can we say overall about the meaning of the word "game?" On this, let's quote from Wittgenstein, in the Philosophical Investigations, part I, remarks 66-67:

"66. Consider for example the proceedings that we call "games". I mean board-games, card-games, ball-games, Olympic games, and so on. What is common to them all?—Don't say: "There must be something common, or they would not be called 'games' "—but look and see whether there is anything common to all.—For if you look at them you will not see something that is common to all, but similarities, relationships, and a whole series of them at that. To repeat: don't think, but look!—Look for example at board-games, with their multifarious relationships. Now pass to card-games; here you find many correspondences with the first group, but many common features drop out, and others appear. When we pass next to ballgames, much that is common is retained, but much is lost.—Are they all 'amusing'? Compare chess with noughts and crosses. Or is there always winning and losing, or competition between players? Think of patience. In ball games there is winning and losing; but when a child throws his ball at the wall and catches it again, this feature has disappeared. Look at the parts played by skill and luck; and at the difference between skill in chess and skill in tennis. Think now of games like ring-a-ring-a-roses; here is the element of amusement, but how many other characteristic features have disappeared! And we can go through the many, many other groups of games in the same way; can see how similarities crop up and disappear.

"And the result of this examination is: we see a complicated network of similarities overlapping and criss-crossing: sometimes overall similarities, sometimes similarities of detail."

"67. I can think of no better expression to characterize these similarities than "family resemblances"; for the various resemblances between members of a family: build, features, colour of eyes, gait, temperament, etc. etc. overlap and criss-cross in the same way.—And I shall say: 'games' form a family."




## Rules and laws

I have previously concluded that games always have rules. However, we have not discussed at any length what rules are. Understanding rules is quite central to understanding games, and to understanding the principles of winning arguments. To understand rules, it is also helpful to compare the meaning of the word "rule" with the meaning of the word "law." Since the terms are close in meaning, comparing and contrasting the meanings of the terms is informative about both. Therefore we will embark on a unified analysis of rules and laws.

There are many different laws and sets of laws of varying kinds, degrees, and scopes of importance and validity. Examples of laws (or at any rate things that people call laws) include: the [laws and regulations](https://en.wikipedia.org/wiki/Primary_and_secondary_legislation) of each [state](https://en.wikipedia.org/wiki/State_(polity)), [region](https://en.wikipedia.org/wiki/Administrative_division), [municipality](https://en.wikipedia.org/wiki/Municipality), and so forth; religious systems of law such as [sharia law](https://en.wikipedia.org/wiki/Sharia), the [Ten Commandments](https://en.wikipedia.org/wiki/Ten_Commandments) and [Talmudic law](https://en.wikipedia.org/wiki/Orthodox_Judaism); the laws of [physics](https://en.wikipedia.org/wiki/Physics), [chemistry](https://en.wikipedia.org/wiki/Chemistry), [biology](https://en.wikipedia.org/wiki/Biology), [statistics](https://en.wikipedia.org/wiki/Statistics), [logic](https://en.wikipedia.org/wiki/Logic), and [mathematics](https://en.wikipedia.org/wiki/Mathematics), and other fields of study which have produced laws; satirical laws like [Murphy's law](https://en.wikipedia.org/wiki/Murphy's_law) and [Godwin's law](https://en.wikipedia.org/wiki/Godwin's_law); sociological laws like [Duverger's law](https://en.wikipedia.org/wiki/Duverger's_law); mystical laws like the [law of attraction](https://en.wikipedia.org/wiki/Law_of_attraction_(New_Thought)) and the [Law of One](http://www.lawofone.info/synopsis.php); and so forth. 

[Examples of rules](https://en.wikipedia.org/wiki/Rule) (or at any rate things that people call rules) include, firstly, the rules of any game. Secondly, there are many rules (or things that people call rules) which are not literally rules of any game, such as: the [M'Naghten rules](https://en.wikipedia.org/wiki/M'Naghten_rules); the [right-hand rule](https://en.wikipedia.org/wiki/Right-hand_rule); the [five-second rule](https://en.wikipedia.org/wiki/Five-second_rule) and other [rules of thumb](https://en.wikipedia.org/wiki/Rule_of_thumb); [unspoken rules](https://en.wikipedia.org/wiki/Unspoken_rule); [rules of etiquette](https://en.wikipedia.org/wiki/Etiquette); rules of grammar; and rules of inference in logic (which we shall discuss at length later in the section titled Logic).

What kinds of things can follow rules? Clearly humans can follow rules. Systems of physical objects can follow rules, as for example all systems of medium-sized physical objects seem to follow the laws of Newtonian physics. Arguments can follow rules; for example, an argument can be logically valid, meaning it proceeds only according to logically correct rules of inference. Correct, fully detailed mathematical proofs are examples of logically valid arguments. In short, many things, human and non-human, can obey rules.

I am not aware of any counterexamples to the following generalization: all (things we ordinarily call) rules are defined and established by humans.

This generalization does not necessarily extend to laws. Many laws are clearly defined and established by humans, such as the laws of the legal systems of states, regions, and municipalities. Other laws, arguably, are not defined and established by humans:

* Though descriptions of physical theories such as Newtonian physics are products of humans, some would argue that Newtonian physics (or some hypothetical complete physical theory) is a description of laws of nature that exist objectively, independently of humans, regardless of whether we know about said laws. If laws of nature exist objectively, then they weren't established by humans.
* Many would argue that systems of religious law, such as sharia law or the Ten Commandments, were created by God and therefore not defined or established by humans.

In summary, some but arguably not all laws are defined and established by humans, whereas essentially all things people call rules are defined and established by humans, so far as the author knows.

There is some potential overlap between the categories of rules and laws. For example, the following statement seems fair: the laws of Newtonian physics are rules describing the motion of physical bodies. From that statement it follows that the laws of Newtonian physics are both rules and laws. If we accept that there are many (or in fact any) things which we can reasonably call both rules and laws, then it follows that there are no properties which all rules have and all laws lack, or vice versa.

Is every rule a law? Is every law a rule? "The laws of chess" and "the rules of the universe" are odd sounding phrases, probably, to most native English speakers. "The laws of chess" sounds odd, perhaps, because it suggests that the rules of chess have some quality of inevitability, whereas in fact people regularly play [variants](https://en.wikipedia.org/wiki/Chess960) [of](https://en.wikipedia.org/wiki/Three-dimensional_chess) [chess](https://en.wikipedia.org/wiki/List_of_chess_variants), cheat at chess, misunderstand the rules of chess, etc. "The rules of the universe" sounds odd, perhaps, because there's nothing we ordinarily call a rule that governs something as big and grand as the universe; only things we ordinarily call laws apply at this scale. These phrases, "the laws of chess" and "the rules of the universe," are still probably understandable in context, and they need not have the illogical connotations we have described for them, but they can refer simply to the rules of chess and the laws of the universe, respectively.

Our linguistic experiments suggest, firstly, that there is a delicate difference in meaning between "rule" and "law," and secondly, that there is a great deal of similarity in meaning between "rule" and "law."

The similarity in meaning between "rule" and "law" is so great that one hypothesis one might venture is that "rule" and "law" are essentially different words for the same concept, which we apply habitually to different kinds of things, without any deeper distinction existing. One way of describing the hypothesized vague, habitual distinction is to observe that laws are usually broad in their domain of applicability, whereas rules are usually narrow in their domain of applicability.

One objection to the view that "rule" and "law" are essentially the same concept is as follows. All things we habitually call rules seem to be made by humans. A qualifier to this is that rules can also be made by the creations of humans, as when computer programs generate rules for other programs to follow. Rules govern humans and the operations of the creations of humans. Laws, on the other hand, according to the objection, are not always created by humans, and sometimes they govern nature rather than humans or the creations of humans. If one accepts the foregoing, then one will acknowledge a substantive distinction between rules and laws.

For the sake of clear nomenclature, I will use the term "natural law" to refer to laws which are not created by humans. By definition, then, natural laws include any laws created by God, if God exists. I am not assuming that any natural laws exist, but we have a word for any that do exist (as well as any that don't exist).

The nature and extent of the difference in meaning between "rule" and "law" is, at the end of the day, a question of opinion. The subtleties of the distinction are interesting. What's perhaps more interesting, though, is looking more into the basic meanings of the terms "rule" and "law." 

It would be helpful towards that goal to establish more relations between "rule" and other terms we have studied. We have generalized that games and gamelike activities follow rules. Can we also generalize that rules occur in the context of games and gamelike activities?

Let's look back to the examples of rules we gave. Do all of them occur in the context of games or gamelike activities? Let's examine. The M'Naghten rules are rules for legal proceedings, which are a gamelike activity used to settle legal cases. The right-hand rule is a mini-game for remembering orientation conventions in 3D vector math. The five-second rule is a mini-game (lacking any factual basis) which we play to justify eating food off the floor. The [rule of 72](https://en.wikipedia.org/wiki/Rule_of_72) is a rule of thumb that one can employ in the gamelike activity of investment portfolio construction. One illustration that investment portfolio construction is a gamelike activity is that [a number of games](https://www.google.com/?q=stock+market+simulator) have been closely modeled after the activity of trading the stock market. Trading the stock market has, in turn, drawn comparison to gambling games such as poker.

Bearing in mind the flexibility of the concept "game or gamelike activity," it seems fair to generalize that rules occur in the context of games or gamelike activities. This generalization gives us further conceptual bearing for thinking about rules: we can always think about rules in the context of games or gamelike activities.

This generalization is violated by the statement "the laws of Newtonian physics are rules governing the motion of physical bodies," if we consider the laws of Newtonian physics to be objective features of nature rather than a human-constructed system for understanding nature. In other words, at least one of these four things is false:

1. Rules always occur in the context of games or gamelike activities.
2. The laws of Newtonian physics are objective features of nature rather than a human-constructed system for understanding nature.
3. The laws of Newtonian physics are rules.
4. Nature itself is not a gamelike activity.

If statements 1-3 are true, then we can conclude that nature itself is a gamelike activity, since if the laws of Newtonian physics are objective features of nature, then the context to which they apply is nature itself. If the laws of Newtonian physics are rules, then it follows that nature itself is a gamelike activity, in direct contradiction of statement 4. Hence, we are required by logic to reject at least one of statements 1-4.

All of these statements can be thought of as terminological rules. Statement 1 says that if we're willing to call something a rule, we should be willing to call its context a game or gamelike activity. Statement 3 says we should be willing to refer to the laws of Newtonian physics as rules. Statement 4 says we should not be willing to describe nature itself as a gamelike activity.

Statement 2 is a compound of a terminological rule and a statement about the world. We can think of it as a combination into one statement of the following two statements:

**Statement 2w:** There are objective laws of nature correlating to human-constructed descriptions of the laws of Newtonian physics.

**Statement 2t:** We should use the phrase "the laws of Newtonian physics" to refer to said objective laws of nature, as opposed to using the phrase "the laws of Newtonian physics" to refer to said human-constructed descriptions.

Statement 2w is a statement about the world, whereas statement 2t is a terminological rule.

For statement 2t to make sense, statement 2w must be true. If statement 2w is not true, then there are no objective laws of nature correlating to the laws of Newtonian physics, making "said objective laws of nature" in statement 2t unable to refer to anything.

One can accept statement 2w and reject statement 2t. If one neither accepts nor rejects statement 2w, if one maintains that one does not know whether or not statement 2w is true, one can still reject statement 2t.

My view is that I can't accept or reject statement 2w because I don't have the knowledge to say whether it's true or false. Why's that?

For starters, Newtonian physics is known to be correct only in restricted contexts and to a certain degree of approximation. Newtonian physics was formulated by inductive generalization from a certain body of phenomena, namely the motions of objects that can be seen with the naked eye. When it comes to very small objects, very large scale phenomena, very high speeds, very high energy levels, etc., Newtonian physics ceases to correctly predict our observations, and other theories, like quantum physics and general relativity, are required to get the right answers. Quantum physics and general relativity, in turn, have proven extremely difficult to unify into a single logically consistent theory explaining all observations. All of this suggests that Newtonian physics, as well as all existing theories, represent only our best current state of understanding. This casts doubt on the idea of assuming that the laws of Newtonian physics as described by physicists are direct correlates of objective laws of nature, rather than being human constructions that approximate the actual behavior of nature to a high degree of accuracy in a wide yet limited variety of cases.

Here is an additional reason to doubt statement 2w. As Kant has argued in the *Critique of Pure Reason*, we cannot have direct knowledge of the world outside ourselves. Our sense perceptions and experiences are constructions of our minds which are presumed to reflect an outside world through many layers of indirection, through processes such as light transmission, retinal stimulation, edge detection, object detection, associative cognition, etc. Our theories of the world are abstractions over our observations, i.e. generalizations over series of sense perceptions had by humans over time. Yet we have no direct observational knowledge that there was a past, that there is an external world, or that there are other minds; the only thing we observe directly is our own conscious experience in the present moment. Then can we, with intellectual fairness, assume that the outside world, a thing of which we have no direct experience, is made in the image of our models of the outside world?

Statement 2w is in doubt on multiple fronts, and probably many are not willing to assume it's true. Hence, we should not assume statement 2t makes sense. Hence, we should not follow it as a terminological rule. In other words, we should not understand the phrase "the laws of Newtonian physics" to refer to objective laws of nature correlating to the laws of Newtonian physics as described by humans.

Instead, in this text I will use the phrase "the laws of Newtonian physics" to refer to said human-constructed descriptions, or to the ideas those descriptions denote.

The same problem we have encountered with Newtonian physics occurs with other phrases that refer either to laws or nature or to human-constructed descriptions of supposed laws of nature. Examples of phrases presenting this problem include "the laws of general relativity," "the laws of quantum mechanics," "the laws of chemistry," "the laws of biology," etc. In all such cases, in this text we shall interpret the phrases as referring to human constructed descriptions of laws, or to the ideas expressed by those descriptions, and not to objective laws of nature. This is, as with the case of the laws of Newtonian physics, not an attempt to say that there aren't corresponding objective laws of nature (though many of the same difficulties with asserting positively the existence of such laws present themselves in other cases as well). It's simply a terminological choice driven by the desire not to take a position on what objective laws of nature exist.

This terminological choice collapses a substantive distinction in meaning we've seen between the terms "rule" and "law," namely that "law" can refer to laws of nature whereas "rule" seems to refer to the rules of humans. Under our sharpened terminology, the natural laws we know (the laws of physics, chemistry, etc.) are just a kind of rule. For example, the laws of Newtonian physics are the rules of the game of Newtonian physics. Newtonian physics is a gamelike activity that humans practice in order to build bridges, go to the moon, etc. Similarly for the rest of natural science.

Do any distinctions in meaning remain for us between "rule" and "law?" Let's consider the case of the laws of legal systems of nations, states, municipalities, etc., or "legal laws" as I shall say. Are legal laws rules? It seems natural to say so. By our principles, that means that legal laws are rules that apply in the context of some game or gamelike activity. The contexts that the laws of a legal system apply to are all situations in the jurisdiction of the legal system about which laws have been made in the given legal system.

In what ways are situations to which legal laws apply like games, and in what ways are they unlike games? Situations to which legal laws apply are always like games in the respect that in them one is required to follow rules. Since laws can be made about anything, situations about which there are legal laws can be arbitrarily non-gamelike. Let's ask, then, about the gamelike and non-gamelike characteristics of typical situations about which there are legal laws.

Let's start with violence. Most forms of violence are illegal in most jurisdictions. Violence is not a very gamelike thing. What laws against violence say is that one should never commit (covered forms of) violence. In other words, laws against violence require people to follow the rule of non-violence in most of their dealings. In what ways is non-violence like a game, and in what ways is it unlike a game?

Non-violence postulates certain rules, which vary depending on who you're asking. These rules usually amount to prohibitions on violating the bodies of other people, where violating the bodies of other people ranges from violations such as touching somebody without permission to violations such as mangling a person limb from limb. These rules serve the purpose of preventing people from being harmed and maintaining social cohesion by prohibiting interpersonal conflicts from escalating to the point of violence. Rules of non-violence are based on the observation, long validated by history, that violence almost always leads to consequences more deplorable than the problems it was trying to solve.

The following suggests itself: non-violence is part of the rules we follow in the game of society. On this way of thinking, society is a game humans play whose purposes include the survival and reproduction of the society and its members. Different societies are different games with different rules. However, all of the ones I'm aware of include rules against violence, because games of society accomplish their purposes better when such rules are instituted.

We can immediately generalize this answer as follows. Legal laws are among the rules of the social game that the members of a society play in order to constitute that society.

Thus far I have gotten rid of two distinctions between laws and rules. I have terminologically separated objective laws of nature out of usages of the term "law" in its unqualified usage. I have also categorized legal laws as rules (of the social games that members of a society play in order to constitute that society). Aside from the qualifier that objective laws of nature are obviously laws in some sense, can we draw any distinction between laws and rules within the terminological framework I have set up? It seems not; within this terminological framework, laws (in the unqualified sense) seem to be a type of rules in every case.

It still feels odd to speak of the "laws of chess." In the terminological framework we've arrived at, we have no substantive distinction between rules and laws, but there is still a distinction of habitual application of the two terms to different things. The laws of Newtonian physics are rules we are generally happy to call laws. The rules of chess, not so much. In general, we seem to apply the term "law" to rules that cover a larger jurisdiction than other rules.

What else can we say about rules?

Essentially any rule can be written in the form of a statement (possibly a very long one). The statement of a rule says what is true in the context of the rule if the rule is followed. When what the rule says isn't true, we say the rule has been broken.

Rules are of two types: **descriptive rules** and **prescriptive rules**. The statement of a descriptive rule states that empirically, the statement of a rule holds true. The laws of Newtonian physics are examples of descriptive rules. The statement of a prescriptive rule, on the other hand, states that people should choose to follow the rule. Legal laws are examples of prescriptive rules.

Anybody can state a descriptive rule. Descriptive rules can be correct or incorrect in a few different ways. If the statement of a descriptive rule is true in all cases, the rule is universally correct. If the statement of a descriptive rule is true in the great majority of cases, with rare exceptions, the rule is correct as a generalization. Descriptive rules that only hold in some cases are also interesting, but probably "rule" isn't the best word for them; maybe "pattern" is better. A descriptive rule is correct or incorrect regardless of who states it. There is no authority besides reality on what descriptive rules are correct.

Prescriptive rules work a little differently. Anybody can state prescriptive rules, but typically they need to persuade others to consent to follow them, which others are typically free not to do. Prescriptive rules are also made by various rule making entities with self-proclaimed and recognized authority based on consensus and/or force. Examples of rule making authorities include governments, international organizations like the UN and the EU, standards organizations like ISO and ANSI, and so forth.

Prescriptive rules can only be about human behavior. Descriptive rules can be about people or things, but descriptive rules about human behavior will never hold universally, because people have freedom of choice.

Dictionaries are examples of sets of descriptive rules about human behavior. Dictionaries describe the senses of words as most commonly used by English speakers. However, English speakers can and occasionally do use words in senses not described in dictionaries.

Dictionaries can also be thought of as sets of prescriptive rules, for example if you're of the opinion that people ought to spell words the way they're found in the dictionary, or if you're of the opinion that people ought to use words that are in the dictionary only in senses defined in the dictionary.

What is it that rules and laws do in the world? Rules and laws have effects. How can we describe their effects?

All rules/laws bind us and/or the world, in some sense and at least in some cases. The sense in which rules/laws are binding varies.

* The laws of physics are binding in the sense that physical theories generally postulate laws that always hold within their domain of applicability, so that any counterexamples would falsify the theory.
* Sociological laws like Duverger's law and Godwin's law are expected to have some counterexamples, and counterexamples are not necessarily considered to undermine the whole law. They are binding in the sense that they represent strong statistical trends that one tends to observe over time.
* The laws of governments are binding in the sense (and to the extent) that they can be enforced and offenses are punishable. Many consider the laws of governments to be binding in a moral sense in many cases. It seems to be a consensus among the people I speak to that laws of governments are not always binding in a moral sense, but I imagine this perspective is not shared by the whole world. For example, many Islamists undoubtedly would agree that sharia law is binding in a moral sense.
* Many believe in many cases that international law is binding on states in a moral sense. International law is binding in another sense (and to the extent) that it is enforceable and offenses are punishable, which varies widely from case to case.

I can't think of any counterexamples to the generalization that all rules/laws are binding, to something in some sense in some cases. This, then, is one truism about rules/laws which I will accept: all rules/laws are binding.

Let's try to study further the phenomenon of rules and laws binding phenomena. If we look at the phenomenon of binding, it seems to divide into two distinct subtypes: the case where the rules and laws are defined by a centralized authority with the ability to enforce rules/laws, and the case where there is no such centralized authority. Let us call these **authoritative binding** and **organic binding**, respectively.

The distinction between authoritative and organic binding is not sharp. In most cases, centralized rulemaking authorities have authority due to some level of consent/agreement to be ruled among the people who follow their rules. In many cases, there are varying levels of force used to enforce the authority of rulemaking authorities. In the case of governments, it seems they are generally able to rule only by a combination of general consent and the ability to use force.

The distinction between authoritative and organic binding is not the same as the distinction between descriptive and prescriptive rules. There are prescriptive rules which are authoritatively binding, like laws, and prescriptive rules which are organically binding, like standards of common decency. As far as I'm aware, there are no descriptive rules which are authoritatively binding; I think all descriptive rules are organically binding.

Prescriptive rules can be made authoritatively binding on people if one is able to use authority to instill sufficient fear in people of the consequences of breaking the rules that they follow the rules. Organic binding of prescriptive rules can also be based on fear, if people fear the consequences of breaking the rules, but not because they fear retribution by an authority. An example is the organically binding prescriptive rule "don't eat mushrooms you are unable to identify."

Prescriptive rules can also be organically binding on the basis of things other than fear. For example, rules of grammar are organically binding not as a result of fear, but mostly as a result of mostly-unconscious mental processes of language formation which follow rules of grammar in order to allow us to communicate more easily.

A prescriptive rule can be organically binding in a given social game if there is an approximation of a [Nash equilibrium](https://en.wikipedia.org/wiki/Nash_equilibrium) around following the rule. What I mean by this is, for essentially every player in the game, their incentives mean that they are better off following the rule, as long as those around them continue to follow the rule. In such a situation, people may violate the rule, but they generally experience consequences from doing so; others observe and anticipate those consequences; and as a result people generally follow the rule.

One simple kind of case of an approximate Nash equilibrium around following a rule is the kind of case where each player is individually better off following the rule, regardless of what others do. "Don't eat mushrooms you are unable to identify" is an example of a rule like this.

In other cases, whether people are better off following the rule depends on whether or not others follow the rule. For example, if others are working hard to get US dollars in your area, then you're probably better off if you work hard to get US dollars, so that you can buy things from all the people working for dollars in your area.

In summary, here are the key statements I have made about rules.

1. Rules are defined and established by humans.
2. Pretty much anything can follow rules.
3. Games have rules, and rules occur in the context of games and gamelike activities.
4. There is no substantive difference in meaning between "rule" and "law," understanding the unqualified term "law" to exclude laws of nature (but to include human-constructed descriptions thereof).
5. Laws are a kind of rule, but the term "law" isn't applied to all rules, but only to certain rules whose jurisdictions are particularly wide.
6. Essentially any rule can be written in the form of a statement, saying what is true when the rule is followed.
7. Rules are of two types: descriptive rules and prescriptive rules.
8. All rules/laws bind us and/or the world, in some sense and at least in some cases.
9. The phenomenon of binding is of two types: authoritative and organic binding. Descriptive rules are always organically binding.
10. Prescriptive rules can be organically binding in a game if there is an approximation of a Nash equilibrium around following the rule.

## Winning

Let's return to a question I asked a ways back. What is winning? Earlier I asked whether winning always happens to people in the context of some game. If true, this would be an interesting generalization that would let us apply the philosophy of games to understanding winning.

What seems to be the case is that **winning always occurs in the context of some game or gamelike activity**. "Game or gamelike activity" is, as we have seen, a very broad category, taking in things as disparate as soccer, capitalism, and life. For that reason, it's not a very strong claim to say that winning always occurs in the context of some game or gamelike activity. We'll assume this generalization going forward.

What is winning? Winning is some kind of condition which a player of a game, or a participant of a gamelike activity, can obtain or be in. Typically it's a condition defined by the rules of the game or activity. In typical cases, the conditions for winning are defined by the (explicit) rules of the game, and there is rarely question about whether and when somebody has won the game. However, these generalizations belie some complexities.

Consider, again, winning at life. Whether somebody is winning at life can only be said relative to some frame of evaluation.

For example, consider a person who amassed great wealth through ethically dubious yet legal means. From a straightforwardly capitalist point of view, one might say they were winning at life, because they are amassing wealth in the free market. From a Christian point of view, one might say they were not winning at life. This Christian judgment can be based on the belief that life is a game where the object is to seek the way of Christ, which the wealthy person in our example is not doing. These two frames of evaluation, capitalism and Christianity, provide two very different perspectives on whether the person of the example is winning at life.

Consider, as a further example, somebody who lived an impoverised life in a rural region of the world where they farmed, gained religious education, gained training as a warrior, and died young in combat. From some frames of evaluation, one could say this person had an underprivileged and ultimately oppressed life where their future as an individual was subjugated to the nonsensical needs of a national war machine. From some other frames of evaluation, one could say that this person led a virtuous and spiritual life which culminated in the honor of dying in a fight for that which is sacred. Again, each of these narratives provides very different conclusions as to whether this person is winning at life.

For any life you can describe, you can imagine multiple frames of evaluation which provide different conclusions about whether the liver of that life was winning. This is to say that facts alone are not sufficient to determine a judgment of winning at life or not winning at life. One also needs some additional ingredient, what I am generically calling a "frame of evaluation."

Winning at life, in this respect, is unlike winning at most games, in that fair observers will frequently disagree about whether somebody is winning at life, whereas for most games, fair observers will almost always agree about who the winner of the game is and whether there is a winner. This disanalogy is an interesting conceptual disconnect that seems worth working out, to see what can be learned.

The disanalogy reminds us of a question I asked a ways back and still haven't answered: is (human) life a game? 

Life is like a game in some ways. In life one must follow rules. In life there is winning and losing. Life is unlike typical games in that there is no privileged, official frame of evaluation for judging winning and losing at life.

"Life is a game" can be thought of as a terminological rule. It says, we should be willing to call life a game. I don't wish to assert or deny this prescriptive rule. I leave it up to the reader whether or not to call life a game.

Let's now dig further into the concept of frames of evaluation. The most practical application of frames of evaluation is the case where one adopts a certain frame of evaluation as a frame of evaluation one routinely and habitually applies to one's own life -- this we can call a value system -- and then one proceeds to attempt to win at life according to the standards of that frame of evaluation. Almost everybody does this to some extent.

People apply many different frames of evaluation to many different parts of their lives as their lives go on. These frames of evaluation generally evolve over time. Conflicts often exist, where different frames of evaluation applied simultaneously or at different times yield conflicting conclusions. This can lead to regret about past decisions, a sense of turmoil and inner conflict in one's present, etc.

If one wants to do something resembling winning at life, then it's useful to unify multiple conflicting frames of evaluation that one applies to one's life into a single frame of evaluation, which can and should evolve over time. The reason this is useful is that without a unified frame of evaluation to judge success and therefore guide decision making, one's decisions won't move one's life in a consistent direction, and rather than reaping the rewards that come from pursuing a well strategized life, one may instead drift about without ever finding lasting success.

This does not argue against drifting about for a time in life; this can be helpful for learning, self discovery, and so forth. It just argues that if one's interested in something resembling winning at life, then one should be interested in the pursuit of coherence in and the working out of contradictions between the frames of evaluation that one applies to one's life and one's choices, and in the eventual development of a philosophy and direction in life if one lacks those things in good measure.

We have analyzed the concept of winning at life at some length. Let's now turn back to the general concept of winning. To understand winning better, it would be interesting to have a theory of winning which might explain some of the confusing observations we've made, such as that there are so many different, incompatible frames of evaluation with which to judge winning and losing at life.

My theory of winning is as follows: **to win is to achieve a purpose one has in a game or gamelike activity**.

Typically, when one is explicitly declared the winner of a game, this means that one has won according to the rules of the game, or in other words that one has achieved the explicit purpose stated in the rules of the game. When one achieves an explicit purpose one has in playing a game, we'll call this **explicit winning**.

An example of explicit winning at life is the case where somebody publicly, explicitly declares their purpose(s) in life, and goes on to achieve said purpose(s). Life of course has no official rules and no official conditions for winning, but nonetheless it seems fair to refer to achieving one's purpose(s) in life as winning at life, and if those purposes are explicitly declared, then it is explicit winning.

When one achieves an implicit purpose one has in playing a game, this typically carries the feeling of victory, but if the purpose is implicit then the celebration will typically also be implicit, meaning in this case internal. We'll call achieving an implicit purpose in a game **implicit winning**.

In most cases, when we talk about winning, we're talking about explicit winning. However, it certainly seems that success in life is not predicated on others' awareness of one's intentions, so that it seems unreasonable to discount implicit winning as a form of winning.

We can think of many examples of implicit winning. One is playing chess with a potential romantic interest and accomplishing the implicit purpose of having meaningful and pleasant social interaction with them which increases the probability of future romantic success with them, perhaps in addition to the explicit purpose of winning the game. Another example of implicit winning is taking care of some financial chores and accomplishing the implicit purpose of increasing one's sense of financial confidence, in addition to the explicit purpose of refinancing one's debt or whatever it was. Another example of implicit winning is finishing a job at work and doing it well, accomplishing the implicit purpose of giving one's life purpose and giving oneself a reason for self-pride in a job well done, as well as the explicit purpose of fulfilling one's work obligations.

## Winning debates

Having analyzed the concept of winning, we can now shed more light on the concepts of winning arguments and winning debates. We are back where we were at the start of the Introduction, at my attempt to explain the meaning of the title, Winning Arguments.

In the first three paragraphs of the Introduction, we distinguished between two senses of "winning arguments," corresponding to two senses of "argument," i.e. "argument" in the sense of "debate," and "argument" in the sense of "a series of statements designed to provide reason to believe some conclusion(s)." We refer to the former as debates and the latter as arguments.

Thus, we have two concepts stated by the title, both of which we need to analyze: the concept of winning arguments, and the concept of winning debates. "A winning argument," we presumed, means essentially "a persuasive argument;" but a deeper conceptual analysis of the phrase remains to be done. "Winning debates" refers to the activity of winning in debates.

Let's start by digging deeper into the concept of winning debates. The phrase "winning debates" presupposes that debates are a kind of game or gamelike activity. Are they? Well, are debates like games? Let's ask some more questions that shed light on that.

Recall I defined debates to be "exchanges of communication where participants discuss with each other the merits and demerits of some claims that they are mutually interested in and believe themselves to disagree about or are uncertain about."

Are debates competitive? Typically so, as participants compete to persuade other participants of their point of view. Are debates cooperative? Yes, in the sense that to engage in a coherent debate, participants need to understand each others' statements and respond with relevant, appropriate statements.

Do debates follow rules? As I have noted before, participants in a debate are usually free to say whatever they want. As such there are no descriptive rules (about the speech of participants) that all debates can be observed to follow. Similarly, there is no rulemaking authority with the power to enforce prescriptive rules applying to all debates. In short, there are no rules that all debates follow. Nor do I have any reason to expect in the future that there will be such rules.

Debates can, however, be observed to follow rules. For example, the speech within debates generally follows rules of grammar and so forth. Other rules of interest applying to debates include rules of logic, rules of politeness and common decency, and other norms governing linguistic communication, such as [Grice's conversational maxims](https://www.sas.upenn.edu/~haroldfs/dravling/grice.html). As I have already observed, none of these rules are always followed in debates. However, they are often followed. Perhaps more interestingly, following these rules, along with various other widely accepted rules of language use and debate, tends to help one win debates. Why this is the case, and what it means to win debates, are questions that we haven't fully addressed, either in this text or as a species. 

Debates are evidently fairly gamelike. This renders palatable the usage of the phrase "winning debates." What is it to win a debate? I have said that to win is to achieve a purpose one has in a game or gamelike activity. Therefore, one can be said to win a debate if one achieves a purpose one has in it.

People can have many different purposes in a debate. Here are some examples of purposes that people can have in speaking in debates:

* To persuade others of a certain point of view on a topic.
* To determine the truth about a topic.
* To be included in the social group having the debate.
* To impress others with one's intelligence and rhetorical skill, and thereby increase one's social status.
* To blow off steam.
* To humiliate one's debate opponents.

Here are some examples of purposes that people can have in listening to debates:

* To learn about the topic and/or form one's opinion on it.
* To learn about the participants in the debate.
* To be included in the social group having the debate.
* To be entertained.

Debates can have a variety of different structures. The most common structure might be the **two-fixed-opposed-sides debate**, which has the following characteristics:

* Two positions are being debated.
* The two positions are (believed by the participants to be) incompatible with each other, meaning that one cannot coherently hold both positions at the same time. In a common special case, the two positions hold that a certain proposition is true or is false, respectively.
* The positions being debated do not change throughout the course of the debate.
* The side that each speaking participant is arguing for does not change throughout the course of the debate.

Another structure, which contrasts in interesting ways with the two-fixed-opposed-sides debate, is the **truth-seeking debate**, which has the following characteristics:

* An open-ended set of positions are being debated.
* Participants do not have any fixed outcome in mind for the debate and are prepared to change their minds as a result of the debate.
* Participants may change their positions at any time and as many times as they choose.
* Participants may refrain from holding a position.

There are, of course, going to be debates which fit neither of these descriptions. However, these two forms of debates seem to be some of the most common and workable debate structures. I think most debates can be placed on a spectrum between two-fixed-opposed-sides and truth-seeking. However, not all debates fall anywhere on that spectrum. For example, there can be three-fixed-opposed-sides debates, and debates with a higher number of fixed opposed sides.

The two primary debate structures I have outlined have substantially different conditions for winning, because they have substantially different purposes. In the two-fixed-opposed-sides debate, generally the participants share the assumption that exactly one of the two positions is correct, and the condition for winning, as understood by the debate participants, is typically more or less to establish to the satisfaction of the observers of the debate that one's own side has the more persuasive arguments in its favor. The purpose of a two-fixed-opposed-sides debate is usually to establish dominance of one of the two positions over the other.

In a truth-seeking debate, the purpose and therefore the conditions for winning are quite different. The purpose is not to establish dominance of one position over another. Usually, the purpose is to arrive at a consensus that a certain position represents the truth on the topic of interest. If every participant's principal purpose was to reach such consensus, then when consensus is achieved, we can say that everybody has won, because everybody has achieved their principal purpose. On the other hand, if consensus is not reached, then we would probably say nobody has won, because nobody has achieved their principal purpose. In short, the goal of a truth-seeking debate is consensus rather than dominance, and in a truth-seeking debate everybody wins or doesn't win together, rather than one side winning and another losing.

The foregoing comments give us some idea of what it means to win debates. There are many different kinds of debates where different kinds of winning are possible, but we have at least a basic grip on that diversity at this point. Now we have fulfilled one of the first objectives I set in the introduction, which was to explain the phrase "winning debates."

Accomplishing the purpose(s) one has in a debate generally entails persuading others of a point of view, and/or exchanging ideas in a way which appears to move oneself and/or others closer to the truth. Arguments, certainly, are great not only as tools to persuade others of a pre-conceived point of view, but also and even more so as tools to explore conceptual space, acting as guides to move us forward towards the truth.

Speaking practically, I would say that the following two rules are the most important rules of winning debates:

**1. Be right.**

There is little it's better to have on your side in a debate than the truth.

The most important rule for winning debates is to **avoid getting into debates that you are going to lose in the first place.** If you'd like to build a reputation as somebody who speaks the truth, you should think before you speak and not get into debates that you are going to lose.

If you are just learning to win debates, that is basically the opposite of what you should do. If you are just learning to win debates, you would do well to be willing to get into any debate, defend any position, and not worry too much about being wrong. When you're learning to debate, just take advantage of opportunities to practice.

Skilled debaters still need to practice to stay skilled. However, skilled debaters probably have a highly developed skill of playing out debates in their head, and the amount of time they need to spend debating with others is likely to be reduced by their ability to practice and extrapolate hypothetical debate lines in their head. (Most people probably have debates in their head. However, inexperienced debaters probably won't be able to anticipate the arguments that would be leveled against their positions in a real debate.)

If you're going to debate people in a fixed-opposed-sides format, then you should make sure that your side is the one that is correct, and then you've taken the first step towards winning. If you're going to engage in a truth-seeking debate, just make sure that's also the purpose of your debate partners, and you don't need to worry about losing.

**2. Prepare.**

Many forms of preparation are required to be an impressive debater on a topic. One needs general facility with language and the techniques of debate. One needs background knowledge on the subject matter of debate. One needs a thorough knowledge of the various arguments for and against the position one wants to argue for. And it should be the case that taken all together, those arguments favor one's own position. This will always be a matter of judgment, and in many cases the best one can hope for is that the arguments will favor one's position in most people's judgment.

Even if you are well prepared, in a debate your debate partners will likely bring up arguments you didn't think of, and these arguments may create big problems for the view you entered the debate with. Hold great respect for what your debate partner knows that you don't know, because you don't know what they know.

We are usually overconfident that our current views are correct. When we notice our own overconfidence in our beliefs, we can over-correct and end up with under-confidence in what we know. Training away our overconfidence in our beliefs and finding a balanced degree of confidence in each belief that we hold is the work of a lifetime. Learning to quickly perform balancing of our confidence levels on the basis of a given information set is an important aspect of debate preparation.








## Winning arguments

What are winning arguments? Recall I defined an "argument" to be "a series of statements designed to provide reason to believe some conclusion(s)." I presumed that a winning argument is something like a persuasive argument.

Based on our prior conceptual analysis of "winning," we can state that a winning argument is, in general, an argument which wins in some sense and in some context, with the context being some form of game. Typically, the game is debate. Typically, a winning argument is one that wins in debates.

That tells us the "why" of winning arguments. The more interesting and more difficult question is "how." How do we construct winning arguments? The first step, and also in my opinion the hardest step, is learning how to discern between winning and non-winning arguments. In other words, the meat of the problem is to answer the "what" question. That is, what are winning arguments?

The "how" of constructing winning arguments is exactly what we aim to get at in this text. The text itself is concerned with theory, as opposed to the exercises, which are concerned with practice. The text is primarily concerned with the "what" question: what are winning arguments?

In the coming sections we will explore the "what" question in great depth. This is us laying groundwork for the theory of rhetoric. We will start by turning our attention to language, the very stuff of which arguments are made. To understand how to construct winning arguments, we must first understand our building material very well.

## Language

What is language? Language involves many different things and can be analyzed from many different perspectives. The next paragraph is an example that we'll use as a point of focus for introducing language.

Just before writing this sentence, I said the sentence "the dog runs." I said it out loud in my apartment. I heard myself, and as far as I know, nobody else heard me. As far as I know, my voice wasn't recorded in any medium. Anybody listening to my reading of this text will have been hearing statements that were referring to the written text and not to the recording they're hearing, until they got to this sentence. Anybody who read the preceding sentence in the written text will be reading a written sentence that talks about a corresponding spoken sentence that hadn't been said as of the time of its writing.

The preceding paragraph illustrates some interesting features of (natural) language, including that **(natural) language is multi-medial** and that **(natural) language is self-referential**.

When I say that natural language is self-referential, what I mean is that natural language can reference and discuss language. For example, this sentence talks about itself. Any sentence containing the phrase "this sentence" has the property of talking about itself.

When I say that (natural) language is multi-medial, what I mean is that language can exist in more than one medium. An English sentence like "the dog runs" can occur in spoken form, as vibrations in the atmosphere; in written form, as graphite or ink markings on a piece of paper; in written form, as bits stored on a computer hard drive; in written form, as light emanating from a computer monitor; in spoken form, as analog electrical signals traveling through an audio cable; in spoken form, as bits representing a digital audio signal traveling through a fiber optic network cable; and so on and so forth. I can think the sentence "the dog runs," and I can do so in different ways. For example, I can mentally recite the sentence to myself, and I can do so in different voices; or I can visualize the written sentence in my mind, and I can do so in different font styles. 

One interesting observation is that all of the items mentioned in the preceding paragraph can be called "the sentence 'the dog runs.'" E.g., "I wrote the sentence 'the dog runs' on a piece of paper," or "anybody listening to a digital audio recording of this paragraph will hear the sentence 'the dog runs.'"

There is a logical difficulty with saying that all of these different items literally are the sentence "the dog runs." Suppose for example that we accept the following premises:

1. *x* is a spoken sentence.
2. *x* is the sentence "the dog runs."
3. *y* is a written sentence.
4. *y* is the sentence "the dog runs."

From premises 2 and 4 we can infer that *x* is *y*, by the transitive property of the identity copula.

By the identity copula, I mean the word "is," used in the sense of "is one and the same thing as," as in the sentence "Earth is the third planet from the Sun," and the sentence "2+2 is 4."

The transitive property of the identity copula is the property that if *a* is *b* and *b* is *c*, then *a* is *c*. In this case, let *a* be *x*, *c* be *y*, and *b* be "the sentence 'the dog runs.'" In this case, the property states that if *x* is the sentence "the dog runs" and *y* is the sentence "the dog runs," then *x* is *y*. Of course 'is' is used in the preceding sentence in the sense of the identity copula, i.e. meaning "one and the same as."

The transitive property of the identity copula is perfectly analogous to the [transitive property of equality](http://www.mathwords.com/t/transitive_property.htm) in math. If you think that the = sign in math is simply a way of writing the identity copula, then these two properties are the same property.

Returning to premises 1-4, we have seen that they imply *x* is *y*, at least if we interpret the "is" in premises 2 and 4 as being the identity copula. But *x* is a written sentence and *y* is a spoken sentence. This contradicts the fact that no written sentence is a spoken sentence. This is the logical difficulty I wished to point to.

What this argument implies is that we cannot accept all of premises 1-4, interpreting "is" as the identity copula in premises 2 and 4 and assuming that the transitive property of the identity copula is a true principle about the identity copula.

Premises 1-4 are plainly true according to common sense and common ways of speaking, for appropriate values of *x* and *y*. As such, rejecting the idea that premises 1-4 are true for some *x* and some *y* is not an appealing option. Rejecting the transitive property of the identity copula also seems to be an unappealing option, because it as well seems obviously true. What's obviously false in this situation is that two different and distinct things, both of which we can call "the sentence 'the dog runs,'" are one and the same thing.

Common sense suggests that we should not interpret premises 1-4 in a way which leads immediately to a false conclusion. The remaining and most appealing way to do that is to interpret "is" in premises 2 and 4 as something other than the identity copula.

For example, "is" in premises 2 and 4 could be short for "is an instantiation of," so that premises 2 and 4 literally mean "*x* is an instantiation of the sentence 'the dog runs'" and "*y* is an instantiation of the sentence 'the dog runs.'" This gets rid of the logical contradiction. It is not the case that if *x* is an instantiation of *z* and *y* is an instantiantion of *z*, then *x* is *y*, unless *z* has only one instantiations. However, quite plainly, sentences can have multiple instantiations. However, quite plainly, sentences can have multiple instantiations.

This solution to the puzzle leaves us with a new puzzle: what is the sentence 'the dog runs?' Clearly we can point to many instantiations of the sentence 'the dog runs,' but the solution we're considering prevents us from saying that any instantiation (or at least more than one instantiation) of the sentence 'the dog runs' is itself the sentence 'the dog runs,' since that would bring us back into our logical contradiction. If none of the instantiations of the sentence 'the dog runs' are the sentence 'the dog runs,' then what is the sentence 'the dog runs?' Is it some sort of metaphysical entity, like a [Platonic form](https://en.wikipedia.org/wiki/Platonic_form), existing outside of space and time?

Another possible way of answering the puzzle is to deny that there is any object which we can literally call "the sentence 'the dog runs.'" The structure of English grammar certainly suggests that there is an object we can call "the sentence 'the dog runs,'" since "the sentence 'the dog runs'" is a noun phrase which can be used to construct infinitely many meaningful statements about this sentence. As such, this way of answering leaves me (and philosophers galore) rather confused about the meaning of phrases like "the sentence 'the dog runs;'" however, that doesn't make this way of answering wrong, and I'm not saying this way of answering is wrong.

This puzzle is one of a variety of logical puzzles that have to do with language's self-referential capabilities. One we will discuss at more length later is the liar paradox. This is the puzzle concerning the sentence "this sentence is false." Is the sentence true or false? You can reason that if it's true, then it's false, and if it's false, then it's true. The paradoxes of self-referencing language are hard. I will talk about them at greater length in the section titled Paradoxes.

Let us therefore turn back to unpacking the notion of language. Consider again the sentence "the dog runs."

Let's consider the sequence of events that occurs when I say "the dog runs." First, I form a mental intention to say the sentence. Then, my brain sends signals which cause a sequence of events to occur in my body.

My tongue is raised to touch my teeth, creating a barrier between the area above my tongue which is contiguous with my respiratory system, and the empty space inside my mouth which is contiguous with the outside atmosphere. My respiratory system pushes air out into my mouth, creating a pressure differential between these two areas. Because of its precise placement near my teeth, the tip of my tongue starts to vibrate against my teeth, creating a "th" sound. I suddenly open up a gap between my tongue and my teeth, releasing the pressure differential and completing the word "the." A similar sequence of physical occurrences completes the words "dog runs." All of this bodily motion is achieved in humans through a powerful and precise muscular system controlled by a sophisticated and fine-tuned nervous system.

These mechanical events in my body cause vibrations in the atmosphere, like waves in a pool of water, which propagate according to the rules of [fluid dynamics](https://en.wikipedia.org/wiki/Fluid_dynamics). Any objects in the nearby area of contiguous atmosphere are going to be disturbed by my speech, but usually only very slightly. Generally the effects will be entirely negligible, except on objects which are specially sensitive to sound vibrations, such as human ears and microphones.

The most interesting action happens when my speech irritates another human's ears while they are paying attention to it. This causes nerve signals to fire in their brain which cause them to have an experience of meaning whose content is partially controlled by precise variations in the patterns of sound vibrations I produce.

Spoken language that is intelligible to a listener is a very special kind of complex of sound vibrations. Only very specifically shaped sounds constitute language that is intelligible to a given listener. Intelligible language is like a psychoactive drug, in the following way. Psychoactive drugs affect the mental experiences of a person by being shaped in very specific ways which allow them to attach to neural receptors. Intelligible linguistic utterances affect the mental experience of a person by being shaped in very specific ways which allow them to attach to that person's inner mechanisms for synthesizing meaning.

The synthesis of meaning by processing language is a reaction between two things: a piece of language, and a person's psychology. A piece of language is a comparatively small, simple thing, whereas a person's psychology is an extremely complex thing. In addition, language is a highly observable and analyzable phenomenon, whereas human psychology seems to be mostly inscrutable (with all due respect to any psychologists who would disagree with this).

My statement that psychology is mostly inscrutable means that unlike in many other domains, humans have been unable to produce a body of psychological theory which provides a rigorous (e.g. mathematical) framework for modeling the psychological state of a person, or the features of their psychological state which are typically salient in problem-solving contexts, and allows for accurate forecasting of their likely behaviors.

Let's break that down slightly. Call a domain of inquiry "scrutable" if humans are able to produce a body of theory about that domain which provides a rigorous (e.g. mathematical) framework for modeling the states of objects in that domain, or the features of their states which are typically salient in problem-solving contexts, allowing for accurate forecasting of the objects' behaviors. Then my statement that psychology is mostly inscrutable means that mostly, it is not scrutable in the aforementioned sense.

This statement does not assume that humans' behaviors can be deterministically predicted; this may be practically or conceptually impossible, as far as I know. That's why I use the term "forecast," suggesting a method that would provide a probability distribution (or something of like that) of behaviors that a person is likely to exhibit looking forward in time, given their current psychological state and surrounding circumstances.

In any case, I have given a thesis about the limitations of present day knowledge of psychology. My thesis is not that it's impossible for individual humans to achieve an understanding of human psychology which allows them to model salient features of a person's psychological state sufficiently to predict their likely behaviors, and to do this with many people in many situations given sufficient information. My thesis describes a limitation on the psychological knowledge that in my judgment a person can straightforwardly gather from the available corpus of psychological research and pedagogy.

I do not assume all readers will agree that psychology is mostly inscrutable in the sense I have described, although to be honest I imagine most will agree or be willing to accept this thesis. Some of what follows depends on this thesis, and I have tried to make it clear when the argument I am making depends on the thesis that psychology is mostly inscrutable. The following paragraph is an example of a place where I invoke the thesis.

The (arguable) inscrutability of psychology infects all attempts to study linguistic meaning from a psychological perspective. It is debatable whether any totally non-psychological theory can justifiably be called a theory of linguistic meaning. However, there is some appeal to such theories. 

If it were possible to describe the relationship of "meaning" as a relationship which obtains between language and the world -- e.g. a noun phrase refers to an object, and a statement describes a state of affairs in the world -- then it might be possible to describe the relationship of meaning simply and rigorously, without involving the messiness of humans. What's not necessarily is clear is that this can be done. Let's explore.

A distinction that sheds light on some of the issues I have just raised is the distinction between three types of linguistic meaning: sentence meaning, speaker meaning, and listener meaning. According to proponents of the concept of sentence meaning, every sentence has a **sentence meaning**, which is what that sentence means in itself according to the conventions of the language. In addition, every meaningful linguistic utterance has a **speaker meaning**, which is what the speaker meant by the utterance. Finally, whenever a listener listens to an utterance, there is a **listener meaning** specific to that listener, which is what it meant to them or what they thought it meant.

Proponents of the sentence meaning/speaker meaning distinction often think that the sentence meaning of a sentence can be determined by applying some sort of [recursive algorithm](https://en.wikipedia.org/wiki/Recursive_algorithm) over the grammar of the sentence, finding the meanings of the smallest meaningful fragments of the sentence (e.g. the words), and iteratively building up the meanings of larger sentence fragments until one has the whole sentence. Such analysis of meaning can be done with some variety of success on artificial formal languages, such as computer programming languages. The field of [formal semantics](https://en.wikipedia.org/wiki/Formal_semantics) is the modern successor of early efforts, such as Wittgenstein's, to develop concepts of sentence meaning along these lines.

Whether there is such a thing as sentence meaning for natural languages such as English is debatable. One can *define* notions of sentence meaning for simple, artificial languages with precise rules of sentence formation. However, nobody can give a *complete* definition of sentence meaning for English, because the complexity of English is vast and constantly increasing. If you wanted a definition of sentence meaning for English, I think you'd need to pay a whole team of philosophers to maintain and update it, and it wouldn't ever work for every English sentence. Moreover, if there were sufficient demand for such a definition of sentence meaning to fund such an enterprise, it's likely there would be multiple competing enterprises with conflicting definitions of English sentence meaning. The whole exercise would lack any clear point. In each case, what you'd get would arguable not be the one true notion of sentence meaning, but just that shop's opinion on how English sentence meaning should be defined. Thus if we imagine the scenario where we had an actual definition of sentence meaning for English, we are led to think that we would have more than one definition for the notion of sentence meaning for English.

Defenders of a single true notion of sentence meaning for English will probably have to say that the one true notion of sentence meaning is out there in the sky somewhere, never to be seen in full by us humans, and that people who pursue the concept of English sentence meaning must content themselves with doing their best to use reason to find the truth about English sentence meaning. It's unclear to me whether a descriptivist about English (by which I mean somebody who agrees that English has no centralized authority regarding rules of usage and at the end of the day people can use English however they want) can coherently maintain the position that there is a single true notion of English sentence meaning out there in the sky.

Probably most readers are skeptical of the idea that there is a single true notion of English sentence meaning. We are left with the possibility of multiple definitions or concepts of English sentence meaning. In addition, we are left with the concepts of speaker meaning (what speakers mean) and listener meaning (what listeners think speakers mean).

However, there is still a conventional notion of the conventional meanings of sentences that are sufficiently literal that they are not open to much variety of reasonable interpretation. Let us call this notion **the conventional notion of literal sentence meaning**.

"The conventional notion of literal sentence meaning" does not refer to one exactly defined notion of sentence meaning. Rather, the phrase is open to interpretation, and its meaning will vary according to context and according to the views of the participants in a conversation where the phrase is used. The phrase can be used in conversations where it serves a useful purpose in advancing the conversation. The phrase can be used in a conversation if participants in the conversation are sufficiently comfortable with using the phrase and they are comfortable that a shared understanding of the phrase exists which is sufficiently robust to solve the conversational problems at hand.

I will discuss meaning more extensively in the section titled Meaning. 

Speaking very broadly, there are three big things to study about language: syntax, semantics, and pragmatics.

**Syntax**, as I shall define it, is the study of language in itself, i.e. the study of the physical artifacts and phenomena which we call language: the spoken word, the written word, and so forth. The field of syntax doesn't study the people who use language; it just studies and forms theories of the structure of linguistic utterances and artifacts themselves.

**Semantics**, as I shall define it, is the study of the meaning of language. As we have seen in our discussion so far, and as we will see some more in the section titled Meaning, linguistic meaning is a highly difficult and opaque topic, presenting deep intellectual challenges.

**Pragmatics**, as I shall define it, is the study of language in the context of its usage by people.

These three fields are probably not entirely disjoint, and they don't cover everything there is to discuss about language. (There is the history of language, and the biology of language, for example.) However, they are a good way of subdividing the space of topics related to language that we are going to cover. More broadly, the mature philosophical approaches to studying language that I'm aware of are subsumed under these categories. 

The preceding paragraphs serve as an extremely high-level introduction to some of the central issues in the philosophy of language and to the study of language as a whole. In the rest of this text we will focus our study of language into specific topics which directly pertain to winning arguments. The reader is encouraged to learn every important lesson they can about language. If one learns to use language well, there is a great deal one can do with that.

## Statements

The first three paragraphs of the Introduction explained the meaning of the title, and the entirety of the text since then has been an attempt to explain the meaning of the first three paragraphs of the Introduction:

> Winning arguments is what I aim to show how to do in this text. I will show how this can be done by constructing winning arguments. As just illustrated, the phrase "winning arguments" has at least two important senses.
>
> First, "winning arguments" can be interpreted to refer to an activity, the activity of winning arguments, where "argument" here is used in the sense of "debate."
>
> Second, "winning arguments" can be interpreted as a noun phrase, where "argument" is used in the sense of "a series of statements designed to provide reason to believe some conclusion(s)." In this interpretation of the phrase, "winning" is an adjective, presumably meaning something like "persuasive."

Back in the Introduction, I gave a series of words from the first three paragraphs that needed explanation. Those words were: "debate," "argument," "winning," "true," "reason," "believe," "persuasive," and "statement."

We've done some analysis of "debate," "argument," "winning," and combinations thereof, but the rest of these words remain to be analyzed: "true," "reason," "believe," "persuasive," and "statement." In this section we shall analyze the word "statement."

What is a statement? A statement is a kind of sentence or sequence of sentences. Here are some dictionary definitions of "statement" that I looked up on Google:

* a definite or clear expression of something in speech or writing.
* an official account of facts, views, or plans, especially one for release to the media.
* a formal account of events given by a witness, defendant, or other party to the police or in a court of law.

In this section I am interested in statements in the sense (somewhat crudely expressed by this definition, in my opinion) of "a definite or clear expression of something in speech or writing."

The study of winning arguments is mostly a study of statements and the relationships between statements. Therefore, we should take care to develop a good understanding of statements.

Logicians frequently provide various definitions of the word "statement." In this text, I shall use the following definition of the word "statement." **A statement is a sentence or sequence of sentences which can be believed or disbelieved.** This definition picks out a certain wide class of sentences, spanning across all natural languages and many formal languages. This class probably agrees in essential intent, if not in actual [extension](https://en.wikipedia.org/wiki/Extension_(semantics)), with most modern logicians' definitions of "statement."

Here are some examples of statements:

* I am cold.
* This is a pen. It has red ink.
* It is early in the morning.
* After we harvest the wheat we will store it in the silos.
* Within the lives of others we can find some of the most important insights about ourselves.
* Every statement is either true or false.
* If the axioms of math can prove themselves to be incapable of proving any statement about numbers to be both true and false, then the axioms of math can prove any statement about numbers.
* 2+2=4.
* 2+2=5.

Here are some examples of sentences that are not statements:

* Why am I cold?
* Get over here!
* Fuck you!
* Help!

"Why am I cold?" is a question. "Get over here!" and "help!" are imperative sentences. An [imperative sentence](https://www.thoughtco.com/imperative-sentence-grammar-1691152) is "a type of sentence that gives advice or instructions or that expresses a request or a command." "Fuck you!" can be literally interpreted as an imperative sentence (telling the target to fuck themselves). However, "fuck you!" is perhaps better thought of as an [ejaculation](https://en.wikipedia.org/wiki/Ejaculation_(grammar)). Clearly, none of these sentences can be believed or disbelieved. For example, I make no sense if I say, "I believe that why am I cold?" or "I believe that fuck you!" Since they can't be believed or disbelieved, "why am I cold?" and "fuck you!" are not statements; similarly with "help!"

Statements follow rules of grammar. Rules of grammar vary between languages, but there are various common themes. The full complexity of grammar is manifest in the grammar of statements. The grammar of statements, which is the study of grammar minus the grammar of non-statement utterances, is an interesting study in its own right. In addition, an understanding of the grammar of statements is important background knowledge for what follows. Therefore, our next step in understanding statements is to embark on a study of the grammar of statements.

There are many approaches to the study of grammar. Indeed, there are a number of different kinds of grammatical theory of note:

* A **natural language grammar** attempts to describe the grammar of a natural language.
  * A **descriptive natural language grammar** attempts to describe the rules of grammar actually followed (in ordinary cases) by some population of language speakers.
  * A **prescriptive natural language grammar** defines the rules of grammar that some population of speakers should use, at least according to the author(s) of the grammar.
* A **formal grammar** exactly describes the grammar of an artificial language, a so-called **formal language**. Formal grammars are usually written in notation with precise, formal meaning, which renders it entirely unambiguous what does and does not constitute valid syntax for a statement of the formal language.
* A **theory of grammar** is a theory which says something about grammars as a whole, or about some restricted class of grammars. For instance, if one wishes to explain the concept of formal grammars in a thorough way, then this requires a theory of formal grammars. The [Chomsky hierarchy](https://en.wikipedia.org/wiki/Chomsky_hierarchy) is an example of a precise theory of formal grammars, explaining what a formal grammar is and categorizing them according to how computationally difficult it is to parse statements in them.

There is a lot of interesting stuff to explore here, and I only have space to scratch the surface. Let's start by learning a simple formal language of statements. We're going to explain how to write statements in a toy language whose only sentences are statements. This language is called **(the language of) first-order logic**. First-order logic is notable in that it has an extremely simple grammar, and essentially any statement can be expressed in first-order logic.

This presentation of first-order logic is notationally different from most other presentations of first-order logic, in that I opted to use English words in place of logical symbols, making the notation more naturally readable for English speakers, but less compact.

In essence the version of first-order logic I present should be equivalent to all other presentations of first-order logic, despite interesting differences. The reader can, as an exercise, find another presentation of first-order logic and find for themselves the reasons why that system is for all intents and purposes interchangeable with ours, despite their differences. This presentation differs in tricky ways (as well as simple ways) from most presentations of first-order logic, so that this can be an interesting exercise for a student of mathematical logic.

Here are some examples of statements in our version of first-order logic:

* (*a* is a dog)
* ((*a* is a dog) and (*a* runs))
* (for all *a*, (if (*a* is Santa Claus) then (*a* is a North Pole inhabitant)))
* (for all *a*, (if (*a* is a person) then (for some *b*, ((*b* is a person) and (*b* loves *a*)))))

As you can see, these statements look a lot like statements in English, but not quite. Let's go through the rules that define the grammar of this language.

There are the following kinds of words in (this presentation of) first-order logic: variable names, object literals, predicates, [copulas](https://en.wikipedia.org/wiki/Copula_(linguistics)#English), such as verbs, and connective words.

* For **variable names** I will use single italicized lower case letters: *a*, *b*, *c*...
* For **object literals** I will use English phrases denoting objects, e.g. "the President of the United States" or "this chicken." The latter example illustrates that English phrases denoting objects can be context-sensitive in meaning.
* For **predicates** I will use upper case letters (P, Q, R, ...) and ordinary English phrases denoting categories of objects, specific objects, or properties (e.g. "a dog," "the President of the United States," "clean").
* For **copulas** I will use ordinary English copulas, e.g. "loves," "instantiates," and "kills," as well as upper case letters (C, D, E, ...)
* The following are all the **connective words:** **if**, **then**, **and**, **or**, **not**, **for**, **all**, **some**, **is**.

An **object term** (of this presentation of first-order logic) is a variable name or an object literal. In other words an object term is any piece of syntax that can denote an object.

We will also need notations for variables ranging over different types of words:

* When we need a variable that ranges over object terms, i.e. an *object meta-variable*, I will use bolded lower case letters starting with "a": **a**, **b**, **c**, ...
* When we need a variable that ranges over predicates, i.e. a *predicate variable*, I will use bolded upper case letters starting with "P": **P**, **Q**, **R**, ...
* When we need a variable that ranges over statements, i.e. a *statement variable*, I will use bolded upper case letters starting with "A": **A**, **B**, **C**, ...
* When we need a variable that ranges over copulas, I will use a bolded lower case **v**.

A **lexical unit** (of this presentation of first-order logic) is a word, a parenthesis, or a comma.

A **statement** (of this presentation of first-order logic) is any sequence of lexical units which can be formed according to the following rules:

1. If **a** is an object term and **P** is a predicate, then (**a** is **P**) is a statement.
3. If **a** is an object term and **v** is an [English copula](https://en.wikipedia.org/wiki/Copula_(linguistics)#English), such as a verb, then (**a** **v**) is a statement.
4. If **a** and **b** are object terms and **v** is a verb, then (**a** **v** **b**) is a statement.
5. If **A** is a statement, then (not **A**) is a statement.
6. If **A** is a statement and **B** is a statement, then:
    * (if **A** then **B**) is a statement.
    * (**A** and **B**) is a statement.
    * (**A** or **B**) is a statement.
7. If **A** is a statement and **a** is a variable name, then:
    * (for all **a**, **A**) is a statement.
    * (for some **a**, **A**) is a statement.

Let us show how these grammar rules can be used to derive the examples I gave above of statements:

**(*a* is a dog) is a statement.**

* *a* is a variable name.
* "a dog" is a predicate (denoting a category).
* By rule 1, (*a* is a dog) is a statement.

**((*a* is a dog) and (*a* runs)) is a statement.**

* *a* is a variable name.
* "runs" is a verb, and therefore a copula.
* By rule 2, (*a* runs) is a statement.
* (*a* is a dog) is a statement (see above).
* By rule 6, ((*a* is a dog) and (*a* runs)) is a statement.

**(for all *a*, (if (*a* is Santa Claus) then (*a* is a North Pole inhabitant))) is a statement.**

* *a* is a variable name.
* "Santa Claus" is a predicate (denoting an object).
* By rule 1, (*a* is Santa Claus) is a statement.
* "a North Pole inhabitant" is a predicate (denoting a property).
* By rule 1, (*a* is a North Pole inhabitant) is a statement.
* By rule 6, (if (*a* is Santa Claus) then (*a* is a North Pole inhabitant)) is a statement.
* By rule 7, (for all *a*, (if (*a* is Santa Claus) then (*a* is a North Pole inhabitant))) is a statement.

The reader should understand the foregoing to the extent that they can produce an unlimited number of statements in first-order logic; produce for each of them derivations conforming to the pattern I have demonstrated; and distinguish between valid and invalid syntax for statements of first-order logic.

The reader should also understand what statements in first-order logic mean. For the most part, this may be fairly self-evident, but it is worth taking a moment to clarify explicitly what each form of statement means:

* (**a** is **P**) means that whatever object the object term **a** denotes in the current context satisfies the predicate **P**, or in other words has the property **P**, belongs to the category of objects **P**, or is the object **P**.
* (**a** **v**) and (**a** **v** **b**), where **v** is an [English copula](https://en.wikipedia.org/wiki/Copula_(linguistics)#English), such as a verb, and **a** and **b** are object terms, means that the corresponding English statement is true for whatever object(s) the object term(s) **a** and **b** denote in the current context.
* (**A** and **B**) means that **A** and **B** are both true.
* (**A** or **B**) means that at least one of **A** or **B**, and possibly both, are true. The "or" in first-order logic is therefore an *inclusive or*. This can be distinguished from an *exclusive or*, which differs from an inclusive or in that an exclusive or is false when both disjuncts are true. That is, an exclusive or requires exactly one of its disjuncts to be true, as opposed to at least one as with inclusive or.
* (not **A**) means that **A** is not true, or that **A** is false, depending who you ask. Whether "not true" and "false" are different properties also depends who you ask.
* (if **A** then **B**) means that if **A** is true, then **B** is true.
* (for all **a**, **A**) means that in any variant on the current context created by setting the object denoted by the variable **a** to some object, **A** is true.
* (for some **a**, **A**) means that in at least one variant on the current context created by setting the object denoted by the variable **a** to some object, **A** is true.

Many connective words/phrases have conventional names:

* **and** is called **conjunction**. A statement of the form (**A** and **B**) can be called **a conjunction**.
* **or** is called **disjunction**. A statement of the form (**A** or **B**) can be called **a disjunction**.
* **not** is called **negation**. A statement of the form (not **A**) can be called **a negation**.
* **if/then** is called **the conditional**. A statement of the form (if **A** then **B**) can be called **a conditional**.
* **for all** is called the **universal quantifier**, and is one of the two examples of a **quantifier** in first-order logic. A statement of the form (for all **a**, **A**) can be called **a universally quantified statement** or **a universal statement**.
* **for some** is called the **existential quantifier**, and is one of the two examples of a **quantifier** in first-order logic. A statement of the form (for some **a**, **A**) can be called **an existentially quantified statement** or **an existential statement**.

An occurrence of a variable name in a statement is called a **bound occurrence** if it is enclosed by a quantifier over the same variable; otherwise it is called a **free occurrence**. For example, in the statement (for some *x*, (*x* is a prairie dog)), the occurrence of *x* in (*x* is a prairie dog) is a bound occurrence. In the statement ((*x* is a prairie dog) and (for some *y*, (*x* loves *y*))), the two occurrences of *x* are free, whereas the occurrence of *y* in (*x* loves *y*) is bound.

The explanations I gave of the meanings of statements in first-order logic assume the notion of a *context*. For the purposes of first-order logic, the context simply determines what objects the variables, and the object terms with context-sensitive meanings (e.g. "it") denote. You can think of a context as a mapping from variable names to objects, from predicates to sets of objects, and from copulas to sets of objects and pairs of objects. (I will provide a more detailed version of this conception in the section titled Model Theory.) Contexts are referred to by various names in the study of logic, with "interpretation" being one term in use. In some presentations of first-order logic, every variable name is required to be given a value by a context, while in other presentations, a context might give values only to some variables. In the latter approach, statements containing free occurrences of variables will be uninterpretable in a given context if that context does not assign values to those variables.

We will take the latter approach in this text. Specifically, we will consider a context to provide a *partial* mapping from object terms to objects, saying for some subset of the set of all of context-sensitive object terms (i.e. variables and context-sensitive object literals) what objects they denote. In our approach, statements containing free occurrences of variables whose denotations are not defined by a given context, and statements containing context-sensitive object literals whose denotations are not defined by that context, will be uninterpretable in that context.

The approach of requiring contexts to define the denotations of all object terms is mathematically convenient because it entails that every statement is interpretable in every context. However, the assumption that every term is defined in every context is unrealistic. In reality, statements are uninterpretable when they contain context-sensitive object terms whose denotations are not clarified by the context.

I will not attempt to further formalize this explanation of the meaning of the statements of first-order logic at this time. At this point it's hoped that the reader understands the meaning of statements of first-order logic in an informal way which enables them to read such statements. There are many intricacies to explore regarding the meanings of statements of first-order logic, but we will defer this discussion for later seciotns.

TODO: Exercises

Earlier I made the claim that essentially any statement can be expressed in first-order logic. Let us now explore this claim. The claim is interesting because if it is true, then we can think of first-order logic as a sort of a theory of statements, an idealized way of thinking about statements as a whole.

First-order logic is not the only formal language about which one may fairly make this claim. There are many formal languages about which one may fairly make this claim. Most of them bear some resemblance to first-order logic. First-order logic is a good example of the category.

The first-order logic statements we have seen, when read aloud, are in essence English sentences (although their use of variable names is not ordinary English). Clearly many English statements are already in essence statements of first-order logic, in the sense that a translation is obvious and closely resembles the original statement. In other cases, varying degrees of rephrasing are required to express an English statement in first-order logic.

In order to investigate further, we shall abandon contrived examples and look at real sentences from out in the wild. We will look at some semi-randomly chosen statements from books that are in my home, and try to translate them into first-order logic. I don't claim that my translations are the only possible translations into first-order logic of these English statements, and I don't claim that they are exact translations capturing every shading of meaning that the statements have for every English speaker. I do claim that they are basically good translations that preserve the core meaning of the original statements.

**This cake is well named, as it has a very delicate consistency.**

Translation: ((this cake is well named) and (this cake is very delicate in consistency) and (the statement (this cake is well named) is justified by the statement (this cake is very delicate in consistency)))

Sentence from: Irma S. Rombauer. The Joy of Cooking. Simon & Schuster Inc.

"This cake is well named as it has a very delicate consistency" seems to mean roughly "this cake is well named because it has a very delicate consistency." That is, the sentence expresses three things:

* The cake is well named.
* The cake has a very delicate consistency.
* The cake is well named because it has a very delicate consistency.

This third statement is expressing a relationship between the first two statements: namely, that "this cake has a very delicate consistency" justifies "this cake is well named." This is reflected in the translation, where the copula "is justified by" relates the two object literals "the statement (this cake is well named)" and "the statement (this cake is delicate in consistency)."

TODO: break down further

**Every passerby could read the sign, for every passerby could read Hebrew, Latin, or Greek --- the three great languages of the ancient world.**

Translation: ((Latin is a great language of the ancient world) and (Hebrew is a great language of the ancient world) and (Greek is a great language of the ancient world) and (for all x, (if (x is a great language of the ancient world) then ((x is Greek) or (x is Latin) or (x is Hebrew)))) and (for all x, (if ((x is a person) and (x is passing by the sign)) then ((x can read a great language of the ancient world) and (x can read the sign) and (the statement (x can read a great language of the ancient world) justifies the statement (x can read the sign)))))).

Sentence from: Max Lucado. He Chose the Nails. Thomas Nelson.

The size of this translation shows that many different logical propositions are contained in this not-exceptionally-long English sentence.

TODO: break down

The hawk-eyed reader may notice that I have taken some liberties with parentheses in this translation. For example, I wrote ((x is Greek) or (x is Latin) or (x is Hebrew)), but technically this statement cannot be produced by our syntax rules, because our syntax rules cannot produce three "or" clauses under one shared set of paretheses. To conform technically to our syntax, the statement must be written with two of the "or" clauses parenthesized together, in one of these two ways (with the added parentheses in bold):

(**(**(x is Greek) or (x is Latin)**)** or (x is Hebrew)

((x is Greek) or **(**(x is Latin) or (x is Hebrew)**)**)

Both of these statements mean the same thing and are true under exactly the same circumstances: namely, when at least one of the three clauses is true. Later we will be able to prove formally that these two statements are true under exactly the same circumstances. We omit the bolded parentheses are am deliberately ambiguous about which of these two statements we mean, purely as a matter of convenience, in an example of what is called an **abuse of notation**.

The other example of us taking liberty with parentheses in this translation is of the same kind, but with "and" instead of "or." The statement as a whole has the form:

((Latin is a great language of the ancient world) and (...) and (...) and (...) and (...))

**Besides the theories on the unity of the concept of being in the writings of Henry of Ghent and John Duns Scotus, there were other late medieval attempts to resolve this basic issue of metaphysics.**

Translation: (for some *a*, (for some *b*, ((Henry of Ghent wrote *a*) and (John Duns Scotus wrote *b*) and (*a* is a theory of the unity of the concept of being) and (*b* is a theory of the unity of the concept of being) and (*a* is late medieval) and (*b* is late medieval) and (the theory of the unity of the concept of being is a basic issue of metaphysics) and (for some *c*, ((not (*c* is *a*)) and (not (*c* is *b*)) and (*c* is a theory of the unity of the concept of being) and (*c* is late medieval))))))

Sentence from: Richard H. Popkin (ed.). The Columbia History of Western Philosophy. Columbia University Press.

**I myself in your situation, if I had an appointment with a Godin... Godet... Godot... anyhow you see who I mean, I'd wait till it was black night before I gave up.**

Translation: (if (I have an appointment with Godot) then (if (not (I am waiting for Godot)) then (the time of day is black night)))

Sentence from: Samuel Beckett. Waiting for Godot. Grove Press.

This translation omits the difficulty the speaker has in referring to Godot, and simply refers to the individual the speaker is referring to. If we wanted to translate the difficulty literally, we could write something like:

(for all *x*, (if ((*x* is Godin) or (*x* is Godet) or (*x* is Godot)) then ((you know the referent of "*x*") and (if (I have an appointment with *x*) then (if (not (I am waiting for *x*)) then (the time of day is black night))))))

This example illustrates that there can be multiple plausible translations of an English statement into first-order logic. This is a usual feature of translation from any language into any other language.

In order to ensure their understanding of the foregoing, which is necessary for understanding of what follows, the reader should ensure they are able to translate statements from English into first-order logic. Perhaps it is evident to the reader how to do this from what has been said. The reader is encouraged to find English statements and translate them into first-order logic. Especially if this exercise is difficult, the reader can try the provided exercises, which include answers and walk through the process of arriving at those answers.

TODO: Exercises

The reader is encouraged to try to find limits to what first-order logic can express. The reader is encouraged not to conclude that first-order logic can't express something just because the reader can't quickly figure out how to express it. [philosophy.stackexchange.com](http://philosophy.stackexchange.com/questions/tagged/logic) is one place to ask questions and find answers to already answered questions about this kind of topic, questions like "how can this statement be translated into first-order logic?" Some statements may lack any wholly satisfactory translation into first-order logic. We will explore this topic in more detail later.

TODO: Discuss general techniques for eliminating English grammar constructs not present in first-order logic

TODO: Discuss philosophy of translation into first-order logic in more depth, including limitations of translation

Let's step back and consider what we've learned so far. We set out to study the grammar of statements. We studied the grammar of statements in a version of first-order logic. We also developed a practical understanding of how to translate English statements into first-order logic. Thus, we have seen that although the grammar of English statements is very complex, for many practical purposes we can reduce the study of English statements to the study of statements in a much simpler and more precisely defined language: the language of first-order logic.

This is because for essentially every English statement, we can find a statement of first-order logic which means essentially the same thing. Almost every lesson that we learn about first-order logic will translate over to English fairly straightforwardly. But first-order logic is much more amenable to rigorous study than English is, because numerous distracting sources of non-essential complexity have been eliminated in first-order logic. Therefore the introduction of first-order logic can be expected to substantially simplify our lives in the study of statements and allow us to get further along in understanding them.

However, it would be wrong of us to carry out our study of statements only in first-order logic indefinitely. Ultimately everything we learn must be applied to English statements. Arguments are almost always made in natural languages, of which English is the example used in this text. Working in English must not be a handicap for us. As debaters we benefit from wishing to become such masters of the full complexity of the natural language(s) we use that the complexity is something we turn to our advantage rather than something that stymies us. However, practically speaking, mastery of English, insofar as English differs from first-order logic, is more a matter of acquaintance and art than it is of theory and technique. Mastery of first-order logic, on the other hand, is mostly a matter of theory and technique. Acquaintance and art come only from practice, and in this book our focus outside of the exercises is on theory and technique.

This simply means that the reader will mostly have to figure out for themselves how to apply the theory and technique of logic to natural language arguments. The reader should seek to synthesize for themselves a deep, intuitive understanding of the theory and technique of logic, and allow it to sink into their mind until it underpins their thinking even on an unconscious level. This, in my experience, is what enables the reflexive, sophisticated application of logic to natural language. 

## Speech acts

One important concept in the philosophy of language is the distinction between acts of speech, and speech acts. This distinction seems to have been first introduced in a systematic way by [J.L. Austin (1962)](http://pubman.mpdl.mpg.de/pubman/item/escidoc:2271128:3/component/escidoc:2271430/austin_1962_how-to-do-things-with-words.pdf). 

 * An **act of speech** is, in other words, the action of speaking.
 * A **speech act** is an action performed by speaking.

Here "speaking" and "speech" should be taken as inclusive of writing, and linguistic communication in general.

Examples of speech acts include:

* The "I do" of a marriage ceremony by which one agrees to marry somebody.
* The signature on a legally binding contract.
* Asserting that the sky is blue. This belongs to the category of speech acts called **assertions**.
* Denying that the sky is blue. Belongs to the category of **denials**.
* Asking somebody to pass the salt. Category of **requests**.
* Asking somebody whether they sky is blue. Category of **questions**.
* Demanding that somebody leave your house. Category of **demands**.
* Yelling "fuck you" at somebody. Category of **insults**.
* Ordering the soldiers you command to fire at will. Category of **orders**.
* Ordering a cup of coffee at a restaurant. Category of **orders** (in a different but related sense).

We could go on.

In debates, three main types of speech act are employed: assertions, denials, and questions. These three types of speech act are the ones that are primarily relevant to us. Mostly we will talk about assertion. Let's take a moment to analyze these three types of speech act in more depth.

TODO

## Truth

What is truth? What does it mean for something to be true? Here are two definitions of "truth" that I looked up on Google:

1. "the quality or state of being true"
2. "that which is true or in accordance with fact or reality"

Some definitions of "true" that I found on Google: 

1. "in accordance with fact or reality"
2. "accurate or exact."

"Truth" we can summarize, essentially, as the property of being true, and to be true is to be in accordance with fact or reality, or accurate or exact.

These dictionary definitions don't help us get to the bottom of the issue, though; they replace one term we don't understand ("true") with related terms that we also don't understand ("fact," "reality," etc.). What we really need is a *theory* of the meaning of the word "true:" a theory that explains how the term is used, which we can examine to get insight into the meaning.

Before we can develop a theory of truth, we need to gather some observations about truth. Let's start by using the words "true" and "truth," and "false" and "falsehood," in a mixture of obviously true statements, and truisms whose truth might be less clear in some cases:

* It is true that smoking causes lung cancer.
* "Smoking causes lung cancer" is a true statement.
* It is false that water is made of methane.
* "Water is made of methane" is a false statement.
* No statement is both true and false.
* Every statement is either true or false.
* What's true is true and what's false is false, independently of what people believe, say, or wish.
* Truth is something worth pursuing.
* Truth is the opposite of falsehood.

Not everybody agrees with all of these statements. Let's examine the ones that are controversial.

**No statement is both true and false.**

This truism is sometimes called **the law of non-contradiction**. This is the least controversial of our controversial statements. Almost everybody who comments on the topic agrees that a statement that is both true and false is an impossibility and would be an absurdity.

Opponents of "no statement is both true and false" are sometimes called **dialetheists**. Dialetheism is the view that there are true contradictions, i.e. statements that are both true and false (at exactly the same time, in exactly the same sense). Examples of dialetheists include Graham Priest, Jc Beall, and the author. It is not a purpose of this text to argue for dialetheism; though the author is a dialetheist, for the purposes of this text the reader will not be discouraged from regarding dialetheists as a strange group of people with a bizarre view that ought to be rejected.

Here is a typical argument in favor of dialetheism. This is an argument called the **liar paradox**. Consider the statement "this statement is false." Call this statement L. In other words, L is the statement "L is false." If L is true, then L is false, because that's what L says. If L is false, then it's false that L is false (because that's what L says), meaning that L is true (since every statement is either true or false). In short, if L is true then L is false, and if L is false then L is true. Since L is either true or false, L is therefore both true and false.

One can reject this argument in favor of dialetheism in many different ways. In the modern academic debate on dialetheism, the liar paradox is rarely presented as an argument in favor of dialetheism, because so many ways of blocking the argument are known; rather, the liar paradox, the argument itself, is taken as an object of study in the debate on dialetheism.

The usual response to the liar paradox, the response of most of the world, is what we can call the "indifferent shrug." This is a mode of response where one disregards the argument and its conclusion without providing any reason to do so.

The indifferent shrug is not necessarily an irrational response to the liar paradox, in my opinion. Life is short and there are many things to do in the world. If somebody feels that they should not devote their time to evaluating an argument with a patently absurd and yet pointless conclusion, I can't necessarily say their decision making is poor.

However, it is undeniable that the indifferent shrug is not a satisfactory response to everybody. I would therefore like to helpfully provide what I think is a better response to the liar paradox for people who don't wish to agree with me that there are true contradictions and also don't find the indifferent shrug to be a satisfactory response. However, I will not do that right now, as we need further background material first. Therefore let's move on to our next controversial statement about truth.

**Every statement is either true or false.**

This truism is sometimes called the **law of the excluded middle**. It states, in other words, that there are no statements which are neither true nor false.

Aristotle is the earliest writer I'm aware of to have asserted this law, when he wrote "there cannot be an intermediate between contradictories, but of one subject we must either affirm or deny any one predicate." (*Metaphysics*, Book IV, Part 7) 

The law of the excluded middle is a feature of the standard rules for first-order logic, and it is a popular idea among logicians that the law is true. However, it is also a fairly popular idea that the law is not true. Here are some possible examples of statements that are neither true nor false:

* "Christianity is a good religion." Whether Christianity is a good religion depends on what one thinks makes for a good religion. Some would argue there is no such thing as a good religion, and Christianity has been a net bad for the world. Others would argue that Christianity is not only a good religion, but the only good religion. There are many perspectives in between. One may argue that this multiplicity of conflicting perspectives, each with at least a grain of truth to it, means that "Christianity is a good religion" is neither true nor false.
* "Science is reliable." Reliable in what sense? Science is not reliable in the sense that following the scientific method doesn't guarantee that you will arrive at the truth, and scientists seem to make mistakes and get things wrong frequently. Science is reliable in the sense that we can rely on it to continue to provide us with new facts about the world as long as people continue to follow the method. Science is probably reliable and unreliable in other senses too. So "science is reliable" isn't a specific enough statement to be true or false.
* "Liberals are tolerant." Different liberals are tolerant of different things to different degrees. Liberals vary between each other in how tolerant they are, and individual liberals vary in how tolerant they are of different things. For example, some liberals might be tolerant of homosexuals but intolerant of religious believers.
* "The King of America is short." Of course there is no King of America, so the statement isn't true. However, if it were false, that would mean its negation was true, and the negation of "the King of America is short" is "the King of America is not short," which certainly isn't true either.
* "The [continuum hypothesis](https://en.wikipedia.org/wiki/Continuum_hypothesis) is true." The continuum hypothesis is a statement about sets which can be neither proven nor disproven on the basis of any widely accepted statements about sets. The continuum hypothesis, being a possibly unsolvable question about a type of thing, sets, that may themselves be an imagination of humans, may be neither true nor false.

It's not obviously necessary to accept any of these as genuine counterexamples to the law of the excluded middle.

* "Christianity is a good religion," you can argue, is either true or false given further definition of "good." Like any statement, it requires context clarifying the meaning of the terms in order to be true or false, but given that, it is either true or false, one can argue.
* "Science is reliable" can be addressed in the same way. Given context clarifying the meaning of "reliable," the statement is true or false, one can argue.
* "Liberals are tolerant" can be addressed in the same way.
* "The King of America is short" is not true or false, because the meaning or referent of the term "the King of America" is not clarified by the context of the world as it exists today and has existed in the past. This needs to be understood as giving us a limitation on the law of the excluded middle. As applied to our version of first order logic, the limitation is that the law only applies to statements that are interpretable in the given context. This makes our original statement of the law of the excluded middle, "every statement is either true or false," not literally true. One can, however, understand the original statement as being truncated language, a simplified statement of the law, rather than being a falsehood.
* One can certainly maintain that the continuum hypothesis is either true or false. For example, one can be a mathematicial Platonist about sets. I.e., one can maintain that there is an objective reality of sets existing in a Platonic realm outside of space-time, and that one with perfect knowledge of this reality (e.g. God) would be able to know whether or not the continuum hypothesis was true or false by observing the universe of sets.

Most of these rebuttals rely on a qualifier regarding the law of the excluded that hasn't been stated: namely, that it holds regarding statements whose meaning is sufficiently clarified by the context. Statements whose meaning is not clarified by context (e.g., "the King of America is short," in the context of politics in 2017) are not either true or false. Thus "every statement is either true or false" has attached to it the implicit qualifier "in contexts where the meanings of its terms are sufficiently clarified."

My aim is not to say that the reader should accept these arguments as showing that the law of the excluded middle is true, but merely to show that the law of the excluded middle is remarkably defensible and can be defended against all the counterexamples I can give. The reader is encouraged to form their own opinion on whether the law of the excluded middle is true.

One can reject that the law of the excluded middle applies to all statements, while maintaining that there are some limited classes of statements wherein every statement is either true or false.

When it comes to sufficiently precise statements of fact about ordinary medium-sized physical objects, such statements are always either true or false, if there is an objective reality which conforms to our ordinary common sense beliefs about physical reality, and specifically that there is a fully existent physical reality whose parts all exist in a single determinate state where every detail that could be investigated exists in a determinate state even when it's not being observed.

A person can reject the principle that sufficiently precise statements of fact about ordinary medium-sized physical objects are always either true or false, if they reject the premise that there is an objective reality whose parts all exist in a fully determinate state even when not being observed. I am not aware of any conclusive arguments for the stated premise, so there is basis for controversy about whether sufficiently precise statements of fact about ordinary medium-sized physical objects are always either true or false.

Many classes of statements are **decidable**, meaning it is possible to write a computer program which decides whether statements in that class are true or false. In other words the program takes as input a statement in the class and as output it (correctly) says either that the statement is true or that the statement is false. If a class of statements is decidable, then most would agree that every statement in the class is either true or false. The argument, essentially, is that in principle one could run the program and the program would correctly tell you either that the statement is true or that it's false; but presumably the statement was already true or false, whichever it is, before you ran the program. Therefore it's already true or false even if we don't run the program. I'm not aware of much controversy about this reasoning, but you can reject it if you're willing to say that mathematical statements don't become true or false until people prove or disprove them.

An example of a class of decidable statements are "bounded-quantifier arithmetical statements," or **BQA statements** as I will call them. TODO

Let's take stock.

Most of the time in the course of life, we are happy to assume that statements are either true or false. Most of the time assuming that a statement is either true or false will not lead us into any kind of error. However, sometimes it is inappropriate to assume that a statement is either true or false, as with the statement "Christianity is good," or "science is reliable."

It is generally safe to assume that sufficiently precise statements of fact about ordinary medium-sized physical objects are either true or false. It is generally safe to assume that mathematical statements are either true or false, because mathematicians regularly do so in the course of proofs; it is a generally accepted rule of mathematics that mathematical statements are either true or false.

However, some schools of schools of thought on math, such as most [constructivist](https://en.wikipedia.org/wiki/Constructivism_(mathematics)) schools, reject the idea that every mathematical statement is either true or false.

For example, consider some universally quantified mathematical statement, i.e. a mathematical statement of the form (for all **x**, **A**). Let's call this statement P. Let's suppose P cannot be proven constructively, and that no counterexamples to P can be constructed, i.e. P cannot be disproven constructively. Then a constructivist might say that P is neither true nor false.

Exactly how broadly we can assume that statements are either true or false is a matter of controversy. Most would agree that statements in decidable classes, like BQA statements, are all either true or false. Most think the same about sufficiently precise statements of fact about medium-sized physical objects. I believe that most mathematicians accept the law of the excluded middle as applied to math. Some philosophers think we can assume the law of excluded middle much more broadly.

The reader is encouraged to use their own judgment in applying the law of the excluded middle, and to form their own opinion on the issues I have raised if they are interested. Practically speaking, I think the reader will find the law of the excluded middle is a useful logical tool which they feel comfortable applying in essentially any real world case where it's applicable.

**What's true is true and what's false is false, independently of what people believe, say, or wish.**

This truism strikes me as basically untrue, in that what's true is dependent on what people say, and more specifically on how people talk. For example, "water is not made of methane" is true in part because of the conventional definitions of "water" and "methane." If "methane" was instead defined to mean H2O, then the statement would be false. So what's true is dependent on what people say. This reasoning works for every statement, since every statement is made up of words which need some previously established meanings for the statement to be intelligible.

What if we assume some fixed framework of definitions? Is what's true true, and what's false false, independently of what people believe, say, or wish, as long as we hold fixed that framework of definitions of words? At least when talking about sufficiently precise statements of fact about medium-sized physical objects, the truism seems basically to hold true under these conditions and in this sense and in my experience. You can form your own opinion, of course.

**Truth is something worth pursuing.**

This truism is not always accepted. In the author's opinion, looking at our experiences in life and at historical events supports the view that things are better in almost all cases when people have the truth about issues that matter.

 * When we understand the causes of diseases, we are in a position to prevent and cure them with medical science.
 * When we know the truth about the politicians who represent us, we can hold them accountable more easily and increase the likelihood that the politicians in office are good, ethical people. 
 * When we understand how the natural world works, we can make technology that allows us to farm more efficiently and produce far more food with a given amount of human labor, allowing most of the human population in developed countries to occupy themselves with activities other than agriculture.
 * When we know the truth about scientific political debates such as the climate change debate, we can make the right decisions that will be best for the future.
 * When we know the truth about what's going on in our personal lives, we can make decisions that will be more likely to strengthen our relationships and avoid future suffering.
 * When we hold values that are based on deep and thorough analysis of the world as it actually is, our values are more likely to bias us towards making decisions that will be helpful in our lives and the lives of others.

In my opinion, the consequences of pursuing and sharing the truth are almost always preferable to the consequences of attempting to conceal, evade, ignore, denigrate, distort, or escape the truth. A life lived in truth and love, in my experience, is a happy and fulfilling life. Pursuing and sharing the truth brings great joy, and it brings lessons which help one in all aspects of life and living. By pursuing the truth, one can learn the lessons of morality more deeply, and experience the fulfillment of a life well lived. Pursuing the truth in bonds of love and trust with others who share the same goal greatly amplifies the efforts of all and produces a far better outcome, in my experience.

**Summary of the truisms discussion**

We've completed our discussion of the non-obvious truisms about truth that I listed. We have certainly unearthed a bit about truth in the process. Let's take stock.

The law of non-contradiction states that no statement is both true and false. This law is endorsed by almost everybody who speaks on the subject, except for some mavericks, such as dialetheists, who think that some statements are both true and false. One argument in favor of dialetheism is the liar paradox. This argument can be blocked in numerous ways, but we have yet to discuss that. The indifferent shrug, however, seems to be a fair enough response.

The law of the excluded middle states that every statement is either true or false. The law of the excluded middle is controversial, and neither side in the debate has struck a decisive blow, in the author's awareness. The reader is encouraged to form their own opinion.

What's true is true and what's false is false, independently of what people believe, say, or wish, at least when you're talking about sufficiently precise statements of fact about ordinary medium-size physical objects, and relative to a fixed framework of definitions of words.

Truth is worth pursuing. A life of pursuing truth (and love), in the author's opinion, is the best life available on this Earth.

**What is truth? What is true?**

Earlier I wrote:

> What we really need is a *theory* of the meaning of the word "true:" a theory that explains how the term is used, which we can examine to get insight into the meaning.

Many theories of the meaning of the word "true" are based on a principle called the **T-schema**. The T-schema is a principle which people have observed about the word "true," which can be stated abstractly as follows. For any statement **A**:

> The statement "**A**" is true if and only if **A**.

For example: "water is wet" is true if and only if water is wet, and "snow is warm" is true if and only if snow is warm.

Note: "**A** if and only if **B**" means the same as "if **A** then **B** and if **B** then **A**."

Theories of truth based on the T-schema been worked on by many people, including (for example) Alfred Tarski and Crispin Wright. A simple theory of truth based on the T-schema is [Horwichian minimalism](https://en.wikipedia.org/wiki/Deflationary_theory_of_truth#Horwich.27s_minimalism), a sort of view based on the writings of Paul Horwich. Horwichian minimalism, as I will formulate it, states essentially that the T-schema explains the meaning of the word "true" by stating the fundamental rule governing the usage of the word "true."

If the T-schema is a definition of "true," then it is what we can call an "axiomatic definition." An axiomatic definition of a word defines the meaning of the word by giving some statements involving the word that are true by the definition of the word.

The T-schema is an infinite set or schema of statements that are (arguably) true by definition of the word "true," that explain how to use the word "true" (in the usage of that word defined (in the sense of axiomatic definition) by the T-schema).

A further example of an axiomatic definition is the axiomatic definition of the word "and" given by the following two rules. First, from (**A** and **B**) you may infer **A**, and you may infer **B**. Second, from **A** and **B**, you may infer (**A** and **B**).

Now we can look at the precise version of my formulation of Horwichian minimalism:

**Horwichian minimalism.** The T-schema is an axiomatic definition of "true."

I do not claim that my formulation of Horwichian minimalism is either essentially original, or the same in spirit as Paul Horwich's view, or the same as the understandings of other philosophers who have endorsed Horwichian minimalism. Nonetheless, I will henceforth use the phrase "Horwichian minimalism" to mean the view that the T-schema is an axiomatic definition of "true."

I myself believe (this formulation of) Horwichian minimalism. It is a very modest view. All it claims is that the T-schema defines a meaning for "true." It doesn't claim this is the only possible meaning for "true."

The notion of axiomatic definition is a small variation on Bob Hale and Crispin Wright's concept of an [implicit definition](http://www.st-andrews.ac.uk/arche/old/pages/papers/Implicit%20Definition%20and%20the%20A%20Priori.pdf). The concept of implicit definitions is explained in the opening paragraph to the linked paper:

> An explicit definition aims to supply a semantically equivalent expression of the
same syntactic type as its definiendum. Implicit definition, taken as the complement of
explicit, embraces a variety of subtypes. What all have in common is the idea that we may fix
the meaning of an expression by imposing some form of constraint on the use of longer
expressions—typically, whole sentences—containing it.

Hale and Wright define implicit definition as the complement of explicit definition. I disagree with their claim (in this paper) that all subtypes of implicit definition have in common the idea that we may fix the meaning of an expression by imposing some form of constraint on the use of longer expressions containing it. As a counterexample, I provide ostensive definitions, which are definitions of words made by (literally or figuratively) pointing at examples: for example, pointing at a dog and saying, "dog." Ostensive definitions are not explicit definitions, so they are implicit definitions, but they don't (in any sense that is apparent to me) fix the meaning of the words they define by imposing some form of constraint on the use of longer expressions containing them. It's possible that the category Hale and Wright defined in this paper contains more things than they intended it to. It's even possible that they made this observation and have since revised their thinking. I am not an expert on either philosopher's work, and I am merely pointing out possiblities not ruled out by the information at hand.

The category of axiomatic definitions seems to be narrower even than the category of implicit definitions in Hale and Wright's (arguably) narrower sense, of definitions which fix the meaning of an expression by imposing some form of constraint on the use of longer expressions containing it. Finding an example of a non-axiomatic definition meeting Hale and Wright's narrower condition is left as exercise to the reader. I haven't solved this exercise myself, and while it may be tricky I suspect that it is solvable. Please contact me if you come up with a solution, or a good argument that there is none. 

Hale and Wright point out some controversy about whether or not implicit definitions are a legitimate form of definition. Such controversy also applies to axiomatic definitions, which are the typical examples of implicit definitions. Personally, I see little reason for fuss. To me it's natural that you can explain the meaning of a word by giving rules that explain how to use it. You can teach somebody how to use the word "and" by teaching them to follow the rules of inference described before. And you can teach somebody how to use the word "true" by teaching them the T-schema.

Of course, the T-schema defines only one possible way of using the word "true," and people might use it in other ways.

So far I have presented one definition, sense, or meaning for the word "true." This is the meaning axiomatically defined by the T-schema (according to Horwichian minimalism). However, there are certainly other possible meanings.

For example, one might define "true" as "accurate to reality." This definition will be equivalent to the T-schema within the context of any conversation where by asserting a statement, one implicitly asserts that the statement is accurate to reality. (In my life, conversations I have at work in my professional capacity are examples of such conversations.)

As far as I am aware, any definition of "true" which tells a person how to use the word "true" and is consistent with common usage, is based on the T-schema or equivalent to it in appropriate contexts. If you find any counterexample to this statement, please tell me about it.

The T-schema by itself is not enough to explain how to use the word "true." Essentially, the T-schema references questions about truth to questions about norms of assertion. If you can rightly assert a statement, then you can rightly assert that that statement is true. Instead of asking, when is a statement true, you can ask, when can you rightly assert a statement? The latter is a question that this entire text is aimed at addressing. 

Still, some readers may feel that this misses the point. If we reduce questions about truth to questions about norms about assertion, then perhaps some readers feel we are no longer asking the questions they are interested in, because readers want to know what's true in the sense of what's real, what's actual. I don't want to convey the appearance of sidestepping these meaty questions in favor of surface questions about what we can say.

It is the author's hope that the reader can get closer to what's true in the sense of what's real and what's actual on the basis of what they are learning in this book. I do not claim to know what's real and what's actual, and therefore I cannot claim to show the reader the path to come to know what's real and what's actual. However, it is my opinion that applying the techniques of this text, taken all together and used with art, skill, and fairness, will tend to help one figure out what's real and what's actual in practical terms, in most problem domains that humans can get a grip on. No guarantees can be made that the methods we discuss will lead one to an accurate understanding of reality in any particular situation. I am unaware of any totally sure, provably infallible methods of knowledge gathering. Consistent with that, I am comfortable calling this text a method of seeking the truth. 

**Other theories of truth**

I have so far described one systematic theory of the meaning of "true:" namely, Horwichian minimalism. How does Horwichian minimalism compare to other theories of truth? I will compare it to (a) a generic postmodern theory of truth, and (b) a generic pluralist theory of truth, a la Michael P. Lynch (in a very high level sense -- I am not attempting to represent Lynch's views in any depth at all).

Our generic postmodern theory of truth runs as follows: "truth is relative." Truth is relative to a context, an observer, a point of view. Thus there are many different truths, none absolute.

Horwichian minimalism does not go as far as this postmodern theory of truth. It does not positively assert that there is no absolute truth. It is silent on whether there is such a thing as absolute truth. However, Horwichian minimalism agrees with this postmodern theory of truth as far as they both go. It agrees that truth is relative, in a sense: the interpretation of "true" is relative to a conversational context which can establish norms governing assertion and thereby determine what can be called true in that context.

Our generic pluralist theory of truth runs as follows: "there are multiple kinds of truth." Horwichian minimalism agrees with this, as different conversational contexts produce different versions of "true" which behave substantially differently. However, some pluralist theories of truth will define meanings of "true" which are different from the minimalist meaning of "true" provided by the T-schema. My view does not discourage the progress of such projects, since my view allows for the consideration of definitions of "true" other than the Horwichian minimalist definition of "true."

Horwichian minimalism is not the most informative theory of truth, and it is only a theory. That's why I'm open to other theories of truth. To me, the value of Horwichian minimalism is that it gives us one viable way of understanding the meaning of "true," of treating our confusions around this word. It references questions about truth to questions about norms of assertion. This proves in practice to be a productive way of thinking about truth. Understanding norms of assertion takes in much of what humans know about seeking the truth. It is the primary topic of the rest of this text.

## Epistemology

So far we have discussed to some extent the "what" of truth. We have yet to discuss the "how." How does one get to the truth? For some readers, I imagine that understanding how to seek the truth is their primary aim in reading this text. I promised in [the "how to read this text" section](#how-to-use-this-text) that you can use this text "as a practical guide to winning arguments, to reasoning about any subject, to seeking the truth about any subject, and to misleading people about any subject." I have therefore promised to discuss seeking the truth.

This is an extremely hairy topic. To get ourselves started, let's pick up some threads that were left hanging in the previous section and notice some puzzles we haven't yet discussed.

In discussing the truism that truth is something worth pursuing, I argued that truth is valuable and indeed worth pursuing. On the other hand, in discussing the word "true," the only explanation of its meaning that I offered was the T-schema and the theory of Horwichian minimalism.

Recall that the T-schema is the infinite set of statements of the form "the statement **A** is true if and only if **A**," for all statements **A** (in some language, e.g. English). Recall that Horwichian minimalism is the view that the T-schema is an axiomatic definition of "true." Recall that an axiomatic definition of a term defines the meaning of the term by giving some statements involving the term that are true by the definition of the term.

Thus, we have two related and yet very different conceptions: the conception of *truth* as a value, and of *true* as an axiomatically defined, context-sensitive, meta-linguistic term that is used to describe statements.

Intuitively, the meanings of *truth* and *true* should have a lot to do with each other, but perhaps it's difficult (it was for me) to see right away how the conceptions of *truth* and *true* I have provided are at all related.

As it happens, we don't need to look terribly deep to see the connection. Horwichian minimalism, as we have observed, lets us reference questions about truth to questions about norms of assertion. Therefore, it follows from Horwichian minimalism that questions about truth can be construed as normative questions about how we should behave (namely what speech acts, specifically what assertions and denials, it is warranted for us to perform).

What is the relationship between the value of truth, and norms of assertion? Norms of assertion, like other norms, are often rooted in values. The value of truth is a simple example. The value of truth undergirds the Gricean maxim of quality (a basic norm of assertion), "where one tries to be truthful, and does not give information that is false or that is not supported by evidence."  

There are values besides truth that undergird our norms of assertion. For example, consider Socrates, who seems to have valued critical, unblinking inquiry into the truth about essentially all matters, but is also on record as fully and uncritically endorsing the state religion of Athens, which residents of Athens were not legally allowed to criticize. It's possible that Socrates genuinely believed in the state religion of Athens. Let's suppose, though, for the sake of argument, that Socrates did not believe in the state religion of Athens. In this case, Socrates was self-censoring, misrepresenting his own views, in order to avoid being put to death (something which he was ultimately unsuccessful in avoiding). In this case, Socrates was basing his norms of assertion on something other than the same values behind his valuing of truth. In this case, Socrates was additionally basing his norms of assertion on the laws of Athens and his desire not to break them.

I think most people base their norms of assertion on values apart from and in addition to their value of truth. Probably most people today live in circumstances where the consequences of saying certain true things in certain contexts in their lives are unacceptable to them. I do think there are people who don't believe anything they're unwilling to say, and don't say anything they don't believe, but I think they're in the minority, and I don't count myself among them.

Is it true, then, as I ventured, that often the values undergirding a person's value of truth, and the values undergirding the norms of assertion they consider correct, coincide? We've concluded that typically they come apart to at least a small degree; any individuals who are exceptions to that rule are unusual.

From a Horwichian minimalist perspective, there is some difficulty in seeing how these two can come apart, since Horwichian minimalism does not allow for much space between truth and assertibility. Horwichian minimalism certainly does not say that there can't be true things we can't say in a given context. It merely entails that in any context where you could appropriately assert a statement **A**, you could also appropriately assert the statement "**A** is true," and vice versa.

What does it mean, though, for a statement to be true and not appropriately assertible in a given context, under Horwichian minimalism? I would explain this as follows. Suppose that I consider the statement **A** to be true, but I don't consider it appropriate to assert **A** in some context (or perhaps all contexts) in my life. In the context of my internal dialogue, the conversation I am continually having with myself in my head, the statement **A** is appropriate to "assert." In some (or perhaps all) interpersonal conversational contexts that I happen to encounter, though, **A** is not appropriate to assert.




We can perhaps place people on a spectrum, with people on one end who base their norms of assertion little or not at all on their value of truth, and people on the other end who base their norms of assertion entirely or almost so on their value of truth. We can call the people on the former end "insincere," and the people on the latter end "sincere." Sincere people believe what they assert is true, at the time they assert it; insincere people don't necessarily. Sincere people believe what they deny is false, at the time they assert it; insincere people don't necessarily.

Sincere people can exhibit a wide variety of different beliefs, behavior, and styles of conversation and debate. I think radical fanatics, for example, are usually sincere. Sincerity is certainly not a sufficient condition for goodness.

Differences between different sincere people can have to do, among many other things, with differences in the values that give them the sense of valuing truth. For example, if a Catholic sincerely asserts that they value truth and that truth comes from the Catholic church, then they are actually stating a political thesis, and this political thesis is inextricable from their personal conception of truth. The same is true of a Protestant who sincerely asserts that truth comes from our individual faculties of perceiving truth, and that truth doesn't come from appeal to authority. The Protestant is also stating a political thesis, though of a more negative nature: namely that truth doesn't come from appeal to authority. Again, this political thesis is inextricable from this Protestant's personal conception of truth.

When two people say "I value truth," they don't necessarily both mean the same thing. There are many reasons that people value truth, and many values undergirding people's valuing of truth. People value truth for practical reasons, because it helps them accomplish other purposes they have. People also value truth for itself. What valuing truth for itself seems to mean, for most people, is something like having a concern for reality, for the actual, for facts, for evidence, and for knowledge, and for having access to these things as an end in itself, a sort of existential end in life. I count myself in the camp of people who value truth for itself in this sense.

That brings us to epistemology. "Epistemology," in mainstream philosophical usage, is a long word for the theoretical study of knowledge. Epistemology asks, essentially, what is knowledge, and how can we get it? Epistemology is most often studied by people who value truth and knowledge as ends in themselves, and undoubtedly the field is biased by that fact.

The approximately 2,500 year history of the field of epistemology (as known to us through the historical record) is colored throughout by a dichotomy between long-standing pessimism about the possibility of humans acquiring knowledge on the one hand, and on the other hand the over-optimistic intellectual arrogance manifested in philosophical project after philosophical project by generation after generation, who purported again and again to finally deliver the human species into knowledge.

What is knowledge? One long-standing, once-popular theory goes back to Plato's Socrates, in the *Theaetetus*. This theory states that knowledge is justified true belief. In other words, for all statements **A**, I know **A** if and only if **A** is true, I believe **A**, and my belief in **A** is justified.

This theory of knowledge is widely believed to have been debunked by [Gettier (1963)](https://academic.oup.com/analysis/article-abstract/23/6/121/109949/Is-Justified-True-Belief-Knowledge?redirectedFrom=fulltext). Quoting [Wikipedia](https://en.wikipedia.org/wiki/Epistemology#Gettier_problem):

> One of the cases involves two men, Smith and Jones, who are awaiting the results of their applications for the same job. Each man has ten coins in his pocket. Smith has excellent reasons to believe that Jones will get the job and, furthermore, knows that Jones has ten coins in his pocket (he recently counted them). From this Smith infers, "the man who will get the job has ten coins in his pocket." However, Smith is unaware that he also has ten coins in his own pocket. Furthermore, Smith, not Jones, is going to get the job. While Smith has strong evidence to believe that Jones will get the job, he is wrong. Smith has a justified true belief that the man who will get the job has ten coins in his pocket; however, according to Gettier, Smith does not know that the man who will get the job has ten coins in his pocket, because Smith's belief is "...true by virtue of the number of coins in Jones's pocket, while Smith does not know how many coins are in Smith's pocket, and bases his belief...on a count of the coins in Jones's pocket, whom he falsely believes to be the man who will get the job." These cases fail to be knowledge because the subject's belief is justified, but only happens to be true by virtue of luck. In other words, he made the correct choice (in this case predicting an outcome) for the wrong reasons. This example is similar to those often given when discussing belief and truth, wherein a person's belief of what will happen can coincidentally be correct without his or her having the actual knowledge to base it on.

My understanding of the current state of [the academic debate on what knowledge is](https://philpapers.org/browse/knowledge) is that many proposals are out there, and many proposed counterexamples to proposals are out there, but no consensus has formed around an alternative conception of how we should define knowledge. The debate is huge, extremely intricate, and, again, totally unresolved. This, in turn, makes it hard to make progress on the debate on whether humans can obtain knowledge.

I don't claim to have the ability to obtain knowledge. I don't claim that nobody can obtain knowledge, but I also don't know how anybody would. This text isn't meant to break the stalemate in the debate on whether we have knowledge. I make no pretense to be able to resolve the debate, and I make no pretense to know what knowledge is, to have knowledge, or to offer the possibility of knowledge.

At this point some readers may be confused about why I am so hesitant to claim the capability of knowledge. It's worth clarifying the factors that I see as blocking me from making such a claim.

First and most important is that I don't know what knowledge is. I probably can't know that something is knowledge if I don't know what knowledge is. Yet most of the time we assume that if you know something, then you know that you know it, you know that you know that you know it, and so forth. Since I don't know what knowledge is, I may know stuff, but if so I don't know that I know it.

Though I don't claim literally to possess knowledge and to know that I possess it and so forth, I will in practical contexts assert that I know something. Generally I do so when I am confident that I can defend the claim. If pressed specifically on my use of the word "know," then I would substitute a weaker term (though my word choice is rarely the key issue in non-philosophical conversations).

My lack of clarity around how to define or use the word "knowledge" is one difficulty I have around claiming possession of knowledge. However, it's not my only difficulty.

The other difficulty is that my experiences are consistent with a lot of different things being true. For example, I could be a [brain in a vat](https://en.wikipedia.org/wiki/Brain_in_a_vat) experiencing a simulated reality created by attaching my brain to computers that fire appropriate signals at my brain to make it seem like it's in a human body on the planet Earth. My experiences are entirely consistent with this possibility. Maybe the brain in a vat thought experiment is metaphysically impossible, somehow; but as far as I know, it is metaphysically possible.

My experiences are consistent with the possibility that the whole world has only existed for five minutes, and all of my older memories were implanted in me by whatever mischevious God created this universe. 

My experiences are consistent with the possibility that I am the only conscious person in the world. I believe that others are conscious, but I don't see how I would know. I believe that if there is a God, then God could create elaborate holograms that are exactly like the world I experience in every way, full of totally realistic people, but in which I'm actually the only conscious person. I don't believe I know that this is not the case.

Similarly, my experiences are consistent with the possibility that parts of the physical world only exist while I'm looking at them.

I don't imagine that everybody will find these skeptical arguments compelling. If the reader is still convinced after what I've said that they know things, then that's OK. My purpose is to teach how to win arguments and seek the truth, not to persuade the reader that they don't know anything.

We are left in an apparently awkward position. We have committed ourselves to a positive project of learning how to win arguments and seek the truth about any topic. However, I have just denied any ambition in this text of developing knowledge. How can we reconcile these seemingly contradictory positions?

The answer to this lies in the distinction between *seeking knowledge* and *seeking the truth*. To seek knowledge is to seek to know things. To seek the truth is to seek to have true beliefs, or perhaps even just true thoughts that one doesn't necessarily believe.

It's widely agreed that knowledge is some condition stronger than justified true belief. If I know *X*, most Western philosophers would agree that I therefore have a justified true belief that *X*. Thanks to Gettier, few would agree that the conditional goes the other way. All knowledge is justified true belief, but some justified true beliefs are not knowledge. Knowledge is a condition stronger than justified true belief.

A condition that's weaker than justified true belief is plain true belief. As somebody who doesn't know how to get knowledge, I aim to believe things that are true, by hook or by crook, doing whatever I think is most likely to work to get me true beliefs. I also aim to have my beliefs be justified whenever possible and desirable, which is most of the time. My belief is that good processes for arriving at beliefs will tend to also produce good justifications for those beliefs. TODO: Should this be strengthened: should beliefs always be justified? Whether or not good processes for arriving at beliefs always produce good justifications for those beliefs depends on what "good" means in the given context, and on how one is construing the meaning of "justification."

So far we have considered a series of progressively weaker conditions: knowledge, justified true belief, and true belief. A condition that's even weaker than true belief is plain truth. I find value in thinking of the truth, even if I don't end up believing it or knowing it. One of the great lessons of philosophy, in my opinion, is the value of speculation. Speculation increases one's likelihood of setting one's eyes on the truth. Setting one's eyes on the truth is step one. It needs to come before believing, knowing, or being able to justify the truth. 

Whenever there's an event, any news coverage that comes out is subject to various degrees of uncertainty, and it's possible to imagine multiple possibilities regarding the truth of the matter. In any news report, it's possible the journalist simply made something up. If a number of different news organizations are reporting the same thing, then either it really happened, or they are in a coordinated conspiracy to make something up, or some of them are making something up and others are listening to the ones making it up. When news coverage is sincere, it can still be inaccurate. When news coverage is accurate, it can still be biased towards a particular political or other type of perspective. There are many layers of interesting interpretation that can be applied to news coverage. Especially when it comes to reporting on events like wars where intelligence can be unreliable and motives for deception run high, often there are many interpretations of what really happened that are consistent with the known facts.

Maybe the reader has found this section and the preceding one on Truth to be climactic; or maybe the reader has found them anti-climactic. I have essentially argued that the field of epistemology, insofar as it has been conceived of as a positive project of finding and verifying the means of obtaining knowledge, has not advanced one step from where it began 2,500 years ago. I do not deny the possibility that future progress in epistemology will deliver us into knowing, knowing that we know, knowing that we know that we know, and so forth. But my own opinion is that, like [cold fusion](https://en.wikipedia.org/wiki/Cold_fusion), we haven't achieved that.

Let's close out our discussion of knowledge with the enjoyable and yet perhaps also uncomfortable opening paragraph from [*On Truth and Lies in a Nonmoral Sense*,](http://pastehtml.com/view/crz4xb2u4.html) [by Nietzsche](https://en.wikipedia.org/wiki/On_Truth_and_Lies_in_a_Nonmoral_Sense).

> Once upon a time, in some out of the way corner of that universe which is dispersed into numberless twinkling solar systems, there was a star upon which clever beasts invented knowing. That was the most arrogant and mendacious minute of ”world history,” but nevertheless, it was only a minute. After nature had drawn a few breaths, the star cooled and congealed, and the clever beasts had to die. One might invent such a fable, and yet he still would not have adequately illustrated how miserable, how shadowy and transient, how aimless and arbitrary the human intellect looks within nature. There were eternities during which it did not exist. And when it is all over with the human intellect, nothing will have happened. For this intellect has no additional mission which would lead it beyond human life. Rather, it is human, and only its possessor and begetter takes it so solemnly-as though the world’s axis turned within it. But if we could communicate with the gnat, we would learn that he likewise flies through the air with the same solemnity, that he feels the flying center of the universe within himself. There is nothing so reprehensible and unimportant in nature that it would not immediately swell up like a balloon at the slightest puff of this power of knowing. And just as every porter wants to have an admirer, so even the proudest of men, the philosopher, supposes that he sees on all sides the eyes of the universe telescopically focused upon his action and thought.

In this text, we're abandoning the topic of knowledge henceforth. However, in my view this does not call for an abandonment of epistemology; instead, in my view it calls for a different formulation of the purposes of epistemology. I will therefore repurpose the term "epistemology," using it to refer to the study of seeking the truth. This is a general umbrella of topics which for us includes truth, true belief, justified true belief, and similar "epistemic" concepts.

With this slightly different conception of epistemology in hand, let us now return to the starting point of our investigation of epistemology.

In essence, we are interested in studying and evaluating potential methods of seeking the truth. We want tools and techonology that enable us to get to the truth. We want tools and technology that work, that actually achieve this purpose. Therefore we want to evaluate various methods of getting to the truth, and to be able to discriminate between effective and ineffective ones.

However, in order to grab the problem by the roots, we need to step back a bit. I noted that there are certain values underlying a truth-seeking attitude or an attitude of sincerity. I also noted that exactly what values are involved will vary between individuals. One way of putting it is to say that truth-seeking attitudes or attitudes of sincerity are like people themselves, in that no two are the same, yet they share uncountable and sometimes indescribable differences and similarities with each other.

As the author, my objective is to help the reader to water their own truth-seeking and their own sincerity, to let it grow in whatever shape is appropriate to it. My objective is not to replicate my own self in others. As such, I can't tell the reader what should be involved in their own attitudes of truth-seeking or sincerity. I also can't choose for the reader whether or not to grow their attitudes of truth-seeking and sincerity at all.

At core, the attitude of truth-seeking is a striving for reality, for what's actual, as I have noted a few times. We can't say this goal necessarily makes sense on the basis of what I've said. In seeking after reality, one can argue that if we knew what we were looking for or how to get there, then we wouldn't need to be seeking.

The experience of philosophy can be compared to groping in a dark system of passageways looking for a way out. Occasionally, by some good fortune or cleverness, one finds one's way out into a brightly lit clearing where one can see many more dark, unexplored passageways terminating. One can continue spelunking indefinitely in this way, finding ever larger clearings with ever more passageways leading out of them. One can rightly wonder whether one will eventually be able to map and explore the entire system of passageways and clearings. One can also rightly wonder whether it's possible to get out of the system of passageways and clearings entirely, and if so what lies beyond it. Many philosophers have thought they had left the system of passageways and clearings entirely, only to discover that they were merely in a very large clearing.

This metaphor is an elaboration of Plato's [allegory of the cave](https://en.wikipedia.org/wiki/Allegory_of_the_Cave). In Plato's allegory, people who were trapped in a dark cave, forced to watch illusions projected onto a wall, are one day let out into the outside world where they get to see the sunlit day, where the sunlight is compared to the light of truth. The passageways and clearings metaphor points out the great joke of philosophy, which is that what we find in philosophy is that the bright sky you emerge out into from Plato's cave later turns out to be just a clearing. 

Is it illusions all the way up? Perhaps. It doesn't really matter, from the perspective of this lifetime. Even if there is truly a way out of the system of passageways and clearings into the daylight of truth, I have no great expectation of reaching it in this lifetime.

The pursuit of philosophy, done boldly and with integrity, shatters many illusions. Behind them, I am happy to say, we find other illusions: but brighter, bigger, more open illusions.

Can humans exist without illusion? Perhaps not. If we can't have knowledge, nonetheless there needs to be a fabric to our psychological lives. What else but illusion, or what we can more charitably call opinion? Plausibly, as Nietzsche suggested in On Truth and Lies in a Non-Moral Sense, "we produce these representations in and from ourselves with the same necessity with which the spider spins." (This quote was paraphrased in [this Partially Examined Life podcast](https://www.youtube.com/watch?v=WA2-s8TJrPc&t=31s), which is how I learned about it.)

To phrase the point more charitably, humans need to form opinions about the world; we make them on the basis of flawed information; and they are therefore subject to revision and update.

This point is a truism when said this more charitable way. I first presented it in the most extreme formulation I could in order to make as clear as possible where we should look for interesting implications of what's being said and for potential objections to the theory.

The general weakness of what we have said so far is that it is overly wiggly and relativistic, and overly abstract, lacking in practical applicability. Our epistemology so far has many appropriate degrees of freedom. But, it is like a helpless infant, not yet useful for anything. This is because it lacks constraints. We need to figure out how constraints get into the picture.

Let's take stock of the degrees of freedom we gathered into our epistemology in the preceding section.

* It is not agreed what knowledge is.
* It is not agreed whether humans have knowledge.
* Arguably, we might know stuff without knowing that we know it, and knowing that we know that we know it, etc.
* Instead of knowledge, for various purposes we might settle for justified true belief, or mere true belief, or merely setting eyes on the truth.
* There are many ways one can understand the meaning and value of "truth" and "true." The interpretation of these words is relative to the context.
* Questions about truth can be understood as normative questions by referencing them to norms of assertion.

Those are some of the degrees of freedom that seem to exist in epistemology based on what we've said. Now let's try to identify some of the epistemic constraints under which humans operate. Most of the time it is epistemic constraints, and not epistemic degrees of freedom, that we really care about. The reason for this is that epistemic constraints enable us to say what is not the case, providing a type of certainty that we can use to solve practical problems.

Let's do a thought experiment to introduce the notion of epistemic constraints. Imagine that your individual conscious mind was in total control of its own experience. This would be the case for an individual consciousness that was the entirety of the universe it was in, that therefore experienced nothing other than itself and its own self-created imaginations and illusions. If such a consciousness was like our own, we can imagine that it would readily discover itself to be in total control of its reality, and that it would experience whatever it pleased forever. Let us call such a consciousness a **totally isolated consciousness**.

It's conceptually plausible, in my opinion, that such a consciousness, without spontaneous experience based on external forces, might not ever form any ideas, feelings, or other experiences apart from the experience of self-awareness of a self which is nothing but bare awareness of awareness. On the other hand, we can also imagine a totally isolated consciousness with the possibility of imagination.

If a totally isolated consciousness had the possibility of imagination, then it could create realities within itself. These realities, though imaginary, would be just as real as any other part of the universe that the totally isolated consciousness was. The consciousness, however, would presumably always be aware that these creations were its own and could be gotten rid of or replaced whenever it wanted. This consciousness would therefore have near-total epistemic freedom, but it might lack the epistemic freedom to believe in the experience of a reality outside itself that it was unable to control.

The only way I can think of that a totally isolated consciousness could gain this freedom would be if it were able to trick parts of itself into losing awareness of its membership in the rest of itself. Those parts of itself could interact with the rest of itself, mistaking the rest of itself to be some kind of external reality. Let us call such a part of a totally isolated consciousness a **separated sub-consciousness**.

Our experience as humans is quite different from the experience of a totally isolated consciousness, but analogous to the experience that a separated sub-consciousness of a totally isolated consciousness might have.

As humans, we have the experience of forces outside ourselves that make our own subjective experiences not entirely subject to control by our individual conscious minds. We have the experience of an enormous variety of different things and the ability to actualize an uncountable number of distinct possible futures, but not the ability to decide completely the contents of our experiences.

This, in my opinion, is basically where constraints in epistemology come from. A totally isolated consciousness would, I think, be essentially epistemically unconstrained, assuming it had the abilities of imagination and of forming separated sub-conciousnesses. A totally isolated consciousness without imagination might be nothing but a bare consciousness of consciousness. A totally isolated consciousness with imagination but without the ability to form separated sub-consciousnesses might have the epistemic constraint that it would be unable to believe in a reality outside its control.

Humans are not like totally isolated consciousnesses. We have epistemic constraints, and they come from the fact that we don't completely control our own experiences. We explain the uncontrolled part of our experiences by the theory that there is an external physical world populated by people much like ourselves, who also have consciousnesses and internal experiences. I don't think we know this, but it is nonetheless the fundamental theory underlying almost everything we do, and I don't propose to stop using the theory to navigate my experiences. I don't know there is an external world populated by people much like myself who also have consciousnesses and internal experiences. I do, however, operate under these assumptions, believe said assumptions, and assert that said assumptions are true.

More generally, let **X** be some simple statement about the ordinary world of common sense which I have clear evidence for: e.g., "there is a pen on the desk I'm typing at." I operate under the assumption that **X**, believe that **X**, and consider it warranted for me to assert that **X** is true, but I don't claim to know **X**.

What is the word "warrant" that I have used a couple of times now, most recently in the previous paragraph? As I'm using it in this text, "warrant" is a placeholder for whatever standard of assertibility is appropriate to a given context. Relative to a given conversational context, "it is warranted to assert **X**" means that asserting **X** is a good move in the game which is the given conversational context.

The concept of warranted assertion, understood in this way, takes in more than justification, evidence, or proof. It also takes in general concerns of appropriateness, such as relevance and politeness. It takes in every positive and negative force within the ambient social game which weighs on the decision to make or not make the assertion. Warranted assertion, understood in this way, is a highly situation-specific concept. In this conception, there is no general standard of warranted assertibility; there are only different standards appropriate to different situations (and in different people's opinions).

Though there is no general standard of warranted assertibility, there are things that should generally be in a standard of warranted assertibility. There are techniques of reasoning that have been observed to work generally, across all or many problem domains. There is a simple argument for accepting the use of these techniques of reasoning to govern norms of assertion when they are applicable: namely, that it works. 

In the rest of this text we are going to spend a lot of time thinking about what should generally go into our concepts of warranted assertibility. What rules of logical inference should play a role? What rules of probabilistic and inductive reasoning should play a role? What else should play a role? In each case, the questions are, how widely does it work, and where does it fail? Our goal is to build a toolkit of tools of reasoning, tools for governing norms of assertion, tools for advancing debates, which work across all or many problem domains.

## Meaning

What is meaning? We discussed this question briefly in the section titled Language. Let's recall in summary what I said back there.

I drew a distinction between three types of meaning: speaker meaning, listener meaning, and sentence meaning. Speaker meaning is what a speaker intends to mean by a sentence in a particular instance. Listener meaning is what a listener takes a sentence to mean in a particular instance. Sentence meaning is what a sentence means in itself according to the conventions of a language. 

In the field of formal semantics, people give mathematical definitions of sentence meaning for formal languages. The same kind of analysis can be done semi-formally but not exactly or comprehensively with natural languages. I pointed out that it is not feasible to give a definition of English sentence meaning that works across the board, because of the complexity and variability of English language use. We can still usefully talk informally about "the conventional notion of English sentence meaning," even though the concept breaks down after a certain point.

I pointed out a competition between two general approaches to understanding linguistic meaning: psychological approaches, and non-psychological approaches. 

The problem with psychological approaches that I pointed out in the section on Language is that (arguably) psychology is mostly inscrutable. I think this is the biggest reason for the attractiveness of non-psychological approaches to understanding meaning.

In my opinion, the non-psychological approaches to the philosophy of linguistic meaning are off track in that they have failed to ask the right question. For me, linguistic meaning is by definition a kind of phenomenon that occurs inside human minds. I would compare a meaningful linguistic utterance to a key that opens some door in an appropriate listener's mind. A lot of non-psychological approaches to understanding linguistic meaning, in my opinion, can be compared to attempting to figure out what's behind a door by inspecting the shape of the key that opens it.

In summary, I take a psychological approach to the philosophy of linguistic meaning, because linguistic meaning is a phenomenon in the human mind.

More specifically, I take a phenomenological approach to the philosophy of linguistic meaning. This means that the basic objects of study in my theory are experiences of meaning caused by language. Since my own experiences are the only experiences I can directly observe, those are the basic objects of study that I have access to.

According to my theory, **a linguistic meaning** is an experience causing production of or caused by consumption of a linguistic utterance in a particular instance. The **speaker meaning** of an utterance is the meaning which the speaker of the utterance intended to convey. For each person who hears a given utterance in a particular instance, there may be zero or more **listener meanings**. A listener meaning of an utterance is any meaning which the listener takes from the utterance.

There are different kinds of listener meanings. In the most typical case of successful literal communication, there is one listener meaning, and it is the same (or nearly enough) as the speaker meaning. A listener may form no listener meanings, e.g. if they aren't paying attention or don't understand. A listener may form multiple conflicting ideas of what the speaker might have meant (a situation in which a follow-up question is usually recommended). A listener meaning may be a misunderstanding, where the listener forms a meaning and falsely assumes that it is the speaker meaning. A listener may also form a misunderstanding which they recognize to be an obvious misunderstanding, but which comes to mind nonetheless, e.g. perhaps because it is humorous. These are not the only kinds of listener meanings.

Speaker meanings can also involve complexities such as double meanings, sarcasm, and irony. When a speaker intends to convey a double meaning, it seems fine to me to say either that there are two speaker meanings, or that there is one speaker meaning with two aspects or layers.

The foregoing gives us a basic theory of what meaning is and how to study it further. According to the theory, linguistic meanings are experiences pertaining to utterances, and they are of two types: speaker meanings and listener meanings.

**Communication** is basically the attempt to coordinate speaker meanings and listener meanings in such a way that we achieve something we call shared understanding. This involves, among other things, a level of agreement on what statements are true and false, and the appearance of meaning the same things by words and sentences. 

There are many aspects to communication, and I encourage the reader to pursue education in communication beyond this book. TODO: Where?

A model of communication that I like is the [Shannon-Weaver model of communcation](http://ieeexplore.ieee.org/document/6773024/?reload=true&tp=&arnumber=6773024). According to this model, communication involves the following components:

 * The **sender**: a person, let's say.
 * The **receiver**: a person' let's say.
 * The **channel**: the medium of communication. E.g., the atmosphere.
 * The **encoder**: the technology used to create a transmission through the channel. E.g., the human vocal system.
 * The **decoder**: the technology used to receive a transmission through the channel. E.g., the human ear.
 * The **noise**: entropy which degrades transmissions through the medium. E.g., ambient background noise in the atmosphere.

According to the Shannon-Weaver model, communication always involves two-way **feedback**. Feedback is a process by which the sender+encoder and receiver+decoder coordinate messages in a way that allows them to verify to their satisfaction that shared understanding is occurring.

Solving difficult communication problems often requires using new kinds of efforts to achieve communicative feedback. One simple way of achieving communicative feedback is to repeat back what somebody said to you in your own words, to verify you understand. Requests for clarification are another simple form of feedback. Body language and facial expressions also provide feedback: e.g., teachers often rely on students' facial expressions for feedback on whether they are understanding the material. One of the most complex forms of communicative feedback is the process of philosophical debate. This is a form of feedback which one must sometimes employ in order to coordinate communication on deep or difficult-to-articulate problems.

Here's a way we haven't yet sliced the set of meanings. Some meanings are explicit, and some are implicit. For example, suppose my boss asks me, "could you schedule a meeting to discuss X?" Explicitly, he is asking me whether I am capable of doing something. Implicitly, he is requesting that I do something. This sentence therefore has an **explicit meaning** (querying a capability) and an **implicit meaning** (requesting an action).

An implicit speaker meaning we can also call an **implied meaning**. An implicit listener meaning we can also call an **inferred meaning**.

What is behind this distinction between implicit and explicit meanings? What is an explicit meaning? What is an implicit meaning? And how is the concept of explicit meaning related to the concept of sentence meaning, or conventional literal meaning? Are explicit meaning, sentence meaning, and conventional literal meaning all the same concept by different names, or are they different concepts?

Explicit meaning and sentence meaning are not the same concept. As you can see by inspecting the name, sentence meaning is a concept that applies only to the meanings of sentences. On the other hand, any piece of language can have an explicit meaning (including paragraphs, words, etc.). 

For the same reason, conventional literal meaning and sentence meaning are not the same concept.

Are explicit meaning and conventional literal meaning the same concept? I think so. As of yet I see no distinction. The concept of explicit/conventional literal meaning is, to recall, a fuzzy concept which apparently won't ever practically be fully detailed or fully well defined.

That brings our textual exploration of meaning in this text to a close. In the phenomenological theory of meaning which I endorse, the primary way of studying meaning is introspectively, studying the range of experiences that are possible to one's mind and understanding how experiences correlate to language in oneself and others. This is mostly a solitary pursuit, in my view, because of the impossibility of finding experiences of meaning outside one's own consciousness, and the difficulty of talking about the fine shadings and qualities of experience. I will always talk about this stuff as much as I can figure out how.

The theory I've laid out is not very novel and it doesn't make a lot of exciting claims. I'm sure that it doesn't do a lot of the work that other theories of meaning aim to do. This theory is compatible with adopting other theories of meaning when they serve some useful purpose. For example, in the section titled Logic, I will develop a traditional theory of the meaning of statements of first order logic, for the purpose of explaining and justifying the principles of classical logic. That theory doesn't contradict or serve the same purpose as the theory of this section.

The purpose of the theory of this section is to provide a high-level philosophical perspective on meaning, and to short-circuit various philosophical conundrums having to do with meaning that might beset us without the theory of this section in hand. According to this section, meaning is by definition a subjective, non-observable phenomenon. Meaning is what we hope to convey with language. We attempt to verify we have conveyed our meanings via the process of communicative feedback. We cannot in any case verify to perfection that we have conveyed the meanings we hoped to convey. Communication is a stumbling, blindfold dance of coordinating meanings.

According to this, we cannot reasonably expect to have a comprehensive and rigorous theory of meaning. If meaning is a psychological notion, and psychology is mostly inscrutable, then meaning, too, is mostly inscrutable. This doesn't follow necessarily; an inscrutable subject can have scrutable parts. I would merely argue that the study of meaning is not separable from the general study of psychology. This is to say that one needs a general, rigorous theory of human psychology as a prerequisite for a general, rigorous theory of meaning. I won't provide either kind of theory here, but if you come across such a theory (of meaning or of psychology) then I'm interested in hearing about it.

## Rationality

What is rationality? "Rationality," being a nebulous, imprecise term, can mean many different things. However, our goal here is not to explore everything the word could or should mean. The goal is to develop a usable, practical understanding of rationality.

It is not productive to try to answer question "what is rationality?" by coming up with a dictionary definition of rationality, a sentence of the form "rationality is..." Rationality is too complex a concept to explain with a dictionary definition.

Here is a first pass attempt at explaining rationality in practical terms. Rationality is in part a set of practices people employ in solving problems. Rationality is involved in the processes we use to organize ourselves into complex societies, develop technologies which improve our standard of living, cure diseases, discover the truth, and solve resource distribution problems (through systems such as regulated capitalist, socialist, and mixed-model economies), among many other things.

Given the immense practical importance of rationality, we don't want to approach the understanding of rationality as a purely theoretical problem. We want to approach it as a practical problem, so that our solution will be as useful as possible to as many people as possible. My goal here is not to develop the most rigorous theory possible, but merely to give the most usable, and yet correct explanation of rationality I can.

One can study rationality from a psychological perspective, by thinking about people's inner states, primarily their processes of belief formation, and by making prescriptive recommendations about how to form beliefs in order to be rational. 

I think it's very informative to think about rationality by thinking about the kind of moral virtues that a person should seek to have in order to be rational. Here some moral virtues that I think of as contributing to or being requirements for rationality (in no particular order):

* Fairness
* Even-handedness
* Patience
* Attention to detail
* Open-mindedness
* Skepticism
* Skepticism of authority
* Self-confidence
* Love for learning
* Skill at listening
* Skill at articulating
* Honesty
* Thirst for truth

Rationality is not a natural state of the human mind. It's a state which is achieved through hard and determined effort towards the right kinds of goals. It's a state which is never perfectly achieved. One is never finished in the development of one's rationality. It's a life-long learning experience, a path not a destination.

The heart of rationality I also call the Tao of rationality. The Tao of rationality is a mystery, a placeholder for our lack of comprehensive understanding of our own ability to reason, consequent of our minds' present inability to comprehend themselves in any great depth.

The Tao Te Ching purports to contain ancient wisdom applicable to every kind of problem. In this case we're mining it for wisdom about the practice of rationality. I'll give you a passage, and then I'll tell you what it brings to mind for me.

    The tao that can be told
    is not the eternal Tao
    The name that can be named
    is not the eternal Name.

    The unnamable is the eternally real.
    Naming is the origin
    of all particular things.

    Free from desire, you realize the mystery.
    Caught in desire, you see only the manifestations.

    Yet mystery and manifestations
    arise from the same source.
    This source is called darkness.

[From Chapter 1 of [Stephen Mitchell's translation of the Tao Te Ching](http://acc6.its.brooklyn.cuny.edu/~phalsall/texts/taote-v3.html).]


Human ability to reason outruns our ability to understand our processes of reasoning on a second-order level. Reasoning is a capability of our minds. Our minds are very complex and our understanding of them is very rudimentary.

Our processes of reasoning are the outcome of a process of biological and social evolution, consisting of the Darwinian evolution of our brains and the transmission and evolution of intellectual DNA forward through history.

Our rules for reasoning are essentially encoded in our languages. Our languages are complex organs with many subdivisions such as mathematical language and technical jargon of various fields, which are outposts dotted around a complex, generalized "everyday" language.

The most important wisdom about reasoning is contained in the rules governing our everyday languages. Good argumentation is mostly a matter of using everyday language in a way which adheres rigorously to common sense best practices of clarity and thoroughness. In other words, deeply understanding common sense about how to use language when attempting to discern the truth is the most important aspect of the development of rationality.

Each of us is much more hidden than we are visible to ourselves and each other, as most of our functioning goes on unobserved inside our bodies and our unconscious minds. Our rationality is a still pond which runs deep. On the surface it is receptive to outside impressions and it reflects the world back at the world. Its depths are dark and mysterious.

A person who wishes to become more rational must seek to manifest their inner rationality in a greater proportion of their conscious experience. They must seek to hone the skill of being that still pond which reflects the world back at the world. In my opinion, this is ultimately a spiritual quest, where one is led to seek to heal old emotional wounds, to face one's fears, and to nurture one's confidence and humility alike, in order to have fewer mental weights holding down one's rationality.

    The supreme good is like water,
    which nourishes all things without trying to.
    It is content with the low places that people disdain.
    Thus it is like the Tao.

    In dwelling, live close to the ground.
    In thinking, keep to the simple.
    In conflict, be fair and generous.

    [ch. 8]

Good reasoning is full of uniform and repeatable patterns. Some of these repeatable patterns can be systematized: for example, deductive logic, mathematics, and science all have a corpus of repeatable reasoning techniques of various degrees of formality. Understanding these patterns is important to the development of rationality.

However, knowing all the techniques of rationality does not give one the practical, procedural knowledge of being rational.

    When you have names and forms,
    know that they are provisional.
    When you have institutions,
    know where their functions should end.
    Knowing when to stop,
    you can avoid any danger.
    
    [ch. 32]

The heart of rationality is a groundedness in reality and a willingness to be swayed by reality created by a kind of emptiness and lack of preconception.

    We join spokes together in a wheel,
    but it is the center hole
    that makes the wagon move.

    We shape clay into a pot,
    but it is the emptiness inside
    that holds whatever we want.

    We hammer wood for a house,
    but it is the inner space
    that makes it livable.

    We work with being,
    but non-being is what we use.

    [ch. 11]

Of course, the moral virtues associated with rationality are merely one aspect of rationality. In my opinion they are a critical one which should be discussed explicitly in treatises on rationality. In my opinion, anybody who seeks to become more rational should seek to cultivate in themselves the virtues I listed above (fairness, even-handedness, etc.); to seek the company of people who have such virtues; and to learn from sources which have such virtues. I would further encourage the reader to reflect on the deeper nature of rationality in an introspective fashion: to study their own mind, how it works, how it processes reality, and to form judgments on which processes are rational and which are irrational.

In addition to looking at rationality from psychological, moral, and spiritual perspectives, we can also study rationality from a behavioral perspective, by studying human behavior that exhibits rationality and trying to formulate rules that describe and explain what makes rational behavior rational. 

Rationality has to do with two primary areas of human behavior: communication and decision making. We employ rationality in communicating with each other, as when employing arguments to persuade others of something. We also employ rationality in decision making, as when making a purchasing decision, and in innumerable other situations.

In practice most rationality-related issues can be studied in the context of norms of assertion. Studying norms of assertion is studying the majority of what there is to study about rationality, at least in my awareness. Of course other topics besides norms of assertion, e.g. beliefs and decision making, are important. Most rationality-related issues apply analogously across all subdomains of rationality (e.g. norms of assertion, beliefs, and decision making). Therefore by studying one of these subdomains, one learns about all of them. However, each should also be studied individually to learn about issues specific to that subdomain.

I find it more effective, as a way of teaching, to focus primarily on one of the many angles from which we can study rationality. This is because we can go deeper into that one angle by spending more time on it, and doing so sheds light on all the other angles. The primary angle I choose is norms of assertion primarily for the reasons that it's the one I understand best, and it's the one I best understand how to talk about.

One thing about norms of assertion that makes them easy to talk about is that unlike beliefs, which are mental phenomena and therefore only privately observable, assertions are objective, publicly observable occurrences.

Our language is primarily for talking about publicly observable things, and it's easier overall to have shared understanding of publicly observable things. Both of these factors lead me to favor focusing my philosophical writing on publicly observable things.

If I can explain the same thing either in terms of publicly observable things or in terms of privately observable things, I am more likely to choose to explain it in terms of publicly observable things, because I am more likely to think that route will lead to a clearer explanation.

Norms of assertion are an important subtopic in the more general topic of **laws of rationality**. The laws of rationality are all of the rules of speech and behavior such that if we break them, we can justly be called irrational. 

A simple example of a law of rationality is the law that if you agree to the assertion that (**A** and **B**), then you should also be willing to agree to the assertion that **A**. Probably the reader knows or imagines that there are many laws of rationality. Discovering exactly what they are is the problem.

On my reading, discovering what the laws of rationality are has been the foremost goal of the study of rationality since that study's inception. As long as we don't understand and apply the laws of rationality, we will run the risk of being irrational without having any idea we are being irrational. That's why I think it's valuable to learn well the understanding (or so called) humans have had of the laws of rationality for thousands of years, as well as the understanding (or so called) we have recently gained, and to try to push forward our evolving understanding (or so called) of the laws of rationality.

In the section titled Rules and Laws, I noted that all laws are binding in some sense. In what sense are the laws of rationality binding? In many different senses, in many different contexts:

* In debates, the laws of rationality are binding in the sense that in the world of debates, there is a loose approximation of a Nash equilibrium around following the laws of rationality as well as one is able. The laws of rationality are broken on a regular basis in debates, probably in most debates. However, there is a general agreement that in some sense those who break the rules of rationality are not debating correctly, and those who break them are more likely to lose debates.
* In academic contexts, the laws of rationality are binding as a matter of professional standards. In the context of academic publishing, the laws of rationality are enforceable (and corruptible) by the peer review process.
* In many professional contexts, such as when performing a technical job such as software development or security analysis, the laws of rationality are binding as a matter of professional ethics, especially in mission-critical scenarios.
* In serious contexts where you need to solve a problem, rationality is usually applicable. In these cases, the laws of rationality are binding in the sense that following them usually maximizes your likelihood of success.
* There is a loose approximation of a Nash equilibrium in the global game of society around making decisions as rationally as one is able.

If following a set of principles of rationality reduced your likelihood of success, then those principles would be a flawed way of thinking about rationality. Observe that this is not a [no true Scotsman fallacy](https://en.wikipedia.org/wiki/No_true_Scotsman), because we are not changing the definition of rationality in an ad hoc way to deal with counterexamples. Rather, the principle that methods that systematically cause you to fail aren't rational is part of our common sense, sort of by-definition understanding of rationality. See Eliezer Yudkowsy, ["Rationality is Systematized Winning,"](http://lesswrong.com/lw/7i/rationality_is_systematized_winning/) if you wish to be persuaded further of this.

There is no centralized authority on rationality. Generally speaking, rationality's dominance can be attributed to the fact that it gets shit done, that it wins. No centralized enforcement is necessary to maintain its dominance; it's a natural consequence of the way that rationality relates to human society as a whole, which is a consequence of properties of rationality, human society, humans, and the world. This situation also means that the dominant understanding of rationality should, over time, naturally tend towards greater correctness. In short, the laws of rationality are organically binding.

Of course, rationality is in competition with other conflicting forces: for example, ignorance. Rationality isn't always dominant, and this qualifier should be stated, though I don't believe it conflicts with anything I've said.

Here's a basic law of rationality, a basic norm of assertion, which I'll call the **law of evidence**: if you assert a statement, you should (ideally) be able to explain your reasons for believing it, and those reasons should (ideally) be good ones.

If you assert a statement, and somebody asks you why it's true, and you can't provide any good answer, it's generally an uncomfortable situation where one feels some social pressure to take back the assertion. 

The law of evidence is widely accepted because it is a good rule to follow if you want the truth. If you want the truth, it's a good idea to be skeptical of what other people say, to ask for the reasons that people assert things, and to doubt people who can't give good reasons for what they assert. If you want to be regarded as a truth-teller, it's a good idea to be able to give good reasons for the things you assert.

The law of evidence is the basic rational norm of assertion, in a sense. Most other laws of rationality have to do with legislating what reasons do and do not adequately support an assertion. In that sense they are elaborations on the law of evidence. Legislating the quality of various reasons for assertions is our main task in the rest of this text. Before starting on that, though, I need to clarify what I am talking about when I talk about reasons.

## Reasons

What are reasons? I am specifically trying to probe the meaning of the word "reason" which is used in the first three paragraphs of the Introduction, which I am still in the process of unpacking. This usage is in the definition of "argument." I defined an argument as "a series of statements designed to provide reason to believe some conclusion(s)." I could also have written "reasons" or "reason(s)" in this definition in place of "reason," without really changing the meaning. We are interested in defining "reason" as the term is used in the definition of "argument." Though that's our primary goal, we'll need to explore the meaning of the word "reason" in a more general way in order to get there.

We say things like:

1. "Euclid's proof provides reason to believe that there are infinitely many prime numbers."
2. "Euclid reasoned that there are infinitely many prime numbers."
3. "Euclid's proof reasons that there are infinitely many prime numbers."
4. "Euclid had great mastery of reason."
5. "Euclid presumably had his reasons for doing math."

In sentence 1, "reason" looks like a noun, a kind of special substance contained somehow in Euclid's proof. In sentence 2, "reason" occurs in a verb form, denoting some kind of activity of Euclid. In sentence 3, "reason" occurs again in a verb form, but now apparently denoting some kind of activity or potency of Euclid's proof. In sentence 4, "reason" occurs as a noun, now seeming to denote an area of study or expertise. In sentence 5, "reason" occurs as a plural noun, now referring to motivations, justifications, and/or rationalizations that Euclid presumably had for his choice to pursue math.

"Reason" is a versatile term that can be used in many different ways. There is some common theme flowing through these different usages: some common thread between the rational substance contained in Euclid's proof, and the rational activity of Euclid, and the rational activity or potency of Euclid's proof, and Euclid's mastery of reason, and perhaps even Euclid's reasons for doing math. If we want to take Euclid's proof as a good example of reasoning, and study it to understand reasoning better, then we can try to examine the proof, and we can do so from various perspectives. We can also try to examine Euclid, and try to discover what qualities in him made him able to produce the proof and motivated him to do so.

Examining reason from all possible perspectives and giving equal time to each in this text would make our lives confusing and difficult. In practice, I think a more useful approach is to choose one primary perspective from which to examine reason, understand reason well from that perspective, and then use that as a jumping off point for developing a more holistic and integrative understanding of reason.

I have already chosen the sense of "reason" that we are primarily going to investigate. This is the sense used in the definition of "argument," which is "a series of statements designed to provide reason to believe some conclusion(s)." In other words, I am interested in the kind of reasons that arguments are supposed to provide for their conclusions. Let's now investigate the meaning of "reason" in this sense.

Arguments are series of statements, and reasons are something that they intend to provide for their conclusions. Taking this literally, reasons are some sort of supervenient phenomena which emerge from properly formulated arguments. This illustrates the risks that can be involved in taking language too literally when doing philosophy. We can be led to remarkable and confusing conclusions in this way.

I'm not going to assume that there is literally a kind of entity in nature called a "reason." Statements that provide reasons exist, but I'm not going to assume that we can equate a reason with the statements that provide it, and I'm not going to assume that some statements are decorated with some sort of metaphysical entity called a reason.

If you like, you can think of "provide reason to believe" as a non-decomposable phrase denoting a relation between series of statements (arguments) on the left hand, and statements (conclusions) on the right hand. If this is correct, then there are many other English phrases which denote the same or approximately the same relation as "provide reason to believe," e.g. "provide justification," "provide evidence," "argue effectively," etc. And there are others which seem to denote different but related relations between arguments and conclusions, e.g. "prove," "demonstrate conclusively," "argue fallaciously," "argue poorly," etc. Taking all of these phrases to be non-decomposable phrases denoting relations having to do with providing reasons has the advantageous consequence of giving us a way to think about reasoning without getting stuck on the idea that there is some such thing as a "reason" which ought to be an object of our investigation.

Despite the appearances suggested by our grammar, what we're actually interested in are not a kind of entities called reasons, but a certain relation between arguments and their conclusions, the relation of providing reasons for.

We can therefore formulate our question as follows: what arguments provide reason to believe what conclusions? Obviously this is a very broad question, and what we're interested in is a general method or body of theory that lets us answer the question in any particular case, or at least in as many particular cases as possible.

Another way of formulating the question is as follows: which arguments are winners, and which are losers? Every properly formulated argument states what its conclusion is. If an argument provides good reason to believe its conclusion, then we can say the argument is a winner. If argument provides bad reasons or no reasons to believe its conclusion, then we can say the argument is a loser. Then we can describe what we're interested in as a general method or body of theory for distinguishing between winning and losing arguments.

The way to develop such a method or body of theory is to start by looking at lots of particular arguments, figure out which ones are winners and losers on a case by case basis, and on that basis begin to formulate general rules about what arguments are winners and losers. This is very slow, methodical work, and in doing it we are retracing the steps of philosophers who have thought about these issues for thousands of years. In developing the theory of discriminating between winning and losing arguments presented in the rest of this text, my goal is not to discover new principles of reasoning, but to formulate what has already been understood by those who looked at this before, in the most correct, persuasive, and useful way possible.

What I am looking to study are **laws of reasoning**. Laws of reasoning are generalizations which guide us in understanding what arguments are winning.  Laws of reasoning help us to evaluate the quality of arguments by telling us what methods of argumentation do and do not work. They are the essential theoretical ammunition that one needs to win arguments.

One example of a law of reasoning is that properly formulated *reductio ad absurdum* arguments are winning. This law is basically correct. However, this formulation of it is still imprecise, and at this point in the text we lack the methodology to justify or even to state precisely the *reductio ad absurdum* principle. Furthermore, there is plenty of controversy around the *reductio ad absurdum* principle among logicians, which we have yet to touch at all. 

The overall primary aim of the text up to this point can be described as the aim of defining and analyzing important concepts that allow us clearly articulate the goal I am now setting to describe laws of reasoning. 

Of course, I don't just want to describe any laws of reasoning. I want to describe laws of reasoning that are correct, or true. It's a messy question what exactly this means, especially because there are very few laws of reasoning which entirely lack plausible counterexamples (cases where they plausibly allow one to infer false conclusions from plausibly true premises).

Let's now get into the weeds with our first important category of laws of reasoning: laws of logic.

## Logic

We are about to take a deep dive into the treacherous, brambly swamp of [philosophical logic](https://en.wikipedia.org/wiki/Philosophical_logic) and the [philosophy of logic](https://en.wikipedia.org/wiki/Philosophy_of_logic), with the heroic aim of acquiring true, classical, foundational principles of reasoning, what we knew all along rigorously articulated and defended from the attacks of the critics.

**Logic**, as I shall describe it, is a field of study where the primary topic is valid rules of logical inference. Let's unpack that.

A **rule of logical inference**, or **rule of inference**, is a rule which states that in any conversational context where a set of statements with given forms (**premises**) are warranted assertible, another statement (the **conclusion**) is warranted assertible. This is my definition.

I'll give some examples of rules of inference for statements in the language of first-order logic.

 * (**A** and **B**) entails **A**.
 * (**A** and **B**) entails **B**.
 * **A**, **B** entails (**A** and **B**).
 * **A** entails (**A** or **B**).
 * **B** entails (**A** or **B**).
 * (if **A** then **B**), **A** entails **B**.

The statement of each of these rules consists of: a sequence of statement forms, the word "entails," and a single statement form at the end. By a "statement form," I mean a first-order logic statement with meta-variables (**A** and **B** in these examples) holding the place of some parts of the statement.

Rules of logical inference can be thought of as descriptive rules, by taking them to describe regularities in people's standards of warranted assertibility. For example, in just about any conversational context, for just about any statements **A** and **B**, just about any speaker who considers (**A** and **B**) warranted assertible in that context will also consider **A** warranted assertible in that context. This is one way of interpreting the meaning of the rule of inference "(**A** and **B**) entails **A**."

Rules of logical inference can also be thought of as prescriptive rules, by taking them as defining norms of assertion. For example, the rule of inference "(**A** and **B**) entails **A**" can be taken to be the normative rule that in any context, for any statements **A** and **B** which are interpretable in that context, if a speaker considers (**A** and **B**) to be warranted assertible in that context, then they should also consider **A** to be warranted assertible in that context.

In this section I am basically asking the following questions. In general, what rules of logical inference are correct as descriptive rules? And, what rules of logical inference should we accept as prescriptive rules? These two questions are closely related. We want to figure out what level of consensus exists around each proposed rule of logical inference, which tells us how correct it is as a descriptive rule. This then becomes the primary consideration in deciding what rules of inference to accept as prescriptive rules.

Rules of logic are about as close as we can get to universal rules for debate. For all that philosophers disagree about almost everything, there is virtually no disagreement that in any or just about any context where a statement of the form (**A** and **B**) is warranted assertible, the statement **A** is also warranted assertible. This is a very simple and self-evident rule, which might not appear to be good for much. In itself, it isn't good for much.

The beauty of logic is that by chaining together long sequences of applications of simple, self-evident rules like this one, one can bring one's listeners along on a journey to a final conclusion that may be very surprising to them, that probably they would not have accepted if they hadn't been presented with such a compelling sequence of logical inferences. Logic ends up being one of the great sources of common ground in debates.

Rules of logical inference can also be called **argument forms**. The term "argument form" is somewhat more neutral in that "rule of inference" may carry the connotation "correct rule of inference" to many listeners, whereas the term "argument form" more explicitly encompasses incorrect rules of inference as well as correct ones. Again, though, both terms literally mean the same thing, as I am using them.

A **case** of an argument form is an argument resulting from substituting concrete expressions for the meta-variables in the argument form. For example, in the argument form "(**A** and **B**) entails **A**," the spaces held by meta-variables are the spaces held by **A** and **B**. Here are some cases of this argument form:

* (Spot is a dog and Spot runs) entails Spot is a dog.
* (Barack Obama is POTUS and POTUS is male) entails Barack Obama is POTUS.

Here are some non-examples of cases of this argument form:

* (Spot is a dog and Spot runs) entails Spot runs.
* (Barack Obama is POTUS and POTUS is male) entails the sky is blue.

I will call an argument form **valid** for a given person if and only if there are no **counterexamples** to the rule for that person. A counterexample to an argument form for a given person is a (hypothetical or real) truth-seeking debate context and a case of the argument form where they consider the premises warranted assertible and don't consider the conclusion warranted assertible. I will only consider a counterexample to an argument form to be genuine if even after participating in thorough discussion and evidencing a good understanding of the relevant issues, the person with the counterexample still finds the premises warranted assertible and the conclusion not warranted assertible.

When an argument form is valid for people in general, I will call it simply **valid**. "(**A** and **B**) entails **A**" is an example of a valid argument form. We can also call it a valid rule of logical inference, since every argument form is a rule of logical inference and vice versa.

I am now in a position to articulate the main question about logic in this section. **What rules of logical inference are valid?** This question is implicitly relative to some language. I will choose the language of first order logic for this investigation.

What language we choose for our investigation is not an entirely neutral choice. When we give formal definitions of rules of inference for a formal language, the syntax of the language imposes constraints on the rules we can straightforwardly formulate. For example, in [second-order logic](https://en.wikipedia.org/wiki/Second-order_logic) we can easily express rules of inference that have no straightforward expression in first-order logic.

The syntax of first-order logic is specially chosen to make standard rules of logical inference easily expressible. First-order logic is as complex as necessary to define enough rules of logical inference to do all of mathematics, but not more complex than necessary to accomplish this. We can feel good about this choice of language for investigating rules of logical inference.

There is a standard, widely accepted answer to what rules of logical inference are valid for first-order logic. This answer is a set of rules called **classical first-order logic**, or **classical logic** as I shall abbreviate it. There is controversy among academic logicians about whether all the rules of classical logic are valid. Few logicians think there are any valid rules of inference for first-order logic that are not part of classical logic. A greater number of logicians think that not all rules of classical logic are valid. It should be noted that the logicians in this debate employ various definitions of "valid" which may not always coincide.

We will start our way into this central debate in the study of logic by looking at the rules of classical logic.

There are infinitely many rules of inference or argument forms that are valid according to classical logic. In order to describe the rules of classical logic in a finite amount of space, we need to use abstraction.

The way the rules of classical logic are conventionally presented is by giving a finite number of rules which allow all of the valid rules of inference or argument forms to be generated. There are a variety of ways of doing this: e.g., via [natural deduction](https://en.wikipedia.org/wiki/Natural_deduction) systems, via [Hilbert systems](https://en.wikipedia.org/wiki/Hilbert_system), or via [sequent calculi](https://en.wikipedia.org/wiki/Sequent_calculus). Here I will do an informal presentation of a sequent calculus formulation of the rules of classical first order logic.

TODO: Add graphical sequents

My presentation is most closely based on the style of presentation of first-order logic articulated in [Dave Ripley's](http://davewripley.rocks/) graduate philosophical logic course taught in spring 2014 at the University of Connecticut. Credit for the original conception of the rules of classical first-order logic is best claimed by Gottlob Frege and Giuseppe Peano, as far as I'm aware.

This presentation requires a concept that is new for this text: the concept of multiple-conclusion sequents.

A **sequent** is basically an entailment between first order logic statements. ((Pa and Pb) entails Pa) is an example of a sequent.

A **multiple-conclusion sequent** is a sequent, which might have multiple conclusions. ((Pa or Pb) entails Pa, Pb) is an example of a multiple-conclusion sequent. In general, a multiple-conclusion sequent has the form (**A1**,...,**An** entails **B1**,...,**Bm**).

The notation **A1**,...,**An** stands for any finite sequence of statements. **n** is supposed to be a variable standing for a number. **n** could be zero, in which case this denotes an empty sequence of statements. **n** could also be one, or any higher number. **B1**,...,**Bm** is another instance of the same notation. Of course the sequences **A1**,...,**An** and **B1**,...,**Bm** may be of different length. A multiple conclusion sequent has zero or more **premises** and zero or more **conclusions**. This presentation of classical logic is based on multiple conclusion sequents, so henceforth "sequent" is short for "multiple-conclusion sequent."

A sequent (**A1**,...,**An** entails **B1**,...,**Bm**) should be interpreted for our purposes as stating that it is incoherent to assert all of **A1**, ..., **An** and to deny all of **B1**, ..., **Bm** at the same time. If what the sequent states is true according to classical logic, I say that the sequent is **valid according to classical logic**, or simply **valid** when context makes the meaning clear.

The sequent ((Pa and Pb) entails Pa) is valid. It states that it is incoherent to assert (Pa and Pb) while denying Pa.

The sequent ((Pa or Pb) entails Pa, Pb) is valid. It states that it is incoherent to assert (Pa or Pb) while denying both Pa and Pb.

According to Dave Ripley in communication with me, this interpretation of the meaning of multiple conclusion sequents was anticipated 'in John MacFarlane's unpublished (but widely-distributed) "In what sense (if any) is logic normative for thought?"' and the notion first appears in the published literature in [Greg Restall's 2005 "Multiple Conclusions."](https://philpapers.org/rec/RESMC) The interpretation was taught to me by Dave Ripley in his 2014 logic course.

I have finished introducing the notion of multiple conclusion sequents. Let us proceed to the rules of classical logic.

The rules of classical logic are the rules that state what sequents are valid according to classical logic. A sequent is valid according to classical logic if and only if it can be deduced according to these rules.

**Rule of non-contradiction**

The first rule of classical logic states that any statement entails itself. For any statement **A**, (**A** entails **A**) is a valid sequent. This sequent states that it is incoherent to assert **A** and deny **A** at the same time.

**Conjunction rules**

The following rules in classical logic are related to conjunction ("and").

1. **Assertion strengthening.** Suppose (**A1**,...,**An**, **B**  entails **C1**,...,**Cm**) is a valid sequent. Then (**A1**,...,**An**, (**B** and **D**) entails **C1**,...,**Cm**) is a valid sequent. What changes between the "suppose" sequent and the "then" sequent is that **B** is replaced with (**B** and **D**). In other words, given an incoherent set of assertions and denials including the assertion of **B**, strengthening **B** to (**B** and **D**) gives you another incoherent set of assertions and denials. In a second version of this rule which is also a rule of classical logic, (**B** and **D**) is replaced with (**D** and **B**). A shorter (and less complete) way of stating the rule of assertion strengthening is to say that *if it's incoherent to assert **B**, then it's incoherent to assert (**B** and **D**) or to assert (**D** and **B**).*

2. **Denial weakening.** Suppose (**A1**,...,**An** entails **B**, **C1**,...,**Cm**) is a valid sequent, and (**D1**,...,**Dk** entails **E**, **F1**,...,**Fj**) is a valid sequent. Then (**A1**,...,**An**, **D1**,...,**Dk** entails (**B** and **E**), **C1**,...,**Cm**, **F1**,...,**Fj**) is a valid sequent. In the "suppose" we are given two incoherent sets of assertions and denials, the first containing a denial of **B** and the second containing a denial of **E**. The rule tells us that it is incoherent to assert the union of these two sets of assertions and denials, with the denials of **B** and **E** replaced with the denial of the conjunction (**B** and **E**). Denying the conjunction of (**B** and **E**) is weaker than denying each of **B** and **E** individually, because to deny the conjunction (**B** and **E**) is to deny that **B** and **E** are both true, which is equivalent in classical logic to asserting that at least one of **B** or **E** is false. It is also true that simply taking the union of these two incoherent sets of assertions and denials would give us another incoherent set, because adding anything at all to an incoherent set gives you another incoherent set. What the rule of denial weakening tells us is that it is still incoherent, given the setup of the "suppose," to combine the two sets and replace the separate denials of **B** and **E** with the weaker denial of just (**B** and **E**). A shorter (and less complete) way of stating the rule of denial weaking is to say that *if it's incoherent to deny **B** and it's incoherent to deny **E**, then it's incoherent to deny (**B** and **E**).*

**Disjunction rules**

The following rules of classical logic are related to disjunction ("or").

3. **Denial strengthening.** Suppose (**A1**,...,**An** entails **B**, **C1**,...,**Cm**) is a valid sequent. Then (**A1**,...,**An** entails (**B** or **D**), **C1**,...,**Cm**) is a valid sequent. What changes between the "suppose" sequent and the "then" sequent is that **B** is replaced with (**B** or **D**). In other words, given an incoherent set of assertions and denials including the denial of **B**, strengthening the denial of **B** to the denial of (**B** or **D**) gives you another incoherent set of assertions and denials. Denying (**B** or **D**) is equivalent in classical logic to asserting that **B** and **D** are both false, which is why it's stronger than denying just **B**. In a second version of this rule which is also a rule of classical logic, (**B** or **D**) is replaced with (**D** or **B**). A shorter (and less complete) way of stating the rule of denial strengthening is to say that *if it's incoherent to deny **B**, then it's incoherent to deny (**B** or **D**) or to deny (**D** or **B**).*

4. **Assertion weakening.** Suppose (**A1**,...,**An**, **B** entails **C1**,...,**Cm**) is a valid sequent, and (**D1**,...,**Dk**, **E** entails **F1**,...,**Fj**) is a valid sequent. Then (**A1**,...,**An**, **D1**,...,**Dk**, (**B** or **E**) entails **C1**,...,**Cm**, **F1**,...,**Fj**) is a valid sequent. In the "suppose" we are given two incoherent sets of assertions and denials, the first containing an assertion of **B** and the second containing an assertion of **E**. The rule tells us that it is incoherent to assert the union of these two sets of assertions and denials, with the separate assertions of **B** and **E** replaced with the mere assertion of the disjunction (**B** or **E**). It is also true that simply taking the union of these two incoherent sets of assertions and denials would give us another incoherent set, because adding anything at all to an incoherent set gives you another incoherent set. What the rule of assertion weakening tells us is that it is still incoherent, given the setup of the "suppose," to combine the two sets and replace the separate assertions of **B** and **E** with the weaker assertion of just (**B** or **E**). A shorter (and less complete) way of stating the rule of assertion weakening is to say that *if it's incoherent to assert **B** and it's incoherent to assert **E**, then it's incoherent to assert (**B** or **E**).*

**Conditional rules**

The following rules in classical logic are related to the conditional ("if/then").

5. **Conditional denial.** Suppose (**A1**,...,**An**, **B** entails **C**, **D1**,...,**Dm**) is a valid sequent. Then (**A1**,...,**An** entails (if **B** then **C**), **D1**,...,**Dm**) is a valid sequent. In this rule, the premise or assertion **B** of the sequent is moved to the conclusion or denial side as the premise of the conditional (if **B** then **C**). A shorter (and less complete) way of stating the rule of conditional denial is to say that *if it's incoherent to assert **B** and deny **C**, then it's incoherent to deny (if **B** then **C**).*

6. **Conditional assertion.** Suppose (**A1**,...,**An** entails **B**, **C1**,...,**Cm**) is a valid sequent and (**D1**,...,**Dk**, **E** entails **F1**,...,**Fj**) is a valid sequent. Then (**A1**,...,**An**, **D1**,...,**Dk**, (if **B** then **E**) entails **C1**,...,**Cm**, **F1**,...,**Fj**) is a valid sequent. A shorter (and less complete) way of stating the rule of conditional assertion is to say that *if it's incoherent to assert **E** and deny **B**, then it's incoherent to assert (if **B** then **E**).* This is one of the less intuitive rules of this presentation of classical logic. I will have more to say about it later.

**Negation rules**

The following rules in classical logic are related to negation ("not").

7. **Negation assertion.** Suppose (**A1**,...,**An** entails **B**, **C1**,...,**Cm**) is a valid sequent. Then (**A1**,...,**An**, (not **B**) entails **C1**,...,**Cm**) is a valid sequent. In this rule, the denial of **B** is transformed to an assertion of (not **B**). A shorter (and less complete) way of stating the rule of negation assertion is to say that *if it's incoherent to deny **B**, then it's incoherent to assert (not **B**).*

8. **Negation denial.** Suppose (**A1**,...,**An**, **B** entails **C1**,...,**Cm**) is a valid sequent. Then (**A1**,...,**An** entails (not **B**), **C1**,...,**Cm**) is a valid sequent. In this rule, the assertion of **B** is transformed to a denial of (not **B**). A shorter (and less complete) way of stating the rule of negation denial is to say that *if it's incoherent to assert **B** then it's incoherent to deny (not **B**).*

**Universal quantifier rules**

The following rules in classical logic are related the universal quantifier ("for all"). Stating these rules requires introducing a new notation. For any statement **A**, any variable name **x**, and any object term **t**, let **A**[**x** -> **t**] denote the statement resulting from replacing all instances of **x** in **A** with **t**.

9. **Counterexample.** Suppose (**A1**,...,**An**, **B**[**x** -> **t**] entails **C1**,...,**Cn**) is a valid sequent. Then (**A1**,...,**An**, (for all **x**, **B**) entails **C1**,...,**Cn**) is a valid sequent. The rule of counterexample states that if it is incoherent to assert **B** for one specific possible value of **x** (denoted by **t**), then it is incoherent to assert the universal generalization (for all **x**, **B**). In short, *it is incoherent to assert any universal generalization that has a counterexample.*

10. **Universal generalization.** Suppose (**A1**,...,**An** entails **B**[**x** -> **y**], **C1**,...,**Cn**) is a valid sequent, and that **y** does not occur as a free variable in any of the statements **A1**,...,**An**, **C1**,...,**Cn**. Then (**A1**,...,**An** entails (for all **x**, **B**), **C1**,...,**Cn**) is a valid sequent. The assumption that **y** does not occur as a free variable in any of the side premises **A1**,...,**An**, **C1**,...,**Cn** is a way of capturing the idea that **y** is a variable that could potentially refer to any object. The rule of universal generalization states in essence that *if it is incoherent to deny **B** for an arbitrary object, then it is incoherent to deny (for all **x**, **B**).*

**Existential quantifier rules**

The following rules in classical logic are related to the existential quantifier ("for some"). 

11. **Non-existence generalization.** Suppose (**A1**,...,**An**, **B**[**x** -> **y**] entails **C1**,...,**Cm**) is a valid sequent, and that **y** does not occur as a free variable in any of the statements **A1**,...,**An**, **C1**,...,**Cm**. Then (**A1**,...,**An**, (for some **x**, **B**) entails **C1**,...,**Cn**) is a valid sequent. The assumption that **f** does not occur as a free variable in any of the side premises **A1**,...,**An**, **C1**,...,**Cm** is a way of capturing the idea that **y** is a variable that could potentially refer to any object. The rule of non-existence generalization states in essence that *if it is inchoherent to assert **B** for an arbitrary object, then it is incoherent to assert (for some **x**, **B**).

12. **Example.** Suppose (**A1**,...,**An** entails **B**[**x** -> **t**], **C1**,...,**Cm**) is a valid sequent. Then (**A1**,...,**An** entails (for some **x**, **B**), **C1**,...,**Cm**) is a valid sequent. The rule of example states in essence that *if it is incoherent to deny **B** for some specific object, then it is incoherent to deny (for some **x**, **B**).*

**Structural rules**

The structural rules in classical logic are rules of inference that tell us that manipulating the structure of valid sequents in certain ways always yields other valid sequents. In summary, these rules tell us two things. First, that we can add premises and conclusions to a valid sequent to get another valid sequent; this is because adding more assertions or denials to an incoherent set of assertions or denials will never make it coherent. Second, that order and repetition of the assertions and denials does not matter for coherence.

The following are the structural rules of classical logic.

13. **Strengthening.** Suppose (**A1**,...,**An** entails **C1**,...,**Cm**) is a valid sequent. Then for any statement **B**, (**A1**,...,**An**, **B** entails **C1**,...,**Cm**) is a valid sequent, and (**A1**,...,**An** entails **B**, **C1**,...,**Cm**) is a valid sequent. The rule of strengthening states in essence that *if your position is incoherent, then your position is still incoherent if you strengthen it by asserting **B** or denying **B**.* This rule is called "weakening" in most other presentations of classical logic.

12. **Assertion contraction.** Suppose (**A1**,...,**An**, **B**, **B** entails **C1**,...,**Cm**) is a valid sequent. Then (**A1**,...,**An**, **B** entails **C1**,...,**Cm**) is a valid sequent. The rule of assertion contraction states in essence that *if a position is incoherent and asserts **B** more than one time, then it is still incoherent if asserts **B** one less time.*

13. **Denial contraction.** Suppose (**A1**,...,**An** entails **B**, **B**, **C1**,...,**Cm**) is a valid sequent. Then (**A1**,...,**An** entails **B**, **C1**,...,**Cm**) is a valid sequent. The rule of denial contraction states in essence that *if a position is incoherent and denies **B** more than once, then it is still incoherent if it denies **B** one less time.*

From the rules of strengthening, assertion contraction, and denial contraction, it follows that the number of times you assert or deny a statement does not matter for the coherence of your position. To arrive at this conclusion one also needs to use the rules of assertion permutation and denial permutation.

13. **Assertion permutation.** Suppose (**A1**,...,**An**, **B**, **C**, **D1**,...,**Dm** entails **E1**,...,**Ek**) is a valid sequent. Then (**A1**,...,**An**, **C**, **B**, **D1**,...,**Dm** entails **E1**,...,**Ek**). What changes between the "suppose" sequent and the "then" sequent is that the order of **B**, **C** is swapped to produce **C**, **B**. The rule of assertion permutation states in essence that *if a position is incoherent, then it's still incoherent after swapping the order of two assertions.*

14. **Denial permutation.** Suppose (**A1**,...,**An** entails **B1**,...,**Bm**, **C**, **D**, **E1**,...,**Ek**) is a valid sequent. Then (**A1**,...,**An** entails **B1**,...,**Bm**, **D**, **C**, **E1**,...,**Ek**) is a valid sequent. What changes between the "suppose" sequent and the "then" sequent is that the order of **C**, **D** is swapped to produce **D**, **C**. The rule of denial permutation states in essence that *if a position is incoherent, then it's still incoherent after swapping the order of two denials.*

From the rules of assertion permutation and denial permutation, it follows in generality that *the order of assertions and denials does not matter for coherence.* From all the structural rules together, it follows in generality that *the order and repetition of assertions and denials do not matter for coherence.*

The final structural rule in this presentation is the rule of **cut**. This rule is redundant; [Gentzen's cut elimination theorem](https://en.wikipedia.org/wiki/Cut-elimination_theorem) shows that any sequent that is valid in classical logic can be shown to be valid using just the 14 rules we have listed so far, without using rule 15, the cut rule. However, sequent derivations not using the cut rule will tend to be much longer than derivations using the cut rule.

15. **Cut.** Suppose that (**A1**,...,**An** entails **B1**,...,**Bm**, **C**) is a valid sequent, and that (**C**, **D1**,...,**Dk** entails **E1**,...,**Ek**) is a valid sequent. Then (**A1**,...,**An**, **D1**,...,**Dk** entails **B1**,...,**Bm**, **E1**,...,**Ek**) is a valid sequent. The cut rule is so named because the rule lets us combine two valid sequents, one with **C** in the conclusions and one with **C** in the premises, while cutting **C** out of the picture. The rule of cut states in essence that *if it's incoherent for you to assert **C** and it's incoherent for you to deny **C**, then your position is incoherent.*

That completes my presentation of the rules of classical first order logic. For another presentation which is more compact and easier to survey, see [Wikipedia's presentation of the LK sequent calculus](https://en.wikipedia.org/wiki/Sequent_calculus#The_system_LK). 

As I have stated before, it is a matter of controversy among logicians whether all the rules of classical logic are correct in all contexts. Before we survey that very intricate debate, I would like to explain how these rules can be used.

Each of these rules states a very basic principle about what kinds of sets of assertions and denials are incoherent. Most of them are so obvious as to appear trivial, perhaps trivial to the extent of near-pointlessness. The reason that rules of logic are useful and allow us to arrive at surprising, non-obvious conclusions is that this can be accomplished by chaining together long sequences of applications of rules of logic.

The most impressive example of the usefulness of logic is in math. Every mathematical statement that is true according to the consensus of mathematics can be arrived at by applying the rules of first order logic I have presented to derive progressively more conclusions from a set of basic axioms, such as the widely accepted axioms of [Zermelo-Fraenkel set theory](https://en.wikipedia.org/wiki/Zermelo%E2%80%93Fraenkel_set_theory). Higher mathematicians build ever more complex towers of thought, exploring an infinite space of true mathematical thoughts; this exploration can be continued indefinitely without ever needing to assume more axioms.

Logic is also indispensable in philosophy, and in any area of serious, truth-seeking human thought. The rules of logic are the most reliable generally accepted principles of reasoning that humans have.

The reason rules of logic are useful is that they can be chained together to form interesting arguments. Let's examine how this is done in the case of classical first-order logic as I have presented it.

Consider the following classic argument. Socrates is a man, and all men are mortal; therefore, Socrates is mortal. We can symbolize this argument as a sequent as follows: (Socrates is a man, (for all *x*, (if *x* is a man then *x* is mortal)) entails (Socrates is mortal)). This is a valid sequent/argument. Let's see how we can prove its validity according to the rules of classical logic.

1. The rule of non-contradiction implies that this is a valid sequent: ((Socrates is a man) entails (Socrates is a man)).
2. The rule of non-contradiction implies that this is a valid sequent: ((Socrates is mortal) entails (Socrates is mortal)).
3. The rule of conditional assertion and steps 1 and 2 imply that this is a valid sequent: ((Socrates is a man), (if (Socrates is a man) then (Socrates is mortal)) entails (Socrates is mortal)).
4. The rule of counterexample and step 3 imply that this is a valid sequent: ((Socrates is a man), (for all *x*, (if *x* is a man then *x* is mortal)) entails (Socrates is mortal)).

I will explain the thought process that I used to arrive at this proof, and unpack why the proof is correct.

In this presentation of classical logic, every proof that a sequent is valid formally needs to start by invoking the rule of non-contradiction. Recall that the rule of non-contradiction is the rule that for all statements **A**, the sequent (**A** entails **A**) is valid, or in other words it is incoherent to assert and deny a statement at the same time.

The rule of non-contradiction is the only rule we have which allows one to conclude that a sequent is valid without having previously concluded that some other sequent is valid. Except for the rule of non-contradiction, all of the rules we have are of the form "if one or more sequents of a given form are valid, then one or more related sequents of a different form are valid." To use the other rules besides non-contradiction, one must have some sequents that are valid already at hand. Originally they always come from the rule of non-contradiction. These considerations told me that for sure, my proof was going to start with an invocation of the rule of non-contradiction.

A useful technique for constructing proofs that sequents are valid is to try to work backwards from the desired conclusion to the beginning of the proof. In this case, the desired conclusion is that ((Socrates is a man), (for all *x*, (if *x* is a man then *x* is mortal)) entails (Socrates is mortal)) is a valid sequent.

The conclusion is a rather long first-order logic statement, and it contains details that are not necessary for the task at hand. We can simplify it and make it easier to work with by replacing the English phrases with symbols. We will symbolize the predicate "is a man" by the letter P. We will symbolize the predicate "is mortal" by the letter Q. And we will symbolize the object term "Socrates" by the letter a. Then our desired conclusion becomes: (Pa, (for all *x*, (if P*x* then Q*x*)) entails Qa). This is a little more surveyable.

Notice that, with the exceptions of non-contradiction and cut, all of the rules of our system build more complex statements out of simpler statements. For instance, rule 2, assertion strengthening, lets us go from **A** to (**A** and **D**) or (**D** and **A**). Rule 8, negation denial, lets us go from assertion of **A** to denial of (not **A**). Each rule of our system (with the exceptions of non-contradiction and cut) builds one new statement with one new logical connective that wasn't there in the premises of the rule.

We are going to start working backwards towards the completion of our proof by finding one or more valid sequents which let us derive the conclusion in one step, applying one rule of our system. Because of what our rules do, our objective is to find premises which let us build the outermost logical connective of one of the statements in our conclusion.

In this case we have only one option. We must attempt to build the "for all" quantifier in the statement (for all *x*, (if P*x* then Q*x*)). This is the only statement in our conclusion sequent that has any logical connectives to build.

We know that the rule we need employ is one of the universal quantifier rules. Specifically, it must be whichever of the two rules can be applied to produce assertions, since in this sequent (for all *x*, (if P*x* then Q*x*)) is an assertion. Therefore the rule we need to employ is the rule of counterexample.

The rule of counterexample reads as follows. Suppose (**A1**,...,**An**, **B**[**x** -> **t**] entails **C1**,...,**Cn**) is a valid sequent. Then (**A1**,...,**An**, (for all **x**, **B**) entails **C1**,...,**Cn**) is a valid sequent.

This tells us that the second to last step in our proof should be to derive a sequent of the form (**A1**,...,**An**, **B**[**x** -> **t**] entails **C1**,...,**Cn**), which the rule of counterexample will turn into the desired conclusion sequent, namely: (Pa, (for all *x*, (if P*x* then Q*x*)) entails Qa). 

Let's find that by matching our conclusion sequent against the form of the conclusion of the rule of counterexample. The form of the conclusion of the rule of counterexample is (**A1**,...,**An**, (for all **x**, **B**) entails **C1**,...,**Cn**). Our desired conclusion instantiates this form as follows:

* **A1**,...,**An** = Pa
* **x** = *x*
* **B** = (if P*x* then Q*x*)
* **C1**,...,**Cn** = Qa

(The interesting equation **x** = *x* expresses that the meta-variable **x** is instantiated to the variable *x*.)

Substituting these variables into the form of the premise of the rule of counterexample, we can say that our premise should be of the form:

(Pa, (if P**t** then Q**t**) entails Qa)

for some object term **t**. **t** is the only variable that's in the premise form of the rule of counterexample and not in the conclusion form. We need to pick a value for **t**. A sensible guess would be to pick **t** = a. As we will now see, this choice works out and lets us complete the proof.

The final step of the proof we are constructing is to invoke the rule of counterexample to go from the validity of (Pa, (if Pa then Qa) entails Qa) to the validity of (Pa, (for all *x*, (if P*x* then Q*x*)) entails Qa). (Pa, (if Pa then Qa) entails Qa) is indeed a valid sequent, as some reflection should show. In order to complete our proof, we have to prove that it is a valid sequent. We will do this by continuing to work backwards, now trying to find a step that will let us arrive at (Pa, (if Pa then Qa) entails Qa).

Our goal in this step should be to build one of the logical connectives in our goal sequent. There is only one choice: we need to build the "if/then" of the statement (if Pa then Qa). This tells us that we need to use one of the conditional rules: whichever one lets us build an assertion, since (if Pa then Qa) is an assertion in our goal sequent. Thus we need to use rule 6, the rule of conditional assertion.

The rule of conditional assertion reads as follows. 

Suppose (**A1**,...,**An** entails **B**, **C1**,...,**Cm**) is a valid sequent and (**D1**,...,**Dk**, **E** entails **F1**,...,**Fj**) is a valid sequent. Then (**A1**,...,**An**, **D1**,...,**Dk**, (if **B** then **E**) entails **C1**,...,**Cm**, **F1**,...,**Fj**) is a valid sequent.

We can match our goal sequent against the form of the conclusion sequent as follows:

* **A1**,...,**An**, **D1**,...,**Dk** = Pa
* **B** = Pa
* **E** = Qa
* **C1**,...,**Cm**, **F1**,...,**Fj** = Qa

We need to produce two premise sequents which the rule of conditional assertion will turn into our goal sequent. One of them will contain the assertions **A1**,...,**An** and the denials **C1**,...,**Cm**. The other will contain the assertions **D1**,...,**Dk** and the denials **F1**,...,**Fj**. In this case, we know that the combined sequence of assertions **A1**,...,**An**, **D1**,...,**Dk** is equal to the one-element sequence containing just "Pa." So Pa goes into either the **A** sequence or the **D** sequence, but we need to decide which. Similarly, Qa goes into either the **C** sequence or the **F** sequence, but we need to decide which.

Let's set that issue aside for the moment and state what we know about the forms our premises need to have. We are settled that **B** = Pa and **E** = Qa, so we can write our needed premises in this way:

* (**A1**,...,**An** entails Pa, **C1**,...,**Cm**)
* (**D1**,...,**Dk**, Qa entails **F1**,...,**Fj**)

Should Pa go into the **A** sequence or the **D** sequence? Should Qa go into the **C** sequence or the **F** sequence? If we put Pa in the **A** sequence and Qa in the **F** sequence, then both our premises become instances of the rule of non-contradiction:

* (Pa entails Pa)
* (Qa entails Qa)

So that is what we should do, and we have reached the beginning of our proof. Let's now write out the steps in forwards order instead of backwards order:

1. (Pa entails Pa) is valid, by the law of non-contradiction.
2. (Qa entails Qa) is valid, by the law of non-contradiction.
3. (Pa, (if Pa then Qa) entails Qa) is valid, by the law of conditional assertion and steps 1 and 2.
4. (Pa, (for all *x*, (if Pa then Qa)) entails Qa) is valid, by the law of counterexample and step 3.

Let's now write the finished version of the proof by substituting back the original English terms:

* P stands for "is a man."
* Q stands for "is mortal."
* a stands for "Socrates."

This gives us the following proof.

1. ((Socrates is a man) entails (Socrates is a man)) is valid, by the law of non-contradiction.
2. ((Socrates is mortal) entails (Socrates is mortal)) is valid, by the law of non-contradiction.
3. ((Socrates is a man), (if (Socrates is a man) then (Socrates is mortal)) entails (Socrates is mortal)) is valid, by the law of conditional assertion and steps 1 and 2.
4. ((Socrates is a man), (for all *x*, (if (*x* is a man) then (*x* is mortal))) entails (Socrates is mortal)) is valid, by the law of counterexample and step 3.

To sum up, the classic argument we have analyzed is valid according to classical logic. Given that Socrates is a man and all men are mortal, it follows that Socrates is mortal. It is incoherent, according to classical logic, to assert that Socrates is a man and all men are mortal while denying that Socrates is mortal.

The basic proof construction techniques that I have introduced in the context of this argument can be applied to construct arbitrarily complex proofs of the validity of valid first-order logic sequents. To go deeper into this topic, I refer you to the exercises. TODO: exercises

That completes my introduction of how to use the rules of classical first-order logic. We can use rules of logic to establish that sets of assertions and denials are incoherent. The rules of classical logic generate an infinite set of valid sequents which describe what sets of assertions and denials are incoherent according to classical logic.

We went entered our discussion of classical logic wanting to discover what rules of logical inference are valid. Rules of logical inference let us go from premises that are warranted assertible in a given context to conclusions that are also warranted assertible in the context. We ended up with something slightly different: we got a system of rules telling us what sets of assertions and denials are incoherent. Let's see how we can get back to the topic of rules of logical inference.

A rule of logical inference can also be called an argument form. For our purposes we can equate rules of inference or argument forms with single-conclusion sequents possibly containing meta-variables. Recall that the result of instantiating the meta-variables in an argument form is called a "case." 

Recall that I call an argument form "valid" for a given person when it lacks counterexamples for that person. A counterexample to an argument form for a given person is a (hypothetical or real) truth-seeking debate context and a case of the argument form where they consider the premises warranted assertible and don't consider the conclusion warranted assertible. Recall also that I call an argument form simply "valid" if it is valid for people in general.

We can call an argument form "valid according to classical logic" when every instance of the argument form is valid (as a sequent) according to the rules of classical logic.

Empirically, my psychological notion of validity coincides well (but not perfectly) with the classical logic notion of validity for most people, and the more concrete the problem domain is, the more this tends to be so. In other words, especially in concrete problem domains, it tends to be the case that an argument form is valid for people in general if and only if all instances of that argument form are valid according to the rules of classical logic. However, there are definite counterexamples to this tendency.

Here is one reason one might doubt the closeness of the connection I have stated between the psychological and classical logic notions of validity. One might ask, if it is incoherent to deny a conclusion in a given context, does it follow that it is warranted to assert the conclusion in that context? When a sequent is valid according to classical logic, classical logic tells us it is incoherent to deny the conclusion in contexts where the premises are held true. When an argument form is valid in the psychological sense, people consider the conclusion warranted assertible in contexts where the premises are held true (and thus warranted assertible).

So then, if it's incoherent to deny a statement in a given context given premises that are held true in that context, then is it warranted to assert it? Not always. For example, if the premises that are held true in the context are incoherent, then any additional assertions or denials will be incoherent with the context (because adding more to an incoherent position gives you an incoherent position). 

Let us then restrict our attention to contexts where the premises held true in the context are, taken together, coherent (i.e. not incoherent according to classical logic). In such contexts, if a statement is incoherent to deny, is it warranted to assert it? Perhaps not, as it might be incoherent to deny the statement and yet there might be a social taboo/norm of some kind prohibiting one from asserting it. Let's set aside this possibility as well. Are there any other reasonable, significant ways that a statement can be incoherent to deny and yet not warranted to assert? 

Yes, there are. Suppose I have a position that is not incoherent according to classical logic. Suppose my position makes it incoherent for me to deny **P**, and that I just learned this. Suppose **P** is such a shocking, implausible statement to me that I think it's probably false. I might not consider it warranted for me to assert **P**, because I might think that the fact that my position makes it incoherent for me to deny **P** means that there is something wrong in my position. As such I might consider it unwarranted for me to assert **P** but incumbent upon me to re-evaluate the assertions and denials in my position.

A sequent (**A1**,...,**An** entails **B**) states that is incoherent to assert **A1**,...,**An** and deny **B**. Even if somebody accepts all the rules of classical logic as applied to the case, and if they believe all of **A1**,...,**An**, the validity of such a sequent cannot compel them to accept **B** as true, as they always have the option of rejecting one or more of **A1**,...,**An** instead of accepting **B**.

This reflects a broader truth about logical arguments in general. Logical arguments always proceed from premises to conclusions, but they never compel the reader to accept their conclusions. Logical arguments always cut two ways: they can compel the reader either to accept their conclusions or to reject some of their premises.

Logic is able to tell us that certain positions are incoherent. This is about all it can do. It can't tell us which of the many logically coherent sets of statements we should believe, disbelieve, assert, deny, hold to be true in a given context, hold to be false in a given context, etc.

I started out this section by presenting a one-directional notion of rules of logical inference, according to which rules of logical inference let us pass from true, warranted assertible statements to other true, warranted assertible statements. Our inquiry has shown us that this is an incomplete picture of how logic works. Every person who accepts the rules of logic and reads a logical argument has two options: to accept the conclusion or to reject one or more of the premises. In real debates logic is not necessarily used simply to pass from truth to truth. Instead, logic constrains humans to bounce about as they will in the space of logically coherent positions. In practice, of course, humans often find themselves in logically incoherent positions; it may take them a long time to notice that; and they may never notice or never care.

Suppose that (**A1**,...,**An** entails **B**) is a valid sequent according to classical logic. Suppose that I accept the rules of classical logic. Suppose **A1**,...,**An** is a coherent set of assertions (or in other words that it is not incoherent according to classical logic, or in other words that (**A1**,...,**An** entails ) is not a valid sequent (empty sequence of statements after "entails" intended)). Suppose that I accept all of **A1**,...,**An** as true and that for the purposes of this question I am not going to back down from any of these premises. Suppose that in the context under question I am not prohibited from asserting **B** by any social taboos/norms. It is incoherent for me to deny **B**. Is it therefore warranted for me to assert **B**? 

Now, finally, the answer seems to me to be "yes;" at least, I am unaware of any further qualifiers that need to be added. However, it does not follow from the rules I have already stated about classical logic that in these circumstances it is warranted for me to assert **B**. Rather, this seems to be an additional principle about classical logic:

16. **Law of inference.** Suppose (**A1**,...,**An** entails **B**) is a valid sequent. Suppose **A1**,...,**An** is coherent according to classical logic. Suppose it is warranted to assert **A1**,...,**An** in a given context. Then either it is warranted to assert **B** in the given context, or re-examination of the warrant to assert **A1**,...,**An** is warranted.

This is a restatement in the form of a rule of the question posed three paragraphs above. I have removed the qualifier that asserting **B** is not prohibited by any social taboos/norms. Any context where this would be a blocker to assertion, despite the other conditions holding, is not a context where the rules of classical logic hold sway. Now I am totally finished describing rules of classical logic, until we get to the topic of paradoxes, where I will introduce one more rule for dealing with paradoxes.

Along the way through stating the rules of classical logic and describing how to use them, we have accumulated a truly impressive series of qualifiers and disclaimers about the rules of classical logic.

Yet, we have not yet scratched the surface of the debates about what specific rules of logic are correct. Classical logic represents the dominant point of view about rules of logic are correct; but there are competing sets of rules for logic, so called **non-classical logics**, which are intended to address perceived deficiencies of classical logic.

In almost every case, non-classical logics are sets of rules which are *weaker* than the rules of classical logic, in the sense that the set of sets of assertions and denials that a non-classical logic counts as incoherent is typically a strict subset of the set of sets of assertions and denials that classical logic counts as incoherent.

How correct are the rules of classical logic? Given the complexity of the rules themselves and the trouble that has been involved in describing them, this may appear to be a difficult question. It is. Let's get started.

I will start by presenting the best argument I can to the effect that the rules of classical logic *are* correct. Specifically, I will argue that the rules of classical logic correctly capture the notion of validity, in the sense that an argument form is valid iff it is valid according to classical logic.

### Set theory

The first step in the argument, for us, has to be to look at some mathematical notions in an introductory fashion, with more rigor than we have afforded them so far. The purpose of doing this is to develop some of the vocabulary that the argument will use. The most fundamental notion I need to explain, and the most difficult to explain, is the notion of a **set**. I will offer what I consider to be the most mainstream, most widely accepted explanation of the notion of sets. This explanation is what I will call the classical notion of sets, as formalized in first-order logic and the axioms of ZFC (Zermelo-Fraenkel set theory with the axiom of choice).

Intuitively, a set is a collection of objects. Any object can be in a set, and for every set, every object either is in the set or is not in the set. The relation of set membership (according to the classical notion of sets) is a purely binary, in or out affair. Everything is either in or out of every set; nothing is both in and out of any set; nothing is neither in nor out of any set; nothing is in any sense in between in and out of any set.

Sets can be finite, or infinite. Examples of finite sets include: the set containing exactly the numbers one, two, and three; the set containing exactly [the City God Temple of Shanghai](https://en.wikipedia.org/wiki/City_God_Temple_of_Shanghai), my left hand, and the number seven; the set containing exactly the number one; and the set containing nothing, also known as **the empty set**.

Examples of infinite sets include: the set of all integers; the set of all positive integers; and the set of all sets of positive integers. The set of all sets of positive integers contains such sets as the empty set, the set containing exactly seven, three, and four, the set of all positive integers except for the numbers nine and twenty, and the set of all positive integers.

No two distinct sets have the same elements (according to the classical notion of sets). For example, there is exactly one empty set. There is exactly one set containing exactly the numbers three and five. And so forth. If two sets have the same elements, then those sets are equal, i.e. they are the same set. This is the axiom of extensionality, which is the first axiom of ZFC:

1. **Axiom of extensionality.** (for all *x*, *y*, ((for all *z*, ((*z* in *x*) iff (*z* in y))) iff (x = y)))

ZFC is a set of axioms for set theory: one we have just stated the first axiom of. The vocabulary ZFC uses contains two copulas: "in," the relation of set membership, and "=," the identity copula (the relation of being one and the same thing as). Thus the whole vocabulary of ZFC is on display in the axiom of extensionality. This axiom is simply a formal statement of the idea that sets containing the same elements are equal. This is also sometimes stated by saying: sets are extensional.

The next axiom of ZFC states that every non-empty set contains a member disjoint from itself. 

2. **Axiom of regularity.** (for all *x*, (if (for some *a*, (*a* "in" x)) then (for some *y*, ((*y* in *x*) and (not (for some *z*, ((*z* in *y*) and (*z* in *x*))))))))

One may rightly ask why one should believe what this axiom says. It is not obviously an intuitive truth about sets. It is, however, an intuitive truth about the universe of sets under the [von Neumann universe](https://en.wikipedia.org/wiki/Von_Neumann_universe) picture of the universe of sets.

The von Neumann universe is one way of intuitively motivating the axioms of ZFC. The universe of sets as described by ZFC can be organized into an infinite hierarchy of "stages." At stage 0, you have no sets. At each successive stage, you form all sets that can be formed from members of the preceding stages. At stage 1, you have just the empty set. At stage 2, you add the set containing exactly the empty set. At stage 3, you add the set containing exactly the empty set and the set containing exactly the empty set, as well as the set containing exactly the set containing the empty set. From here the number of sets added at successive stages increases extremely rapidly. After the stages 0, 1, 2, 3, etc., going on forever, one arrives at stage omega, where infinite sets start to appear. Then there are stages omega+1, omega+2, omega+3, going on forever. After all of those is stage omega+omega, also called 2omega. Then there are stages 2omega+1, 2omega+2, etc., all followed by stage 3omega, which is similary succeeded by stages 4omega, 5omega, etc. each with infinitely many stages in between, going on forever. These are all followed by stage omega*omega or omega^2, which is followed by stage omega^2+1, and on to stage omega^2+omega, eventually to omega^3 and omega^4 and so on forever to stage omega^omega, eventually to stage omega^(omega^omega), stage (omega^(omega^omega)) and on forever and what is beyond that, and so forth. This does not begin to cover the tiniest iota of the vast extent of the von Neumann universe. The sets that are produced at the stages we named are such a tiny fraction of all the sets that exist in the von Neumann universe that they are as nothing compared to infinity, only that does not begin to describe the tininess of what we have covered, which can only be understood by deep study of the mathematics of infinity.

In total, the von Neumann is a series of stages, one corresponding to each [ordinal number](https://en.wikipedia.org/wiki/Ordinal_number), where at each stage all sets of sets formed at preceding stages are formed. The vast series of numbers we begun but failed to scratch the surface of in the preceding paragraph is the series we call the **ordinal numbers**. The ordinal numbers may seem to be an incredible concept, but they can be explaind in a completely rigorous and formal way by developing them in ZFC.

What we have done so far is not a formal development of the theory of the ordinal numbers. Indeed, the whole discussion we are currently engaged in about the von Neumann universe is merely intuitive motivation for the axiom of regularity and the rest of the axioms of ZFC. This discussion is merely explanation, an aid to intuition, and does not form a part of any argument. However, everything I am saying is, to the best of my awareness, true according to the classical conception of mathematics as grounded in first-order logic and ZFC.

One interesting fact about the von Neumann universe is that in it there are only sets. There is nothing else in the von Neumann universe besides sets. In fact, this is a consequence of the axiom of extensionality as it is formulated in ZFC. However, this illustrates that obviously the axioms of ZFC are not the one definitive way of understanding sets. For example, it is useful for many purposes to talk about sets containing non-sets. However, a universe containing only sets is quite sufficient to form a foundation for all of mathematics.

The axiom of regularity is true in the von Neumann universe. The axiom of regularity is not usually necessary for doing math. [Wikipedia has a lot of interesting discussion about the axiom](https://en.wikipedia.org/wiki/Axiom_of_regularity), covering assorted implications it has. Nothing we do will depend on the axiom of regularity; I have introduced it only because it is one of the axioms of ZFC.

After the axiom of extensionality and the axiom of regularity comes an infinite series of axioms, stating for every statement *P* of first-order logic (in the vocabulary of ZFC) that for every set *A*, there exists the set of elements of *A* of which *P* is true. Each statement of first-order logic (in the vocabulary of ZFC) generates one instance of the axiom schema of specification stating the same.

3. **Axiom schema of specification.** Let *P* be a statement of first-order logic in the vocabulary of ZFC, which does not contain any free occurrences of the variable name *y*. Then the following is an axiom of ZFC: (for all *z*, (for some *y*, (for all *x*, ((*x* in *y*) iff ((*x* in *z*) and *P*))))).

The axiom schema of specification can be compared to the axiom schema of comprehension. The axiom schema of comprehension is not part of ZFC. It states, for every statement *P*, that there exists the set of objects of which *P* is true. The axiom schema of comprehension leads to logical contradictions. For example, it leads to [Russell's paradox](https://plato.stanford.edu/entries/russell-paradox/), the paradox concerning whether or not the set of all sets that are not members of themselves is a member of itself. The logical contradictions that arise from the axiom schema of comprehension have probably been the most major driving force in the development of set theory.

The von Neumann universe picture and the axioms of ZFC are an outcome of the following series of efforts (with many others contributing along the way, I'm sure): Gottlob Frege's articulation (in essence) of the laws of classical first-order logic in his  *Grundgesetze der Arithmetik*, along with some laws that led to Russell's paradox; Bertrand Russell's and Alfred North Whitehead's efforts to resolve Russell's paradox and rescue Frege's project, particularly in their *Principia Mathematica*; efforts along similar lines by Ernst Zermelo resulting in the axioms of [Zermelo set theory](https://en.wikipedia.org/wiki/Zermelo_set_theory); and further contributions by von Neumann, Abraham Fraenkel, and I assume others, leading to the axioms of ZFC.

The axiom schema of specification is the compromise ZFC makes to deal with the fact that the axiom schema of comprehension is logically contradictory. Rather than letting you form any set you can describe, ZFC tells you that you can only form any *subset* you can describe of any existing set. This appears to block Russell's paradox and the other paradoxes of set theory; nobody has been able to find a logical contradiction in ZFC.

The next axiom is a straightforward one, which states that given any two objects, there is a set containing both.

4. **Axiom of pairing.** (for all *x*, (for all *y*, (for some *z*, ((x in z) and (y in z)))))

The next axiom states given any set of sets *F*, there is a set containing all elements of elements of *F*.

5. **Axiom of union.** (for all *F*, (for some *A*, (for all *Y*, (for all *x*, (if ((*x* in *Y*) and (*Y* in *F*)) then (x in *A*)))))) 

We call the set containing exactly the elements of elements of *F* the **union** of *F*. The axiom of union does not in itself state that the union of *F* exists. One can prove that the union of every set exists in ZFC by applying the axiom of union, which yields a set containing all elements of elements of *F*, and then applying an axiom of specification to narrow down to a set containing exactly the elements of elements of *F*. For this purpose one can use the axiom of specification for the statement *P* = (for some *A*, ((*x* in *A*) and (*A* in *F*))).

After the axiom of union comes another axiom schema, another infinite set of axioms. This schema, the axiom schema of replacement, tells us that the image of any set under the image of any definable function is contained in some set.

A definable function, for our purposes, is a statement *P* relating a variable representing the input of the function, namely *x*, to a variable representing the output of the function, namely *y*. For a statement *P* to be a function over a set *A*, it must be the case that for every *x* in *A* there is exactly one *y* such that *P*.

An example of a definable function is the statement *P* = (for all *z*, ((*z* in *y*) iff (*z* = *x*))). This statement defines the function that maps any object *x* to the set *y* containing exactly *x*. 

The image of a set *A* under a definable function *P* is the set of all *y* such that for some *x* in *A*, *P*. In other words, it is the set resulting from collecting all of the output values that *P* produces for input values in *A*.

The axiom schema of replacement states that the image of any set under any definable function is contained in some set. Formally:

6. **Axiom schema of replacement.** Let *P* be a statement of first-order logic in the vocabulary of ZFC, which does not contain any free occurrences of the variable name *B*. Then the following is an axiom of ZFC: (for all *A*, (if (for all *x*, (if *x* in *A* then (for exactly one *y*, *P*))) then (for some *B*, (for all *x*, (if (*x* in *A*) then (for some *y*, ((*y* in *B*) and *P*)))))))

In this axiom we have used the quantifier "for exactly one," which is something we have not seen before, which was not part of our definition of statements of first-order logic as given back in the section titled Statements. A statement of the form (for exactly one **x**, **Q**) can be taken as an abbrevation for (for some **x**, (**Q** and (for all **y**, (if **Q**[**x** -> **y**] then **x** = **y**)))).

The axiom schema of replacement implies that there exists the image of any set under any definable function. The argument is a straightforward variation on the argument that the axiom of union implies the existence of the union of any set of sets.

The next axiom is the axiom of ZFC which allows it to prove the existence of infinite sets. If one removes this axiom from ZFC, then in the resulting system, it is not possible to prove the existence of infinite sets.

7. **Axiom of infinity.** (for some *X*, ((for some *e*, ((*e* in *X*)) and (not (for some *y*, *y* in *e*))) and (for all *y*, (if (*y* in *X*) then (for some *s*, ((*s* in *X*) and (for all *z*, (*z* in *s* iff *z* = *y*))))))))

This axiom requires significant unpacking. Let's begin with the concept of the natural numbers as they are most commonly defined in classical set theory. This is the so-called [von Neumann construction](https://en.wikipedia.org/wiki/Natural_number#Von_Neumann_construction) of the natural numbers. Here by "natural numbers," I mean non-negative integers, i.e. the numbers 0, 1, 2, 3, etc. on to infinity.

In the von Neumann construction, we equate each natural number with the set of smaller natural numbers. We equate the number zero with the empty set, since there are no natural numbers smaller than zero. We equate the number one with the set containing exactly the empty set, or in other words the set containing exactly the number zero. We equate the number two with the set containing exactly the empty set and the set containing exactly the empty set, or in other words the set containing exactly the numbers zero and one. And so on to infinity. We may call the sets which are the natural numbers as defined and constructed in this fashion, the **von Neumann natural numbers**.

The axiom of infinity, viewed at a high level, asserts that there is a set containing all the von Neumann natural numbers. The axiom of infinity asserts the existence of a set *X*. The substatement (for some *e*, ((*e* in *X*) and (not (for some *y*, *y* in *e*)))) asserts that *X* contains the empty set. The rest of the statement asserts that for every element *y* of *X*, *X* also contains the set containing exactly *y*. This axiom serves as a precursor for fairly intricate set-theoretical developments that lead ultimately to the conclusion that the infinite set of exactly the von Neumann natural numbers exists. TODO: provide a source

The next axiom, the power set axiom, implies that for every set *A*, the set of all subsets of *A* exists. This is called the **power set** of *A*.

A subset of a set *A* is a set *B* such that (for all *x*, (if *x* in *B* then *x* in *A*)). The power set axiom reads as follows.

8. **Axiom of power set.** (for all *A*, (for some *P*, (for all *B*, (if (for all *x*, (if *x* in *B* then *x* in *A*)) then *B* in *P*)))).

The final axiom, the [axiom of choice](https://en.wikipedia.org/wiki/Axiom_of_choice), is a subject of much wonderment, controversy, and [metamathematical](https://en.wikipedia.org/wiki/Metamathematics) research. 

"Informally put, the axiom of choice says that given any collection of bins, each containing at least one object, it is possible to make a selection of exactly one object from each bin." (Wikipedia)

9. **Axiom of choice.** (for all *X*, (if (for all *y*, (if *y* in *X* then (for some *z*, (*z* in *Y*)))) then (for some *f*, (for all *A*, ((if (*A* in *X*) then (for exactly one *B*, (*A*,*B*) in *f*)) and (for all *B*, (if ((*A*,*B*) in *f*) then (*B* in *A*))))))))

In order to state the axiom of choice, I have introduced a new notation: the [ordered pair](https://en.wikipedia.org/wiki/Ordered_pair) notation (*A*,*B*). Conceptually, an ordered pair is exactly what it sounds like: a sequence of exactly two objects, one after the other. In classical set theory, ordered pairs are commonly defined by [Kuratowski's definition](https://en.wikipedia.org/wiki/Ordered_pair#Kuratowski_definition), which equates the ordered pair (*A*,*B*) with the set {{*A*,*B*},{*A*}}, or in other words the set containing exactly two sets, namely: the set containing exactly *A*, and the set containing exactly *A* and *B*. Intuitively, this set can serve functionally as an ordered pair because it tells us what both the elements of the ordered pair are (namely whatever elements are in either set in the Kuratowski pair) and it tells us which is the first element of the pair (namely whatever element is in a one-element set in the Kuratowski pair). I will not pursue further formalization of the ordered pair notation, to avoid getting too distracted by details. TODO: source for formal development

The next notion the axiom of choice employs is the set-theoretic notion of a function. In set theory, a function is a type of set of ordered pairs. If *f* is a function, we can equate the statement *f*(*a*) = *b* with the statement ((*a*,*b*) in *f*). A function, by definition, is a set *f* of ordered pairs such that (for all *a*, (if (for some *b*, ((*a*,*b*) in *f*)) then (for exactly one *b*, ((*a*,*b*) in *f*)))).

My formulation of the axiom of choice states roughly that for any set *X* of nonempty sets, there is a function *f* such that for all *A* in *X*, *f*(*A*) in *A*.

That completes my presentation of the axioms of ZFC. From these axioms, together with the laws of classical logic, one can prove almost all widely accepted mathematical truths, almost all practically important mathematical truths, and in general all mathematical truths except for some esoteric or alternative truths. The known esoteric mathematical truths not provable in ZFC are often logical or set theoretical in nature. One example of a mathematical statement not provable in ZFC, but widely accepted as true, is the statement that no logical contradiction can be proven from the axioms of ZFC. Another example of a mathematical statement not provable in ZFC, but widely accepted as true, is the statement that there is an [inaccessible cardinal](https://en.wikipedia.org/wiki/Inaccessible_cardinal).

That completes the introduction of mathematical notions which is the first step in the argument I am currently presenting for the correctness of the rules of classical logic.

### Model theory

The next step in the argument is to introduce the notion of **model theory**. Model theory is a mathematical theory which describes how to apply an interpretation to a statement of first-order logic.

To introduce model theory, let's start with the concept of **truth values**. There are two truth values: true, and false. Classical model theory provides a theoretical, mathematical method for assigning truth values to all statements in a given vocabulary. In other words, it provides a method of saying (in theory) for each statement in a given vocabulary whether it is true or false.

Specifically, classical model theory applies to first order logic with some fixed sets of object literals, predicates, and copulas, which we will call a **vocabulary**. Henceforth we will assume in the background some fixed vocabulary, with the understanding that the arbitrary, fixed vocabulary we assume could stand for any vocabulary. I will use the letters t, u, v, etc. to denote object literals, and I will assume that variables *a*, *b*, *c*, etc. are among the object literals. I will use the letters P, Q, R, etc. to denote predicates. I will use the letters C, D, E, etc. to denote copulas. Recall that a predicate is a relation which applies to exactly one object: e.g., "is red" is a predicate. Recall that a copula is a relation which applies to exactly two objects: e.g., "loves" is a copula.

Most presentations of model theory allow, in addition to predicates and copulas, ternary relations which apply to exactly three things, and in general they allow *n*-ary relations for every positive integer *n*. I am departing from this common practice for the reasons that it simplifies the presentation, and that lacking relations between more than two things is not actually much of a handicap. For example, all of mathematics can be expressed in set theory, in terms of a single copula or binary relation, "in," the relation of set inclusion.

In any vocabulary which has the concept of an ordered pair, one can replace a ternary relation with a binary relation between objects and ordered pairs. Let R be a ternary relation. We will write R*abc* to denote that the relation R holds between the objects *a*, *b*, and *c*. I can define a binary relation S between objects and ordered pairs by the definition: S*a*(*b*,*c*) iff R*abc*. By repeating this operation, one can replace *n*-ary relations (for any positive integer *n*) with binary relations.

Because lacking ternary and higher *n*-ary relations is not a problem for our purposes, the choice to provide only for predicates (unary relations) and copulas (binary relations) is adequately justified.

**Definition.** A **model** *M* is an ordered pair *M* = (*D*,*I*), where:

* *D* is a set, called the **domain** of *M*. For convenience, we shall assume that all elements of *D* are object literals. This means that we automatically have a way to refer to elements of *D* in statements.
* *I* is a function, called the **interpretation** of *M*, defined on all words in the vocabulary:
   * For each object literal t, *I*(t) is an element of *D*. If t in *D*, then *I*(t) = t.
   * For each predicate P, *I*(P) is a subset of *D*.
   * For each copula C, *I*(C) is a binary relation on *D*, or in other words a set of ordered pairs of elements of *D*.

A model is a context or a possible world in which statements can be said to be true or false. Given a model *M*, we can say for any statement *P* of first order logic (in our fixed vocabulary) whether or not *P* is true in *M*. We define whether any statement *P* is true in *M* as follows:

1. For any predicate **P** and any object literal **t**, **Pt** is true in *M* iff *I*(**t**) in *I*(**P**).
2. For any copula **C** and any object literals **t** and **u**, **Ctu** is true in *M* iff (*I*(**t**), *I*(**u**)) in *I*(**C**).
3. For any statement **P**, (not **P**) is true in *M* iff (not (**P** is true in *M*)).
4. For any statements **P** and **Q**:
    * (if **P** then **Q**) is true in *M* iff (if (**P** is true in *M*) then (**Q** is true in **M**)).
    * (**P** and **Q**) is true in *M* iff **P** is true in *M* and **Q** is true in *M*.
    * (**P** or **Q**) is true in *M* iff **P** is true in *M* or **Q** is true in *M*.
5. For any statement **P** and any variable name **x**:
    * (for all **x**, **P**) is true in *M* iff for all *a* in *M*, **P**[**x** -> *a*] is true in *M*.
    * (for some **x**, **P**) is true in *M* iff for some *a* in *M*, **P**[**x** -> *a*] is true in *M*.

Let's take a breather to deal with a technicality. Earlier I wrote:

"We will consider a context to provide a partial mapping from object terms to objects, saying for some subset of the set of all of context-sensitive object terms (i.e. variables and context-sensitive object literals) what objects they denote. In our approach, statements containing free occurrences of variables whose denotations are not defined by a given context, and statements containing context-sensitive object literals whose denotations are not defined by that context, will be uninterpretable in that context."

Yet, in the definition of "model," I wrote: "the **interpretation** of *M*, defined on all words in the vocabulary." How do these statements square with each other? In a simple way: terms that aren't defined in the given context are not considered part of the vocabulary. Technicality handled.

We now have a mathematical way of understanding the concept of a context, for the purposes of judging whether statements of first-order logic are true or false in that context according to the rules of classical logic. The reader may note the odd circularity in the rules, which reminds of the *T*-schema: (**P** and **Q**) is true in *M* iff **P** is true in *M* and **Q** is true in *M*, and so forth.

What grounds this type of *prima facie* circular definition is that the definition of "model" is implicitly stated in the context of classical first-order logic and set theory, as axiomatized in ZFC. We have, in this text, skipped the extremely laborious mathematical developments required to define and prove, in ZFC and classical first-order logic, essential properties of basic mathematical concepts such as the definition of "model" and the definition of truth in a model. We have skipped over these formal details in this text, but all of these details can be filled in by any logician worth their salt who chose to devote the time the task would take. TODO: provide a source of formal development

In short, the point is this. We know how to use the word "and" as it occurs in the phrase "**P** is true in *M* and **Q** is true in *M*," because the rules for first-order logic which I have provided tell us how to use the word "and." There is therefore no circularity in the definition of whether (**P** and **Q**) is true in *M*. The definition is grounded in the rules of classical logic as described earlier.

Model theory is an interesting achievement in which we have been able to turn first-order logic back on itself. By using first-order logic and some well-chosen axioms of set theory, we are able to develop all of mathematics, including a mathematical theory called model theory which can be used to study the concept of truth as implicitly described in the rules of classical first-order logic.

Model theory is philosophically valuable because it offers a more explicit way of looking at the concept of truth as understood by classical logic. It is also specifically valuable for the present purpose. My present purpose is to present the best argument I can that the rules of classical logic are correct. Model theory is an essential tool in this argument. The whole argument up to this point has been laying the groundwork of set theory and model theory, which I can now put into motion to argue that the rules of classical logic are correct.

Here is a natural question. Suppose that (**A1**,...,**An** entails **B**) is a valid sequent, and that **A1**,...,**An** are true. Does it follow that **B** is true? It does in the following sense: 

**Soundness theorem.** Let **A1**,...,**An**, **B** be statements. Suppose that (**A1**,...,**An** entails **B**) is a valid sequent. Let *M* be a model. Suppose that **A1**,...,**An** are true in *M*. Then for any model *M*, if **A1**,...,**An** are all true in *M*, then **B** is true in *M*.

**Proof.** TODO

Here's another natural question. Can we reverse the conditional in the soundness theorem? In other words, if in any model *M* where **A1**,...,**An** are true, **B** is also true, does it follow that (**A1**,...,**An** entails **B**)? The answer is "yes."

**Completeness theorem.** Let **A1**,...,**An**, **B** be statements. Suppose that for any model *M*, if **A1**,...,**An** are all true in **M**, then **B** is true in *M*. Then (**A1**,...,**An** entails **B**) is a valid sequent.

**Proof.** TODO

In summary, the rules of classical logic I have described are sound and complete with respect to classical model theory. We can restate the soundness and completeness theorems together as one statement, in the following way.

Say that a sequent (**A1**,...,**An** entails **B**) **has a counterexample** iff there is a model *M* such that **A1**,...,**An** are true in *M* and **B** is false in *M*. The soundness and completeness theorems, taken together, state that (**A1**,...,**An** entails **B**) is a valid sequent if and only if it has no counterexample.

One philosophical reading of this mathematical fact is to say that the sequent (**A1**,...,**An** entails **B**) is valid if and only if **B** is true under any possible way of bestowing precise meanings to the vocabulary in **A1**,...,**An**, **B** such that  **A1**,...,**An** are true. This reading assumes that every possible way of bestowing precise meanings to the vocabulary in **A1**,...,**An**, **B** can be represented by a model in the sense I have defined.

I think the assumption of the reading is true, if we understand a "precise meaning" for a predicate as necessarily saying for every object that the predicate is true or is false of the object, and similarly for copulas. 

I don't take model theory to be a theory of meaning. For me, meaning is a psychological notion, as I elaborated in the section titled Language and the section titled Meaning. I don't think classical model theory offers a plausible or realistic model of human psychology; for example, human psychological processing does not seem to readily classify every single object in the universe as either satisfying or not satisfying any given predicate. 

The thesis I have agreed to does not construe model theory as a theory of meaning. Rather, it hypothesizes a connection between precise meaning and classical model theory: namely, that every possible way of bestowing precise meanings to the terms of a vocabulary can be represented by a model. Such a representation is not a *complete* representation of such precise meanings; it only goes as far as is required to determine for every first-order logic statement over the given vocabulary whether the statement is true or false. It does not, for example, capture anything of the subjective qualities of the meanings of the terms as experienced by some language user. Moreover, these precise meanings are not necessarily a kind of meaning in the psychological sense, because meanings of predicates in the psychological sense seldom satisfy the preciseness condition of classical logic of classifying every object in the universe as either satisfying or not satsifying the predicate. Precise meanings, in this discussion, are therefore a theoretical concept not assumed to be a type of speaker or listener meaning. Precise meanings can be thought of as some kind of idealization of meanings in the psychological sense.

In short, here is the conclusion I've arrived at. The sequent (**A1**,...,**An** entails **B**) is valid according to classical logic, if and only if there is no possible set of precise meanings for the vocabulary in the sequent under which **A1**,...,**An** are true and **B** is false. We can shorten this conclusion further by saying: an argument form is valid according to classical logic iff it has no precise counterexamples.

Everything I said since I started discussing model theory can be generalized pretty straightforwardly to multiple conclusion sequents, though I carried out the whole discussion only in terms of single conclusion sequents. TODO: go through and double check that

### Vagueness

A precise meaning for a predicate classifies every object in the universe as satisfying or not satisfying the predicate. In the real world of language use, predicates are rarely precise in this sense.

Predicate words in natural languages are almost always **vague**. What this means is that one can find intermediate cases where it is unclear whether or not the predicate applies. This situation is classically described with [the paradox of the heap](https://en.wikipedia.org/wiki/Sorites_paradox). 

Imagine a heap of sand. Imagine the process of removing sand from the heap one grain at a time, going on indefinitely until no sand remains where the heap was. At some point, one no longer has a heap of sand. This point is probably reached before the sand is entirely gone. For example, one grain of sand is probably not a heap. Two grains of sand is probably not a heap. If it's unclear whether or not 137 grains of sand in a pile should be called a heap of sand, then such a collection of sand grains is an intermediate case of the predicate "is a heap." Maybe it's clear to you that 137 grains of sand is (or is not) a heap. What about two grains? What about seven? What about 500? Whatever your intuitions about heaps are, I imagine it's clear that there are numbers of sand grains which when piled together create intermediate cases for the predicate "is a heap," where it's not obvious whether or not to call those collections of sand grains "heaps."

If "is a heap" is a precise predicate, then at some point, removing a single grain of sand from the heap must cause us to pass from having a heap of sand to not having a heap of sand. However, it seems intuitive that a single grain of sand cannot make the difference between having a heap of sand and not having a heap of sand. The usual conclusion is that "is a heap" is a **vague predicate**. "Is a heap" is somehow different from a precise predicate, in such a way that some objects are neither discernibly heaps nor discernibly non-heaps.

There are many different theories of what it means for a predicate to be vague, and of how to make sense of vague predicates and their seemingly paradoxical nature. [PhilPapers has catalogued over a thousand scholarly works on vagueness and indeterminacy.](https://philpapers.org/browse/vagueness-and-indeterminacy) The academic debate around vagueness is, as you might guess, stunningly intricate and complicated.

I don't propose to provide a theory of vague predicates in this work. I don't think one needs a theory of vague predicates in order to make effective use of them in arguments, or in life in general. 

For the purposes of the present argument for the correctness of the rules of classical logic, the challenge posed by vagueness is that it challenges the applicability of the rules of classical logic to real world problems. A sequent is valid according to classical logic iff it has no precise counterexamples. The existence of vagueness and other forms of non-preciseness in real predicates might make us wonder: are there invalid argument forms which lack any precise counterexamples, or valid argument forms which have precise counterexamples? If so, then the rules of classical logic are not necessarily correct in all cases.

We can quickly rule out the possibility of valid argument forms which have precise counterexamples. An argument form with precise counterexamples has convincing counterexamples, and so it is not valid. 

* Here I am using "valid" in the sense of "valid for people in general" explained earlier.
* I am using "counterexample" in the sense of a person, a (hypothetical or real) truth-seeking debate context, and an instance of the argument form where the person would generally consider the premises warranted assertible and the conclusion not warranted assertible.
* I am using "convincing counterexample" in the sense of a counterexample which generalizes to people in general, meaning that the debate context and the argument will form a counterexample for more or less anybody (given sufficient time, attention, openness to the process of truth seeking debate, etc.) I take it to be true by the definitions of these terms that if an argument form has a convincing counterexample, then it is not valid.
* I am using "precise counterexample" in the sense of a counterexample where the terms in the argument have precise meanings, the premises of the argument are true, and the conclusion is false.

The one-sentence argument just given, for the conclusion that there are no valid argument forms with precise counterexamples, relies on the assumption that every precise counterexample is a convincing counterexample. This assumption is intuitively plausible, but I have not argued for it yet. My argument for the assumption is that as far as I am aware, when it comes to practical applications of logic, there are no convincing counterexamples to the assumption that every precise counterexample is a convincing counterexample. If given such a counterexample, then I would need to re-evaluate my position. I haven't come across one and I can't conceive of one, but I don't rule out the possibility of one existing. 

That completes my argument that there are no valid argument forms with precise counterexamples. It is not a water-tight argument, to be sure. This doesn't bother me very much, for the reason that I don't consider it possible to give a water-tight argument for the correctness of a set of rules of logic. 

One reason for this is that as far as I can tell, any argument for the correctness of a set of rules of logic must employ rules of logic. I have been employing the rules of classical logic all throughout this argument for the correctness of the rules of classical logic, mostly implicitly; for example, in order to derive, from the axioms of ZFC, the mathematical facts that I have used in the argument.

In some sense, any argument for the correctness of a system of rules of logic will be circular, since it will almost inevitably employ the rules of logic it is trying to justify. This is an interesting paradox. It would be a problem for my view if my present goal was to establish knowledge of the correctness of the rules of logic. However, my goal is merely to justify the idea that the rules of classical logic are correct from a pragmatic standpoint: in particular, that using the rules of inference of classical logic will not lead us to some kind of pragmatic error, that they will not in practice lead us from truth into falsehood, and that these rules can safely and generally be employed in the practice of constructing winning arguments.

I am comfortable that I can safely and reasonably use classical logic in the course of making this argument for the following reasons. Though I don't think I know anything about the world outside my own consciousness, I operate in the context of a subjective series of experiences affected by a reality that is hostile in many ways to my existence and my goals. I am aware of many risks and problems, and many ways of dealing with them. I am able to accomplish a great deal in the world and to do so with a degree of dependability that inspires people to rely on me for important tasks. Logic is my most central tool, and certainly my most reliable cognitive tool, for accomplishing this.

More generally, my cognition is a tool that is part of my being which I use to do work. In the present context, I am applying my cognition to do the work of introspecting on my methods of cognition and investigating their reliability for the purposes of doing any kind of work. I am employing logic in this work, as I would with any other work where logic could help me to progress. 

It is a possible outcome of the investigation that logical principles which I currently deem to be reliable are shown to be unreliable for some purposes. In such an event, I would change my views about what rules of logic are reliable according to what was shown. Until that happens, however, I have no hesitation about using the rules of logic that I ordinarily believe and assume to be reliable -- namely, the rules of classical logic -- in the course of the investigation. My views about the rules of logic are in a state of [reflective equilibrium](https://plato.stanford.edu/entries/reflective-equilibrium/).

Earlier I raised two questions expressing doubts about the correctness of the rules of classical logic. These questions are pertinent even if we accept the conclusion I arrived at that a sequent is valid according to classical logic iff it lacks precise counterexamples. The first question was whether there are any valid argument forms with precise counterexamples. I answered "no" to this question.

The second question was whether there are invalid argument forms which lack precise counterexamples.  This is a trickier question. Whether or not such argument forms exist is a primary and crucial topic of debate between believers in classical logic and believers in non-classical logics. Because this is such a complex and contentious debate, I do not harbor the ambition to resolve the debate in this text. What I will do is offer the best argument I can to the effect that every invalid argument form has a precise counterexample, and that from a pragmatic standpoint we can safely operate under this assumption.

I want to argue that there are no pragmatically convincing counterexamples to the claim that every invalid argument form has a precise counterexample. Stated differently, the claim is that there is no real world context where practical problems would arise from assuming that some argument form with no precise counterexamples is valid. My basis for this claim is that I have looked for such pragmatically convincing counterexamples without finding them. I will illustrate the difficulty of finding such pragmatically convincing counterexamples by exploring some avenues for producing such counterexamples that may seem promising, and explaining why it seems to me that those avenues yield no such counterexamples.

Let us first consider the topic of vagueness. Can vague predicates provide us with any pragmatically convincing examples of invalid argument forms with no precise counterexamples?

Let's start with a very simple example: the argument form ( entails (**A** or (not **A**))). This argument form has an empty sequence of premises and a single conclusion. All instances of this argument form are valid sequents in classical logic. According to classical logic, for any statement **A**, either **A** is true or its negation (not **A**) is true. According to classical logic, an equivalent way of stating this is to say that for any statement **A**, either **A** is true or **A** is false. This principle is sometimes called the **law of the excluded middle**, or **LEM** for short.

Many proponents of non-classical logics reject the law of the excluded middle. Many also reject the idea that all of the formulations of LEM given in the previous paragraph are equivalent statements. The main question for our purposes is: are there any pragmatically convincing counterexamples to LEM? 

Vague predicates provide a potentially promising avenue for finding pragmatically convincing counterexamples to LEM. All we need to do is to find a pragmatic context where assuming for a specific statement **A** that the statement (**A** or (not **A**)) is true will lead us into some kind of pragmatic error.

Note that it is not sufficient to find a case where philosophical reflection tells us that a statement is, intuitively speaking, neither true nor false. Such cases, one might argue, are easy to find by considering intermediate cases of vague predicates. For example, is a human male "tall" if they are exactly the average height of a human male plus an inch, or a half inch, or a quarter inch, etc.? An example such as this one does not by itself meet the burden of proof I have defined for the non-classical logician. We need a case where there is something erroneous about assuming that the statement (**A** or (not **A**)) is true, and the error could matter for some practical purpose.

Suppose one needs to classify human males as tall or not tall for some practical purpose. For example, one can suppose we are trying to select players for a men's basketball team, and we are trying to follow the rule that we will only select players who are tall. Or, one can suppose that we are recruiting male soliders for an elite division of an army, and that one of the physical requirements is that the recruits be tall.

If one needs to classify human males as tall or not tall for some practical purpose, then in practice one will usually define a semi-arbitrary exact cut-off point, some height measurement *x*, stipulating that all males of height less than *x* are not tall, and all males of height more than *x* are tall. If one encountered a male whose height was exactly *x*, to within the limits of the precision of the measuring apparatus, then one would semi-arbitrarily make a decision as to whether or not they count as tall for whatever practical purpose is at hand. 

The generalization of these comments is that in practical contexts, when one is dealing with vague predicates (e.g. "tall"), intermediate cases are handled (usually) by semi-arbitrarily classifying the object as satisfying or not satisfying the predicate in question. 

In contexts where this is the rule, we can think of vague predicates as being incomplete precise predicates, whose definitions are extended on an *ad hoc* basis to deal with intermediate cases as they arise.

Let's think about pragmatic situations where one encounters an intermediate case of a vague predicate and one's response is not to classify the object as satisfying or not satisfying the predicate. In such situations, the pragmatics of the situation do not require one to classify the object as satisfying or not satisfying the predicate. Instead, one is somehow able to work around making such a classification. Such cases will only pose a problem for the argument I am making if it can be shown that classifying the object as satisfying or not satisfying the predicate would lead to some form of pragmatically significant error. I haven't come up with any such examples. I am interested if you have any. For the time being, I operate under the assumption that there are none.

I will assume going forward in the argument that there are no pragmatic situations where classifying an object as satisfying or not satisfying a vague predicate will lead to some form of pragmatically significant error (regardless of which of the two classification options is chosen). If this assumption holds, then it rules out vagueness as a source of pragmatically convincing counterexamples to the claim that every invalid argument form has a precise counterexample. This is because if one has a vague counterexample to an invalid argument form, one can in principle turn it into a precise counterexample by filling in all details of the vague predicates (and copulas), classifying every object (or pair of objects) in the domain/universe as satisfying or not satisfying each predicate (or copula).

### Intuitionism

What other potential areas can we look at for pragmatically convincing counterexamples to the claim that every invalid argument form has a precise counterexample? 

Scholarly readers will likely expect a discussion of [intuitionism](https://en.wikipedia.org/wiki/Intuitionism), a school of thought founded by the mathematician and philosopher [L.E.J. Brouwer](https://en.wikipedia.org/wiki/L._E._J._Brouwer).

Here is my summary of some core theses or assumptions of intuitionism. I don't assume that all intuitionists would agree with all of these statements, but they represent my best attempt to represent the views of intuitionists in general and simple terms. I have weakened phrasing that my sources use in some places (particularly, by removing the term "know"/"knowledge"), in order to assist the presentation going forward.

1. Immanuel Kant, in the Critique of Pure Reason, was correct in arguing that there are *synthetic a priori truths*. [A priori truths](https://en.wikipedia.org/wiki/A_priori_and_a_posteriori) are truths which we can discern by introspection alone, without engaging in empirical investigation. [Synthetic, as opposed to analytic, truths](https://plato.stanford.edu/entries/analytic-synthetic/) are truths that are not true merely by virtue of the definitions of the words they contain, but rather are true by virtue both of the definitions of the words they contain and by virtue of facts about the world. Synthetic a priori truths, then, are truths about the world which we can discern by introspection alone, without engaging in empirical investigation.
2. Mathematical statements are synthetic a priori truths. One verifies mathematical statements by means of constructing mathematical objects in one's mind and observing their properties.
3. The rules of classical logic are not correct in all cases in the context of mathematics. The rules of logic that are correct in the context of mathematics are the rules of [intuitionistic logic](https://en.wikipedia.org/wiki/Intuitionistic_logic). All of the argument forms that are valid according to intuitionistic logic are valid according to classical logic. The converse is not true; some argument forms that are valid according to classical logic are invalid according to intuitionistic logic.

If intuitionism, as represented by these three statements, is correct, then evidently the rules of classical logic are not correct in all contexts. 

I'm going to give my personal opinion on intuitionism. I don't lean on these opinions as assumptions of my argument, but this explanation of my opinion is part of the exposition of the argument, because I'm going to grant the intuitionists the assumptions they have that I myself agree with, in order to construct an argument that's compelling to me (but I hope to the reader as well).

I agree with statement 1. I agree with statement 2, except that crucially I replace the word "constructing" with the word "observing," a replacement which substantially modifies the meaning of the statement. I am a [Platonist about mathematics](http://www.iep.utm.edu/mathplat/). I believe that mathematical objects exist in an infinite, timeless realm of heavenly forms, and that humans can discern mathematical truths by observing mathematical objects. I will go into this topic more in the section titled Math. My present argument does not in any way depend on the assumption that mathematical Platonism is true. However, I do make use of the concept of mathematical Platonism in making the mechanics of the argument run.

For our purposes, the key question is whether intuitionists can make a pragmatically convincing case for their position. According to intuitionists, some mathematical statements are neither true nor false, because they can neither be verified nor falsified by means of correct mathematics. Since the statement (**A** or (not **A**)) expresses that **A** is either true or false, according to intuitionists, following the rules of classical logic will sometimes lead one from truth to untruth. The key question for us is whether their case can be made pragmatically convincing.

It is informative to consider my difference with intuitionists on the topic of whether we construct mathematical objects in our minds, as opposed to merely observing them in our minds. Does this difference of philosophical opinion have any pragmatic significance? Does it matter, in any practical case, which opinion is correct, if either of them? Only if there is some practical application of mathematics where classical mathematics brings us to conclusions that are untrue in a way that is significant and bad for some practical purpose, and where intuitionistic mathematics does not lead us into the same untruth. Having looked for such a case, I'm not aware of such a case. Going forward in the argument, I assume there is no such case, but please inform me if you think of one.

It follows from this assumption that pragmatically, it does not matter whether statement 2 (in my description of intuitionism) is true or false. To recall, here is statement 2. "Mathematical statements are synthetic a priori truths. One verifies mathematical statements by means of constructing mathematical objects in one's mind and observing their properties." One way statement 2 can be false is if mathematical Platonism is true, so that mathematical objects are not constructed by humans, but merely observed by humans. Since it doesn't matter pragmatically whether intuitionism or mathematical Platonism is true, it doesn't matter pragmatically whether statement 2 is true or false.

If mathematical Platonism is true, then most would agree that it is coherent for statement 3 to be false. To recall, statement 3 is the following. "Not all rules of classical logic are correct in the context of mathematics. The rules of logic that are correct in the context of mathematics are the rules of intuitionistic logic." I think most philosophical logicians would agree that it is coherent and logically possible for mathematical Platonism to be true and for all mathematical predicates to be precise, and that this would imply that all the rules of classical logic are valid for mathematics, in the sense that the rules of classical logic would not allow inferences from true mathematical premises to false mathematical conclusions. Henceforth I will assume that it is coherent for mathematical Platonism to be true and statement 3 to be false.

It follows from what I have said that if mathematical Platonism is true then statement 2 is false, and that it is coherent for mathematical Platonism to be true and statement 3 to be false. According to what I have so far assumed, it is coherent for mathematical Platonism to be true. It follows that is coherent for statements 2 and 3 both to be false. Additionally, as far as I can see, there is no practical application of mathematics where it is pragmatically significant whether statements 2 and 3 are both true (as intuitionists hold) or whether they are both false. I will assume the preceding sentence going forward, but please inform me of any counterexamples you find.

From what has been assumed it follows that it does not matter pragmatically whether or not intuitionists are correct in saying that the rules of classical logic are not valid in all cases in the field of mathematics (this being part of the content of statement 3). In other words it follows from what has been assumed that intuitionists have no pragmatically convincing counterexamples to the claim that every invalid argument form has a precise counterexample. That completes my pragmatic defense against the intutionist attack on classical logic. This defense is pragmatic indeed for our purposes in that it means we have not encountered a need to learn/teach another system of logic, such as intuitionistic logic, in addition to classical logic.

### Paradoxes

The next category of challenges to classical logic which we will consider are the challenges posed by logical paradoxes, and schools of non-classical logic which advocate for ways of weakening the rules of classical logic in order to resolve paradoxes.

We are still in the midst of my argument that for practical purposes, classical logic correctly captures the notion of validity, in the sense that an argument form is valid iff it is valid according to classical logic.

The overall structure of the argument is fully on display at this point. First I argued that an argument form is valid according to classical logic iff it has no precise counterexamples. This is a philosophical interpretation of the soundness and completeness theorems for classical logic. Since then I have been defending against possible counterexamples to the claim that an argument form is valid iff it has no precise counterexamples. The overall strategy is to argue from a pragmatic perspective for the following two claims:

1. An argument form is valid according to classical logic iff it has no precise counterexamples.
2. An argument form is valid iff it has no precise counterexamples.

From these two claims it follows by logic that:

3. An argument form is valid iff it is valid according to classical logic.

This statement is the goal of the whole argument, and the goal of the whole section titled Logic.

In this subsection, the last subsection, our question will be: do any paradoxes give rise to pragmatically convincing counterexamples to the claim that an argument form is valid iff it has no precise counterexamples? If so, then the paradoxes pose a challenge to classical logic which matters from the pragmatic perspective we are taking. If not, then they don't.

Paradoxes are the last place I am going to look in this text for potential pragmatically convincing counterexamples to the statement that an argument form is valid iff it has no precise counterexamples. It's not that this is the last place that there is to look. Rather, there are inexhaustibly many places to look for potential counterexamples to the statement. Yet, I have to end the argument somewhere. I have tried to cover what I think are the most important challenges to classical logic; paradoxes are our last stop. This topic is impossible to cover comprehensively. [PhilPapers has catalogued over 2,000 scholarly works on paradoxes.](https://philpapers.org/browse/paradoxes) Therefore this argument will be based on a very shallow/narrow analysis of the topic of paradoxes, relative to the extent of the literature.

Let's collect together some paradoxes to work with. In this section I will analyze the following paradoxes:

1. **Russell's paradox.** Let *R* be the set of all sets which do not contain themselves. In other words *R* is the set of all objects *x* such that ((*x* is a set) and (not (*x* in *x*)). If there is such a set *R*, then in classical logic contradiction ensues, from the observation that (*x* in *x*) iff (not (*x* in *x*)).
2. **The liar paradox.** Let *L* be the statement (*L* is false). If there is such a statement *L*, which is the statement that it itself is false, then in classical logic contradiction ensues, from the observation that *L* is true iff *L* is false.
3. **A [paradox of relevance](https://philpapers.org/browse/relevance-logic/).** Let **A** be any true statement and let **B** be any statement. It can be shown in classical logic that (if **B** then **A**) is true. For example, since the sky is blue, it is true according to classical logic that (if (the sky is red) then (the sky is blue)). However, the latter statement is intuitively false.

The goal of this section is to provide a pragmatically satisfying resolution to these three paradoxes. This is a very minimal list of paradoxes. Russell's paradox and the liar paradox are representative examples of the most pressing category of logical paradox. These are paradoxes which allow us to prove anything on the basis of the rules of classical logic and elementary, plausible assumptions. I have also chosen one example of a paradox of relevance: another type of paradox plucked out of the large field of relevance logic, which studies logics intended to correctly capture the notion of relevance. By solving these three paradoxes, I will illustrate methods which can be applied to solve, not all paradoxes, but all paradoxes I'm aware of that are paradoxical in virtue of resulting in logical contradictions. TODO: find counterexamples to this claim

Russell's paradox made a big splash in the development of mathematical logic and the foundations of math. Consideration of this paradox was probably the most pivotal process in the development of those disciplines. Let me explain the history.

In my awareness, Aristotle was the first to formulate a useful system of rules of logic. [Aristotle's logic](https://en.wikipedia.org/wiki/Term_logic) allowed for the classification of certain simple arguments as valid or invalid. However, Aristotle's logic did not obtain to anything resembling the generality of first order logic, which can be used to analyze the validity of essentially any precisely formulated argument. 

The next development I'll mention is George Boole (1815-1864)'s discovery or invention of so-called [boolean logic](http://www.i-programmer.info/babbages-bag/235-logic-logic-everything-is-logic.html). This forms most of the basis for the biggest practical application of logic, which is in the design and programming of computers.

The next development I'll mention is the development of predicate logic (a category of logics including first order logic). As far as I understand, predicate logic was more or less independently conceived by [Guiseppe Peano](https://en.wikipedia.org/wiki/Giuseppe_Peano) and [Gottlob Frege](https://en.wikipedia.org/wiki/Gottlob_Frege). The discovery/invention of predicate logic occurred in the late 1800s and early 1900s, in a flurry of seminal research which is without a doubt in my mind the most important period so far in the history of logic. I think the full potential impact on society of the innovations of this period has yet to be felt.

This period is also the most important period in the history of set theory. On the basis of first order logic, people first put set theory on rigorous logical foundations, with ZFC being the most popular way of doing this. These developments in turn conceptually put all of mathematics on rigorous logical foundations, as it became apparent that all definitions of mathematics could be constructed on top of basic set theory notions, and that essentially all mathematical theorems could be proven from the axioms of ZFC via the rules of inference of first order logic. These are the developments which brought rigor to mathematics, resolving for the most part various doubts and controversies that had plagued mathematics.

One controversy was the controversy over the correctness of [Cantor's results in set theory](https://en.wikipedia.org/wiki/Georg_Cantor#Set_theory). Another historical controversy in math was [the controversy](https://en.wikipedia.org/wiki/History_of_Grandi%27s_series) regarding [Grandi's series](https://en.wikipedia.org/wiki/Grandi%27s_series). There were various doubts and controversies surrounding [calculus](https://en.wikipedia.org/wiki/Calculus#History) until it was put on rigorous foundations.

"The crisis of foundations in math" usually refers to the time in the late 1800s when the hunt for rigorous foundations of mathematics was at the peak of its intensity, driven in part by paradoxes like Russell's paradox which posed serious challenges to the coherence of mathematics. This crisis of foundations is usually seen as having settled down after rigorous foundations of math were obtained, and they were studied in sufficient depth that people became satisfied that the foundations were coherent and unlikely to contain hidden contradictions.

Russell came up with his paradox through studying Frege's [Grundgesetze der arithmetik](https://en.wikipedia.org/wiki/Gottlob_Frege#Logic.2C_foundation_of_arithmetic). Russell's paradox demonstrated that Frege's logical foundation of mathematics, as described in the Grundgesetze, was logically inconsistent. Frege was never able to repair his own system, but Russell showed a way to resolving the paradox, which was then refined by other scholars, resulting in first order logic and the axioms of ZFC.

As far as most of the mathematical community is concerned, these developments resolved the crisis of foundations. However, many scholars continue to be concerned with the foundations of math, and many find fault with the foundations provided by first order logic and ZFC.

We have already discussed the attacks of the intuitionists, which argue for weaker logic and weaker mathematical axioms which can prove less, in order to bring math into alignment with that which intuitionists consider humans to be able to intuit.

A very different line of attack has been advanced by philosophers who take issue with the means by which the axioms of ZFC resolve set-theoretic paradoxes such as Russell's paradox. In my mind the most important trailblazer of this way of thinking was [Graham Priest](https://en.wikipedia.org/wiki/Graham_Priest). I will carry out this line of attack now.

[The Von Neumann universe](https://en.wikipedia.org/wiki/Von_Neumann_universe) provides a picture of the universe of set theory which the axioms of ZFC describe. In the von Neumann universe, we begin at stage zero with no sets, and at each successive stage, we add all the sets which have not yet been formed and which can be formed from elements of the preceding stage. We run through all of the ordinal numbers building stages, resulting in a whole lot of sets.

Some sets are never formed in this way. For example, in the von Neumann universe, there is no set of all sets. Each set is produced at some stage; each set contains only sets from preceding stages; and for each stage, there are infinitely many stages after it. Thus there can be no set of all sets in the von Neumann universe.

It is relatively easy to prove from the axioms of ZFC that there is no set of all sets. Assume the axioms of ZFC. Suppose, in order to reach a contradiction, that there is a set of all sets. Then, by using an instance of the axiom schema of specification, we can produce the set of all sets which do not contain themselves, which is a subset of the set of all sets. Having produced the Russell set, we are led a contradiction.

Similar arguments show that in ZFC there is no set of all [groups](https://en.wikipedia.org/wiki/Group_(mathematics)); no set of all [binary relations](https://en.wikipedia.org/wiki/Binary_relation); no set of all [topological spaces](https://en.wikipedia.org/wiki/Topological_space); no [category of all categories](https://en.wikipedia.org/wiki/Category_(mathematics)); no category of all [rings](https://en.wikipedia.org/wiki/Ring_(mathematics)); and so forth. Lots of things that are natural to talk about, that would be useful to talk about for many purposes, that seem to some form of common sense like they ought to exist, don't exist according to ZFC.

A common and reasonable response to this complaint is that while it's a shame to be unable to talk about these things, it is after all logically contradictory to do so, so we need to accept the reality that these notions are incoherent.

I don't know about you, but that response is not satisfying to me. One reason for this is that the logical paradoxes don't just create problems for talking about very large sets. They also create problems for talking about language.

That brings us back to the liar paradox. The liar paradox concerns any statement *L* such that *L* is equivalent to the statement (*L* is false). For example, the English sentence "this sentence is false" is such a statement.

If anything, the liar paradox poses a challenge to the idea that classical logic is applicable to English. Statements can be formulated in English which, by the rules of classical logic, make it possible to formulate statements which are true if and only if they are false, which by classical logic allows us to infer any statement. Can we then with justification employ the rules of classical logic in debates conducted in English? I think so, generally speaking, but the attack remains to be defended against.

Let's turn to the problem of defending against the attack on classical logic and ZFC which I have just carried out. My defense will grant (for the sake of argument) the following premises of the argument:

1. It is unintuitive to disallow talk about the set of all sets and such objects. It is desirable to have a way to coherently discuss these sets.
2. It is desirable to be able to apply classical logic to English without worrying about incoherence resulting from paradoxes such as the liar paradox.

The defense will demonstrate a way to be able to coherently discuss the set of all sets and such objects, and a way to apply classical logic to English without worrying about incoherence from the liar paradox and such paradoxes.

The standard of success is pragmatic adequacy. The solutions I come up with should be, ideally, suitable for whatever useful purposes people are going to put them to. There is no higher standard of theoretical perfection (e.g. perfect precision, perfect well-definedness) that I am trying to reach. The standard of success is pragmatic adequacy.

Along the way, I will review, to a shallow extent, the literature on the other approaches that have been developed and which I am not taking. This will help me to make my case for the approach I am taking.

I'm going to start with the Russell paradox. It will be useful for the structure of my argument to look at the Russell paradox first and in much more detail than the liar paradox, because set theory is a much clearer, sharper context within which to examine the essential underlying problem behind both of these paradoxes. It is also by far the context with which I am more familiar. The solution I arrive at for the Russell paradox will carry over more or less identically to the liar paradox, and (as I said before) to every other logical paradox I'm aware of.

Let's study a little more formally how the Russell paradox arises. The Russell paradox arises in the context of so-called **naive set theory**. In one sense, "naive set theory" refers to set theory as it was practiced before the rigorous formalization of set theory, and to modern equivalents. In another, more specific sense, "naive set theory" can refer to set theory based on a specific logic and set of axioms in which any describable set can be formed and therefore the Russell paradox arises.

I'll present an axiomatization of naive set theory in this latter sense. This axiomatization is a standard, prototypical axiomatization of naive set theory. It is set in classical first order logic, in the same vocabulary as ZFC, containing two copulas: "in" and "=."

The axiom of extensionality states that if two sets have the same elements, then they are equal (and conversely). You can't have two distinct sets with exactly the same elements.

1. **Axiom of extensionality.** (for all *x*, *y*, ((for all *z*, ((*z* in *x*) iff (*z* in y))) iff (x = y)))

The axiom schema of comprehension is an infinite set of axioms stating in essence that if you can describe a set by stating what conditions an object needs to fulfill to be in the set, then such a set exists.

2. **Axiom schema of comprehension.** Let *P* be a statement of first-order logic in the vocabulary of naive set theory. Then the following is an axiom of naive set theory: (for some *y*, (for all *x*, ((*x* in *y*) iff *P*))).

From now on, I will refer to the set theory just described as "naive set theory." Naive set theory is much simpler and easier to understand than ZFC. Also, one can prove logical contradictions in it, and therefore one can prove anything in it. It is for this reason generally considered unsuitable as a foundation of math. If we can make naive set theory work, then we can obtain a key goal of the present defense: namely, to be able to coherent discuss the set of all sets and such objects. Our standard, to recall, is pragmatic adequacy. Can we find a pragmatically adequate resolution to the Russell paradox in the context of naive set theory?

I think so; and it's very simple. Reject proofs which end in logical contradictions or employ means that could be used to prove any statement. One can do this on an *ad hoc* basis, as one encounters them. Let us call this **the ad hoc method of rejecting contradictions**.

This method, clearly, can be generalized to any logical paradox we arrive at. Thus it solves the liar paradox as well as the Russell paradox, along with any other paradox where the problem is that it produces a logical contradiction.

This method does introduce some risk of reaching logical contradictions and falsehoods unintentionally. I think that this risk is quite tolerable in practice. More generally, I think the method I have just outlined is pragmatically adequate.

In favor of this point, I would offer the observation that theoretical physicists frequently make use of mathematics which is unrigorous to the point of actually being false or logically inconsistent from time to time. Such incorrect math can still yield correct experimental predictions with high degrees of accuracy. I have all this on the authority of my personal experience reading theoretical physics and the authority of Spencer Kwit, a person I know who is competent in theoretical physics. It is clear from this observation that math does not need to be logically consistent, or even true, to be pragmatically useful.

I conclude with substantial comfort that the ad hoc method of rejecting contradictions is pragmatically adequate for general purposes. According to this, it achieves one of the main goals of the present defense.

Here is a counter-argument. The problem with accepting a logically inconsistent system is that any such system is based on axioms that are false. It is impossible to derive falsehood from truth by the rules of classical logic, and so from the fact that contradictions are provable in naive set theory, we can infer that some of the axioms of naive set theory are false. But if you let any atom of falsehood into the system, it infects the whole system, as classical logic tells us by allowing us to infer anything from a contradiction. The consequences of the falsehood inherent in naive set theory are ultimately unpredictable. We never know how the falsehood might manifest and lead us into error in complex mathematical situations. The consequences of this for practical purposes could potentially be disastrous.

I do agree that the risk the counter-argument points to exists. I simply consider the risk adequately small for practical purposes. In practice it seems to be the case most of the time that higher math users (e.g. physicists) rely on intuitions to move from conclusion to conclusion, and that those intuitions have a degree of reliability which does not require verification by validation in a formal system. Indeed, physicists can pass through false statements on their way to conclusions that are perfectly good and reliable. Doing so may even be better in some cases, because it may simplify their work. Who are philosophers to tell physicists (or anybody, more generally) how to play the games they play to do their jobs? Such philosophical criticism of the methods of physicists could credibly come from physicist philosophers, but they would need to persuade other physicists by the standards of physicists.

To address the criticism from a less practical and more conceptual angle, I will also attack it at the premise that it is impossible to derive falsehood from truth by the rules of classical logic. In the section titled The Law of One, I have laid out a metaphysics based on the premise that all is one, from which I deduce that every statement is true (and false). If indeed every statement is true and false, then every inference proceeds from truth to falsehood. I think this is a coherent metaphysics -- more coherent, in my opinion, than others I have come across -- and that even as a thought experiment it undermines to some extent the premise that it is impossible to derive falsehood from truth by the rules of classical logic. The logical paradoxes themselves, such as the liar paradox and the Russell paradox, also of course undermine this idea. 

From the point of view of defending against the attack that I am currently defending against, the counter-argument I am currently criticizing is potentially self-defeating. The attack on classical logic that I am currently defending against advances the point of view that it is possible to derive falsehoods from truth by means of classical logic, and the Russell paradox and the liar paradox are among the examples demonstrating this phenomenon. Therefore an objection to my defense which is based on the premise that the rules of classical logic can't take us from truth to falsehood is potentially self-defeating, in the following way. If the objecter (who, as we can tell by inspecting the objection, is a believer in classical logic) were to abandon my defense on account of this objection, then they would once again be subject to the attack I am defending against. This is fine, as long as the objector has their own defense against the attack which meets their own standards.

The objector can certainly reject the idea that there is something sufficiently necessary or useful about having things like the set of all sets to justify logical innovations like the ad hoc method of rejecting contradictions. The sticking point as I see it is more with the liar paradox. Without a solution to the liar paradox, the classical logician is left without a way to synthesize their logic with natural language in a coherent way. I claim that the text of Winning Arguments, including the present defense of classical logic, and the ad hoc method of rejecting contradictions, taken all together provide a coherent way to synthesize classical logic with natural language. I'm not currently aware of a better way to do it.

Let's ask if there is a better way to do the particular task at hand right now, which is solving logical paradoxes, with a focus on the Russell paradox. For starters, let's stack up the pros and cons of the ad hoc method of rejecting contradictions.

**Pros:**

 * Easy to understand.
 * Easy to practice.
 * Adequate for general practical purposes.
 * Part of a relatively coherent synthesis of classical logic with natural language.

**Cons:**

 * Not perfect.
 * Not based on exceptionless rules that are always correct.
 * Could lead one into falsehood, perhaps with bad consequences.

Let's now survey the landscape of approaches to logical paradoxes that compete with this text's approaches, and stack up their pros and cons as well. We will do this primarily from the pragmatic perspective which is the modus operandi of this text.

All competing approaches that I'm aware of for solving logical paradoxes are based on weakening the systems of rules that give rise to the paradoxes. For example, to solve the liar paradox, one can propose weakening the rules of logic, or weakening the self-referential capabilities of language. It's not obvious to me how the latter approach can make sense for natural language, since natural language is not based on formal rules or centralized authority. The competing solutions to the liar paradox that I'm aware of all ride on weakening the rules of logic.

There are many, many ways that one can weaken the rules of logic in such a way that self-referential language is rendered coherent. For an introduction to this topic, I would recommend [An Introduction to Non-Classical Logic, by Graham Priest](https://books.google.com/books?id=Gm82ItOO9C4C), with Chapters 7 and 8 being the chapters that are relevant specifically to paradoxes of self-referential language. You can also take a look at any of [over 2,000 scholarly works on the liar paradox catalogued by PhilPapers](https://philpapers.org/browse/liar-paradox).

The real challenge here is not to find ways of weakening the rules of logic that render self-referential language coherent. The challenge is to make such a revision of the rules of logic coherent with the rest of the structure of Western academic thought. As far as I'm aware, nobody has made this work. My opinion is that nobody is going to make it work. I say this as somebody who tried to make this work and worked on it with some of the top academics working on the problem.

I did some research that tried to advance the perspective that weakening the rules of logic is not ultimately a workable and satisfactory way to resolve logical paradoxes. I did this research in the context of set theory and the Russell paradox.

I wrote [a paper](https://drive.google.com/file/d/0Bx_KuRX8hgkZc1FWX3REOFp4Mzg/edit?usp=sharing) giving a general meta-theory of logic describing and proving properties of a wide class of systems of logic, certainly not inclusive of all systems of logic that have been advanced as possible ways of solving the logical paradoxes. I wrote [a paper](https://drive.google.com/file/d/0Bx_KuRX8hgkZZjhaS0NLQnZEZzg/edit?usp=sharing) stating a mathematical conjecture to the effect that none of these systems would form a basis for naive set theory in which one could do math. This conjecture was a vast generalization of a type of result I proved in one case in [a paper](https://drive.google.com/file/d/0Bx_KuRX8hgkZb21GWm9jV2FUV1k/edit?usp=sharing) proving that a particular handful of logics did not make it possible to do math in naive set theory. I found the conjecture much too difficult to make progress on proving. Moreover, if one were to prove the conjecture, it would only rule out a certain wide class of logics, and one would be far from ruling out all logics that have been proposed in this approach to solving the logical paradoxes. I therefore gave up on the idea of furnishing mathematical proof of the claim that no way of weakening the rules of logic would make naive set theory a suitable foundation of mathematics. This is probably impossible to accomplish, anyway.

Here is the situation according to my opinion:

1. There is no way of weakening the rules of logic which would render naive set theory a suitable foundation of mathematics without invoking the ad hoc method of rejecting contradictions.
2. Statement 1 is impossible to prove.
3. Statement 2 is impossible to prove.

And so forth.

This being how I view the situation, I don't expect to be able to resolve the debate about whether weakening the rules of classical logic is a viable approach to solving the paradoxes.

One who does wish to stick to the approach of weakening the rules of logic, and who accepts statement 1, can still exclude the paradoxes of naive set theory from conisderation as ones that need to be solved. For instance, maybe their only goal is to solve the paradoxes of self referential language. I don't know how many find this way of thinking satisfying. To me, it seems like a solution to the logical paradoxes should solve all of the logical paradoxes. No approach to paradoxes based on weakening logic can accurately claim to solve all the logical paradoxes, as far as I'm aware.

One can disagree with my opinion that we should not be satisfied with a solution to logical paradoxes that only works for some logical paradoxes and not others. In general, one can disagree with my overall assessment on approaches to paradoxes based on weakening logic, in a whole variety of ways. The philosophical debate will go on. I don't claim the ability to resolve it.

For the present practical purposes, we are just looking for a resolution to the debate about logic that is satisfactory in the sense that we can be confident it is practically adequate, and that we are not likely to find solutions that are, practically speaking, better.

I have already argued that classical logic with the ad hoc method of rejecting contradictions is practically adequate. The question remains, are we unlikely to find solutions that are, practically speaking, better?

The main way that the ad hoc method of rejecting contradictions could be improved is if it were replaced with a more certain method. I think that applying the ad hoc method of rejecting contradictions in challenging situations will lead to development of generalizable principles and rules of thumb for practicing this discipline. In this sense the ad hoc method of rejecting contradictions is open to improvement and has the possibility of improvement built in. I will explore this idea further in the section titled Inconsistent Math.

One could improve the certainty of the ad hoc method of rejecting contradictions further beyond this by replacing it with a mechanical method. This is in essence an equivalent way of describing the approach of weakening the rules of logic while maintaining that the rules of logic are always correct to apply. I've already argued that one can't in this way improve upon the ad hoc method of rejecting contradictions.

For further discussion of logical paradoxes, which does not form a part of the present argument, I refer the reader to the section titled The Law of One. The material in that section assists the case of this section if one buys it, but I don't discourage the reader from leaving the idea of the Law of One alone if it doesn't resonate with them, although in my opinion it is true.

TODO: discuss relevance paradox

That completes my defense of classical logic, except for the final task of rounding up the conclusions drawn in this section on Logic.

### Conclusion

The conclusion of the section titled Logic is that an argument form is valid if and only if it is valid according to classical logic.

The argument for this conclusion was completed at the end of the previous subsection.

This conclusion is a fairly complete summary of this section, in that it is the only major conclusion drawn in the section and everything else in the section was driving towards explaining and arguing for this conclusion. Indeed, all the rest of the text up to this point can be viewed as providing context, motivation, and groundwork for this, the first major conclusion of this text. This conclusion gives us a system of logic. It is, furthermore, built on sketches of an understanding of how to apply that system of logic to natural language arguments and to real debates. At this point in the text, we have the bare bones of a coherent system of meta-philosophy. The foundations are laid.

By way of the argument for the conclusion, I have pointed out a system of rules for determining when sets of assertions and denials are incoherent, and for inferring reliably from truths to other truths. This system is called classical logic. I have defended classical logic against difficult, complex, and serious attacks from competing points of view on logic, and from intrinsic challenges created by paradoxes and by the mismatch that exists between the exactness of formal logic and the fuzziness of natural language.

Frequently, pragmatism has been an anchoring perspective. The standard for success has been adequacy for the practical purposes of winning arguments, seeking the truth, etc. I have argued at length that the rules of classical logic meet the standard set. I am quite confident in this conclusion.

My presentation of classical logic is largely equivalent to any other presentation in terms of what it provides, though the form is different from other presentations I've seen. There are, however, two differences of substance between my presentation of classical logic and those presentations which I have seen before this one.

I introduced the Law of Inference, a law which I first described to my awareness, although I would not be surprised to learn others who I haven't read have also come up with the idea.

I also introduced the ad hoc method of rejecting contradictions as a necessary part of the system for the purpose of solving logical paradoxes. This method is a pretty obvious suggestion, I think, and I am confident it has been thought and described before, although again, I first described it to my awareness.

The argument I have advanced is in essence a psychological argument. I defined an argument form as being valid iff it is valid for people in general. I defined an argument form as being valid for a given person iff the argument form has no counterexamples for them. The conclusion that an argument form is valid iff it is valid according to classical logic is a psychological conclusion which I have arrived at via philosophical methods based on armchair reflection and introspection, but also through the practical experience of observing and being involved in many kinds of debates, and learning how to win at them.

## Fallacies

Applied logic basically has two branches: the study of the rules of logic, and the study of logical fallacies. These are really opposite ways of approaching the same topic. The study of the rules of logic asks, what arguments are valid? The study of fallacies asks, basically, what arguments are invalid? An argument is valid exactly when it lacks counterexamples, and it is invalid exactly when it has counterexamples. Therefore the basic question of the study of logical fallacies has already been answered.

The subtlety left out so far is that there is a large distinction between valid arguments and winning arguments. Validity is a formal property of an argument, in the sense that it only depends on the form of the argument. Most winning arguments lack this formal property. If one's argument happens to be formally valid, and one is dealing with a receptive, reasonable audience who considers the premises of the argument to be true, then one is probably going to win the argument. Yet it is rare outside of the field of math that all of those pieces come together. Most winning arguments in most fields are not formally valid.

If we consider invalid arguments to be fallacious, then many fallacious arguments are winning. It would not be conservative to equate fallacious arguments with non-winning arguments, because whether an argument is winning is extremely context-dependent, and this understanding would not line up with the traditional understanding of fallacies. I will simply say that strictly speaking, fallacious arguments are by definition logically invalid arguments; many winning arguments are fallacious; and that's not necessarily a problem.

According to this, the study of fallacies covers ground already covered by the study of the rules of logic. Yet approaching the topic with a fresh perspective, but from the opposite angle, is also quite useful. Medieval logicians catalogued lists of categories of fallacies. The widely known terminology of fallacies -- *ad hominem* fallacy, appeal to authority fallacy, etc. -- provides easily accessible ammunition for debunking bad arguments. It is very intellectually productive to ask "what arguments are bad?" and set about categorizing them. In this spirit, I recommend [Wikipedia's list of fallacies](https://en.wikipedia.org/wiki/List_of_fallacies). 

[Inductive arguments](https://en.wikipedia.org/wiki/Inductive_reasoning), such as statistical inferences, are usually logically invalid. One can almost never infer infallibly from observations about a [sample](https://en.wikipedia.org/wiki/Sample_(statistics)) of a population to generalizations about the entire [population](https://en.wikipedia.org/wiki/Statistical_population). Statistically inferred generalizations are fallible, and strictly speaking they are fallacies.

This illustrates that pointing out that an argument is fallacious, i.e. logically invalid, does not necessarily undermine the argument. Fallacious arguments can be rationally persuasive: e.g., well-constructed statistical arguments. Establishing that an argument is fallacious establishes that there is some room for doubt about its conclusion, even assuming all its premises are true. It does not, however, establish that the argument is devoid of merit from a rational perspective. To establish that an argument is devoid of merit from a rational perspective, I think it's usually not sufficient to pattern-match it against a fallacy; usually, you also need to look at some of the particulars of the case.

## Biases

Bias is a complex and important topic in the study of psychology and rationality. In this section, we are specifically going to look at **cognitive biases**: that is, biases which influence cognition or thought.

The basic idea of cognitive biases is that cognitive biases are psychological tendencies or predilections which distort humans' views of reality. Briefly, here are some examples of types of cognitive biases:

 * The [fundamental attribution error](https://en.wikipedia.org/wiki/Fundamental_attribution_error) is a cognitive bias according to which we are more likely to explain others' behavior by postulating and looking to internal/intrinsic characteristics of others, whereas we are more likely to explain our own behavior by looking to our surrounding circumstances as opposed to our own internal/intrinsic characteristics.
* [Confirmation bias](https://en.wikipedia.org/wiki/Confirmation_bias) is a cognitive bias according to which we tend towards recalling and seeking out information which confirms our current opinions, as opposed to information which challenges our current opinions, and according to which we tend towards interpreting information in ways which confirm our opinions rather than contradicting them.
* [Self-serving bias](https://en.wikipedia.org/wiki/Self-serving_bias) is a cognitive bias according to which we tend to interpret information in ways which bolster our self esteem as opposed to threatening it.

[Wikipedia has a longer list of cognitive biases](https://en.wikipedia.org/wiki/List_of_cognitive_biases). The [Less Wrong sequences](https://wiki.lesswrong.com/wiki/Sequences) are another source covering the topic of cognitive biases, which has been educational and entertaining to me.

In this section I'm going to try to provide a general philosophical theory of cognitive biases. There is one main idea about cognitive biases which I'm going to argue for and which I think has been underappreciated in most treatments of cognitive biases I've seen. This is the idea that cognitive biases are not in themselves good or bad, but can be either good or bad (relative to some purpose(s)).

In many treatments of the topic, cognitive biases are regarded as defective by nature, as imperfections of human thought. In this point of view, we should seek to approximate being free of cognitive bias, achieving some approximation of a hypothetical state of being unbiased. The title of the blog [Overcoming Bias](http://www.overcomingbias.com/) can be read as a terse linguistic expression of this perspective on cognitive biases.

I am going to advance a substantially different perspective on cognitive biases, opposed to the perspective just described. I will argue that cogntive biases, rather than being inherently defective, are ineliminable, indispensable, and foundational to all human thought. I will argue that the notion of being cognitively unbiased or approximately unbiased, at least as applied to humans, lacks sense. I will argue that rationality is an example of a style of cognitive bias, and that as this example shows, cognitive biases can be good and laudable as opposed to defective.

The biggest source of inspiration for the explanations of this section is [the general theory of reflexivity, as explained by George Soros in his lecture at Central European University, Oct. 26 2009](https://www.youtube.com/watch?v=oCaCrWzFPYY). I highly recommend this talk; in my opinion, although George Soros is or at least appears to be a malevolent actor in some of what he does in the global game of society, his general theory of reflexivity is fundamentally true and insightful.

There is vastly more information in existence than humans are able to gather or process. This is true no matter how trivial the portion of existence you are looking at is. As a simple example, consider the lamp in my living room. It is an inexpensive, mass produced lamp which I bought at Target. I know very little about it. For some examples of what I know about it, I know what it looks like from various perspectives; I know how to turn it on and off; I know where I got it.

As I argued in the section titled Epistemology, really I don't know that I know anything; so here I am using "know" in a loose, weak, practical sense. The preceding statements of so-called knowledge should be understood as a manner of speaking, carrying an implicit qualifier regarding my lack of knowing that I know anything. Strictly speaking, these statements of so-called knowledge are statements of confident opinions about a mundane, publicly observable thing.

What don't I know (even in this loose sense) about my lamp? Certainly almost all of what there is to know about it. I don't know how much it weighs. I don't know what materials it's made of. I don't know the location of its manufacture. I don't know where the raw materials came from. I don't know the names of any of the people who were involved in gathering the materials and manufacturing the lamp. In general there is a great deal of historical information which I lack about the lamp. I also lack a great deal of information about the lamp's future. For example, I don't know at what time and location it will be destroyed or permanently disassembled, never again to function as a lamp.

I also lack most information about the lamp as it exists in the present. There are for example unimaginable numbers of facts that I could (but haven't) learned about the microscopic structure of the lamp: the positions of each individual particle in the lamp at any given instant in time, etc. According to the [Heisenberg uncertainty principle](https://en.wikipedia.org/wiki/Heisenberg_uncertainty_principle), to the extent that quantum mechanics is an accurate model of the subatomic structure and behavior of the lamp, it's not possible under the laws of physics for me to know everything about what's going on in the lamp at the subatomic level.

My information about my lamp is, at the end of the day, incredibly meager and impoverished compared to what there is to know about it. The comment generalizes to all subjects whatsoever. This severe disparity between the amount of information there is, and the amount of information I can know, is mainly a consequence of various limitations I have as an organism. My sensory organs are limited to taking in a microscopic fraction of the information that's in the universe at any given moment in time. Even with modern innovations like the Internet, it is only a microscopic fraction of existing information which I can physically access to present to my sense organs. Of the information that is physically accessible to me, I can only take in a microscopic fraction. Additionally, of the raw information that my sensory organs take in, my conscious mind can process only a tiny fraction of it. This raw sensory information is filtered through many unconscious processing stages before any of it rises into my conscious awareness. My grasp of those processes of filtering is extremely limited and impoverished.

This severe shortage of our information processing capabilities, relative to infomation in existence, is the key problem which cognitive biases address. The basic utility of cognition is to allow us form models of the world which are accurate or true to the best of our capabilities, and on that basis to make decisions which create effects that serve our purposes. Cognitive biases are tendencies of our psychology which affect the outputs of our cognition, biasing us towards processing information that is likely to be salient to us, and towards applying interpretations to the information that are likely to serve our purposes in cogitating.

Our cognitive processes take as input sensory stimulus, feedback from our cognitive processes themselves, and feedback from other parts of our psychologies (e.g. our emotions). They produce as output thoughts, beliefs, decisions, and so forth. The process of cognition is part of a feedback loop between our minds and reality, wherein our minds are influenced by the process of experiencing reality, and our minds influence reality through the medium of our actions.

The same process is characterized by feedback loops that are internal to the mind. Various examples can be given of such feedback loops. For example, people commonly experience feedback processes between their thoughts and emotions. Our emotions tend to influence our thoughts, as for example negative emotions bias us towards pessimistic thoughts and views, and positive emotions bias us towards optimistic thoughts and views. Our thoughts also tend to influence our emotions, as for example when we evaluate an emotion as reasonable/appropriate or unreasonable/inappropriate, a judgment which tends to reinforce or to discourage the emotion, respectively.

The process of sensory perception involves feedback loops as well. According to some theories of visual object recognition (TODO: which?), the process of object recognition involves feedback between early-stage sensory processing networks in the brain, and late-stage sensory processing networks. For example, suppose I'm looking at a cow. My earlier-stage sensory processing networks will present inputs to later-stage networks which are suggestive of a cow. Later-stage networks will send feedback to the earlier-stage networks giving them information about what they should expect in the visual data as represented in my brain, if in fact I am looking at a cow, and assuming nothing goes awry. This feedback will resonate with the visual data if I'm looking at a cow (and I am succeeding in perceiving what it is). That will cause the earlier-stage networks to send more data to the later-stage networks reinforcing the notion that the seen object is a cow. The probable outcome of the feedback process is that I form a conscious mental visual representation of a cow, a belief that I am looking at a cow, a short-term memory of seeing a cow, etc.

In the context of those general comments about the process of cognition, I can give a better explanation of the concept of cognitive biases. I've said that cognitive biases are tendencies of our psychologies which distort our views of reality, and that cognitive biases are tendencies of our psychology which affect the outputs of our cognition. These two statements are not obviously equivalent or consistent with each other. I square them with each other with the statement that all human views of reality are distorted.

All human views of reality are distorted because, firstly, they are based on severely limited information, and secondly, they are a result of mostly-unconscious feedback processes between our minds and reality, and of feedback processes internal to our minds.

What would it mean to have an undistorted view of reality? To explain the meaning of the phrase "undistorted view of reality," I will say that hypothetically, I would have an undistorted view of reality if I was aware of all information, if I possessed all wisdom and all insight that can be had about all information, if I could see all possible perspectives and perfectly synthesize them into the whole truth. Plainly, humans can't have an undistorted view of reality in this sense. We have to settle for possessing views of reality that are far inferior to this hypothetical ideal of an undistorted view of reality. Therefore I say that all human views of reality are distorted. 

Our cognitive processes involve many choice points, with most of these choice points being navigated more or less unconsciously. As a consumer of information who hopes to learn more about what's true by my consumption, I have the task of deciding what information to consume, with the awareness that what information I decide to consume will have an impact on what conclusions about reality I arrive at.

To give another example of how choice is involved in cognition, largely-unconscious mental processes filter my sensory data, causing me to ignore most of it most of the time, and for my attention to be drawn to things in my sensory data that are most relevant to my interests.

Similar filtering processes apply to my thoughts themselves. Largely-unconscious mental processes cause some thoughts I have to feel especially salient and interesting to me, and to draw my attention for a relatively long time, while other thoughts I have are relatively uninteresting to me and pass by my attention quickly.

Our views on reality, which are outputs of our cognitive processes, are distorted in that our cognitive processes are based on heavily filtering the data available to us, and mixing that data with layers of interpretation that we synthesize inside our minds. This is nothing that we should hope to change. This is fundamentally how our minds work.

Now let's get back to the main question: what are cognitive biases? Cognitive biases are tendencies of our psychology which affect the outputs of our cognition by selecting the influential data and how it is interpreted. Cognitive biases are those tendencies affecting the process of selecting the tiny fraction of information which gets to be cognitively salient to us, and of selecting among the innumerable possible interpretations of the information.

How does one check for the presence of cognitive biases? According to this theory, cognitive biases are always there; they are fundamental to how our minds work. One can observe the effects of cognitive biases by looking at differences in how different people evaluate the same issues. For example, one can observe biases at work in political discussions, where people tend to favor interpretations of political issues which are favored by their social in-groups, and people tend to favor interpretations of political issues which benefit them personally. One can get a good example of cognitive biases at work by looking at how news outlets with different political biases cover the same events. Depending on the political biases of a news outlet, they will select different information to present, and they will select different interpretations of the information they present, with the effect being coverage that tends to reinforce the political biases held by the news outlet.

According to this theory we can't be cognitively unbiased, and we can't even approach being cognitively unbiased. What we can do is to become aware of the process of cognitive bias, and observe and take control of our biases to some extent. A bias which we have and are aware of is a bias which is under our control to some extent. A bias which we have and are unaware of is a bias which controls us. When we are conscious of our cognitive biases, we can choose biases which in our opinion make the best of the unavoidable and unreliable process of human cognition, with its information poverty, its information selectivity, and its self-influencing tendencies.

When we are aware of the process of cognitive biasing, we have the ability to choose our biases to some extent. Practically speaking, what are good choices in this area, assuming we wish to have accurate views about things? I would say that the study of rationality is the area which studies this question. That is, the study of rationality is concerned with figuring out what cognitive biases are good and healthy for the purposes of seeking the truth and making decisions that result in winning.

The bias towards self-examination of one's cognitive biases is one example of a cognitive bias which is encouraged by modern theoretical treatises on rationality. This is a bias which encourages us to pay attention to our cognitive processes and how they are biased, to interpret our cognition as importantly influenced by our cognitive biases, and to consider possible ways our current cognitive biases could be improved upon.

Another example of a cognitive bias encouraged by modern treatises on rationality is the bias towards the application of logic to look for incoherence in one's views and for interesting logical consequences of one's views.

A further example of a bias which I consider helpful in rationality is a bias towards consideration of interpretations of information which are contrary or potentially challenging to one's current opinions.

That completes the argument of this section. I have characterized cognitive biases as tendencies which influence the outputs of our cognition and create our always-distorted views of reality, particularly by influencing the processes of selecting salient information and selecting how to interpret the information. I have argued that cognitive biases, rather than being inherently defective, are ineliminable, indispensable, and foundational to all human thought. I have argued that the notion of being cognitively unbiased, or having an undistorted view of reality, lacks sense as applied to humans. I have argued that rationality is an example of a style of cognitive bias, and that as this example shows, cognitive biases can be good and laudable as opposed to defective.

Cognitive biases are an extensively studied topic, and the theory of this section should harmonize with most of the information out there. I would encourage the reader to study cognitive biases with more than just this section. Two sources I've suggested are [the LessWrong sequences](https://wiki.lesswrong.com/wiki/Sequences) and [Wikipedia's list of cognitive biases](https://en.wikipedia.org/wiki/List_of_cognitive_biases).

To make productive use of this information, one should apply it to analyzing one's own cognitive processes and to analyzing social phenomena around oneself.

The main applications of this information which I encourage are the application of self-examination of one's cognitive biases with an aim towards improving them, and the application of understanding how social phenomena around oneself are affected by cognitive biases.

Insight about cognitive biases can also be applied in many other ways. One can use insight about cognitive biases to find more ways to disassemble views which one is biased against, and thereby use insight about cognitive biases to reinforce one's own cognitive biases. This is bad from the perspective of rationality when it amounts to using one's intellect to protect oneself from the truth.

In addition to using insight about cognitive biases for the purpose of self-deception, one can use it for the purpose of deceiving others. One way to do this is to use insight about cognitive biases to find ways to exploit others' cognitive biases for the purposes of deception.

On the flip side, insight about cognitive biases can be used to protect oneself against others' attempts to exploit one's cognitive biases to sell untrue ideas. The information needed to exploit others' cognitive biases and to defend against exploitation of cognitive biases is basically the same information. Since most people don't study cognitive biases, this symmetry might appear to create a great advantage for those looking to exploit others' cognitive biases. I think a lot of evidence suggests that such a great advantage exists. On the bright side, I think that those of us who can defend against such exploitation can help to provide a form of herd immunity by being trusted influencers of others' opinions who make a habit of calling out nonsense. The strength of the defensive side will grow if cognitive biases and the exploitation thereof become more widely appreciated.

## Uncertainty

A big foundational question in rationality is the question of how to deal with uncertainty. This is a very broad question. The best way to deal with uncertainty is going to depend on the case. The overall goal of this section is to survey the various methods of dealing with uncertainty, to develop a meta-theory of how to evaluate the suitability to a given purpose of a given method of dealing with uncertainty, and to make general recommendations as to how select a method of dealing with uncertainty that is appropriate to a given application.

The most rigorous available methods of dealing with uncertainty are the methods of statistics and probability. These methods are very complex and difficult to understand. The process of applying them to real problems is complex and highly prone to error. This is illustrated by [research](http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124) arguing, via statistical methods, that most published research findings are false. This research specifically concerns research employing statistical methods. That many experts in statistics are now making such dire pronouncements is an illustration of the extent to which application of statistical methods to real problems is error-prone. Yet, statistics and probability remain our most rigorous and theoretically dependable methods of dealing with uncertainty. 

One source where you can learn about statistics and probability is [A Modern Introduction to Probability and Statistics by Dekking et. al.](http://www.springer.com/us/book/9781852338961). Here I will discuss statistics and probability at a high level without delving into the math. My intent is that what I'm saying should make some sense regardless of your level of familiarity with statistics and probability, though the more familiar you are, the more ability you will have to evaluate the sense of what I'm saying. As I am not an expert on these topics, I would especially appreciate your feedback on this section if you are an expert on these topics.

The reliability of classical logic is demonstrated (through an admittedly circular process of reasoning) by the soundness and completeness theorems. Similarly, statistical and probabilistic methods often have the advantage that their reliability is demonstrated in some sense by mathematical theorems. A theorem demonstrating the reliability of a statistical or probabilistic method essentially states that if the method is applied to data which meets the assumptions of the method, then the conclusions about the data which the method yields will be true.

For example, a [one-sample *t*-test](https://en.wikipedia.org/wiki/Student%27s_t-test) allows one to conclude that with a certain probability, the [expected value/mean](https://en.wikipedia.org/wiki/Expected_value) of some [variable](https://en.wikipedia.org/wiki/Variable_and_attribute_(research)) over a [population](https://en.wikipedia.org/wiki/Variable_and_attribute_(research)) is not equal to a specified value. A one-sample *t*-test requires as input the values of the variable over some [representative sample](https://en.wikipedia.org/wiki/Sample_(statistics)) of the population. In addition it makes some [mathematical assumptions](https://en.wikipedia.org/wiki/Student%27s_t-test#Assumptions) which will never be exactly true in the real world.

In essence the *t*-test allows you to say that if the population mean is equal to the specified value, and the sample you are looking at is representative, then the probability of observing at least the amount of difference between the sample mean and the specified value is *p*. For example, if *p* = 0.01, then there is only a 1% chance of observing the observed difference between sample mean and population mean, assuming the population mean is equal to the specified value. When *p* is low, e.g. when *p* = 0.01, this suggestive that in fact the population mean is not equal to the specified value.

For example, suppose you prescribe a weight loss drug to 100 people, and you measure their weights at the start of the regimen and the end of the regimen, taking the difference between those starting and ending weights. Suppose you wish to reject the null hypothesis that the mean difference between start and end weights is zero. You run a one-sample *t*-test for the variable "difference between start and end weights," specifying the value zero for the mean of this variable. Your sample is your set of 100 people, and you know the value of the variable for each. Your population is all people who might hypothetically take the drug, or something like that. Suppose the *t*-test yields *p* = 0.01 of observing the given data assuming the population mean is zero. This lets you conclude that probably, the population mean is not zero.

This test does not tell you whether the population mean is probably greater than zero or probably less than zero. It tells you nothing about the value of the population mean other than that it is probably not zero. Further, it does not tell you that taking the drug caused any of the weight changes observed in the sample participants. Other statistical tests can be used to get answers to these questions.

The conclusion of the *t*-test is justified assuming that the *t*-test was carried out by correct procedures, and to the extent that the assumptions of the *t*-test are met. The assumptions of the *t*-test are essentially never met perfectly by real world data. For example, it is usually impossible (as in the case of populations of people) to gather a perfectly representative sample. 

Addressing the question of how closely the assumptions of a statistical test are met, and how problematic the inevitable deviations from the assumptions are likely to be, is in general a complex matter of opinion and guesswork. This is one of the main reasons that application of statistical and probablistic methods to real problems is so problematic and error-prone. Nevertheless, statistical and probablistic methods are often good, appropriate ways to deal with uncertainty. I don't intend to criticize statistical and probabilistic methods as bad methods, but merely to warn of the risks and challenges involved in applying them, and to encourage appropriate care and conscientiousness in this area.

Methods of statistics and probability often belong to overarching paradigms. The most important of these paradigms are the **frequentist** and **Bayesian** paradigms of statistics and probability.

Frequentist statistical and probabilistic methods are fundamentally methods of counting. In the frequentist conception of probability, the probability of an [event](https://en.wikipedia.org/wiki/Event_(probability_theory)) is essentially the size of the set of cases where the event occurs, as a portion of the set of all possible cases under consideration. 

For example, the probability of a flipped fair coin landing heads is 0.5. In the frequentist paradigm, one can arrive at this conclusion in the following way. Consider the set of possible cases under consideration to be the two-element set of the outcomes "the coin lands heads" and "the coin lands tails." The event of interest is the coin landing heads, which we can equate with the one-element set containing the outcome "the coin lands heads." The size of this set is 1. Dividing by the size of the set of all cases, we get the result 1/2 = 0.5, the (frequentist) probability of the coin landing heads.

The one-sample *t*-test is an example of a frequentist statistical method, or at least a statistical method to which a frequentist interpretation can be applied. It considers the set of all possible outcomes of the process of taking a random sample from the population of the same size as the actually available sample. Within this it measures the size of the set of such outcomes where the deviation of the sample mean from the expected population mean is at least the observed deviation. This gives the probability of observing a deviation such as the observed deviation, assuming the actual population mean equals the expected mean (or in other words assuming the null hypothesis is true). 

Whereas the frequentist paradigm is founded on the notion of probabilities produced by counting outcomes, the Bayesian paradigm is founded on the notion of **subjective probability**. A Bayesian subjective probability of an event is essentially somebody's opinion of how likely the event is to occur, expressed as a number between 0 and 1.

This subjective understanding of probability harmonizes with interesting, useful, and mathematically correct statistatical and probabilistic methods. For a friendly introduction to this area, I would recommend [the Arbital guide on Bayes' rule](https://arbital.com/p/bayes_rule/?l=1zq). 

Frequentist and Bayesian statistics and probability represent rival factions to some extent in the world of statistics and probability. From a practical perspective, though, there is no conflict as far as I'm aware. The methods of both schools are correct as far as they go. One can use the perspectives and methods of either or both schools. One can use whichever statistical and probablistic methods and paradigms are useful for one's purposes.

Let's continue this survey of methods of dealing with uncertainty by turning away from statistics and probability and considering other methods of dealing with uncertainty.

One simple way of approaching uncertainty is to reflect on and categorize the possibilities one considers likely enough to be worth considering; to weigh subjectively how likely they all seem; and to make decisions on that basis. Reflecting on and categorizing the possibilities is essentially a matter of creativity. It is a matter of expanding one's imagination to encompass the various possibilities that are not ruled out by logic, physics, and so forth. Weighing subjectively the likelihood of the possibilities and making decisions on that basis is, in general, an art of a high order. Everybody with a sufficient degree of intellectual maturity presumably does this process of reflecting on possibilities, weighing subjectively their likelihoods, and making decisions on that basis. This is also the method by which powerful, important decision makers make almost all of their decisions.

This simple way of approaching uncertainty is frequently the best or only available way. One reason for this is that time pressure and scarcity of intellectual resources usually prevents the employment of more rigorous decision making processes, such as can be developed using statistics, probability, and/or [decision theory](https://en.wikipedia.org/wiki/Decision_theory). Nor is it obvious *a priori* that more rigorous methods would yield better outcomes, as compared to this intuitive method relying on the natural intelligence of an individual.

This simple way of approaching uncertainty has the advantage that it makes good use of our natural capabilities. We are naturally good at thinking of possibilities. We are naturally good to some extent at subjectively weighing the probabilities of events. Of course in general it is a very difficult and error-prone art. Still, we are naturally good at least at easy cases; e.g., it's easy to understand that it's likely that the sun will rise tomorrow, and that it's not likely that my apartment will burn down tomorrow. We have some natural fluency with subjective probabilities, which we can employ in intuition, appreciating the nuances of the particular situation and having no need or ability to assign numbers to our subjective weighings of the probabilities.

That completes this section's survey of various methods of dealing with uncertainty. In summary, we surveyed the methods of statistics and probability, including the frequentist and Bayesian paradigms, and we considered the method of thinking of possibilities and intuitively weighing probabilities which is part of common sense. Undoubtedly these categories do not cover all methods of dealing with uncertainty which are good for something.

How do we evaluate the suitability for a given purpose of a method of dealing with uncertainty?

## Beliefs

## Persuasion

## Math

## The Law of One

This section presents my theory of how to resolve logical paradoxes. There was also a subsection titled Paradoxes of the section titled Logic. That subsection aimed to provide a pragmatic resolution to logical paradoxes. It made no pretense to provide a conceptual, intuitively appealing resolution to the paradoxes that would satisfy the metaphysical itch that is natural to humans. This section aims to provide such a conceptual, metaphysical resolution to the logical paradoxes.

The theory of this section rests on a premise that I think most people will find implausible, though I nonetheless believe it to be true. I do not intend to present this section as more than my own opinion. I believe the premise of the section because my faith tells me it is true (see the next section, titled Faith). The source from which I heard the premise is [a source officially called The Law of One and also more commonly (in my experience) called the Ra material](http://www.lawofone.info/). The much-anticipated premise is:

**The Law of One.** All is one. All objects are identically the same object.

I have provided what I consider to be two equivalent formulations of the premise, which I call the Law of One. The first formulation, "all is one," is a standard New Age trope. The second formulation is the most logically precise and the most unintuitive idea which I believe to be true and consider to be a formulation of the Law of One. I will now quote commentary on the Law of One from the Ra material, which provides much more intuitive color:

[4.20](http://www.lawofone.info/results.php?s=4#20) "The Law of One, though beyond the limitations of name, as you call vibratory sound complexes, may be approximated by stating that all things are one, that there is no polarity, no right or wrong, no disharmony, but only identity. All is one, and that one is love/light, light/love, the Infinite Creator."

[1.7](http://www.lawofone.info/results.php?s=1#7) "Consider, if you will, that the universe is infinite. This has yet to be proven or disproven, but we can assure you that there is no end to your selves, your understanding, what you would call your journey of seeking, or your perceptions of the creation.

That which is infinite cannot be many, for many-ness is a finite concept. To have infinity you must identify or define that infinity as unity; otherwise, the term does not have any referent or meaning. In an Infinite Creator there is only unity. You have seen simple examples of unity. You have seen the prism which shows all colors stemming from the sunlight. This is a simplistic example of unity.

In truth there is no right or wrong. There is no polarity for all will be, as you would say, reconciled at some point in your dance through the mind/body/spirit complex which you amuse yourself by distorting in various ways at this time. This distortion is not in any case necessary. It is chosen by each of you as an alternative to understanding the complete unity of thought which binds all things. You are not speaking of similar or somewhat like entities or things. You are every thing, every being, every emotion, every event, every situation. You are unity. You are infinity. You are love/light, light/love. You are. This is the Law of One."

How clear is it that my precise formulation of the Law of One is part of the intended meaning of the author(s) of the Ra material? My precise formulation is: all objects are identically the same object. The closest the Ra material gets to my formulation of the Law of One is at the end, when it says: "You are not speaking of similar or somewhat like entities or things. You are every thing, every being, every emotion, every event, every situation. You are unity. You are infinity. You are love/light, light/love. You are. This is the Law of One." I think it's clear that my meaning is part of the author(s)' meaning. I think my reading is a pretty straightforward, surface level reading into a meaning that is explicit. If I am every thing, then every thing is me, and therefore every thing is every other thing, or in other words all things are identically equal. You can form your own opinion on whether my textual interpretation is correct, of course.

I came to believe the Law of One through my attempts to make philosophical sense of my [mystical experiences](https://en.wikipedia.org/wiki/Scholarly_approaches_of_mysticism). In these experiences I had the sense of a higher truth being revealed to me through direct experience. The first such experience I had was on LSD, and I had the sense of seeing the world in a dimension of consciousness which I had hitherto been unaware of and unable to conceive of. I reproduced similar experiences later without LSD. Eventually this higher dimension of consciousness became a regular experience for me, and years later it became (as far as my introspection can tell me) my only state of consciousness. I can delineate very clearly in my memory the times when I have been in the lower or the higher dimension of consciousness. Of course this is all merely my subjective analysis of my memories, and in my opinion no observer of themselves can pretend to be a neutral, unbiased observer whose analyses can be counted as definitely reliable. I offer these thoughts to the reader for whatever they consider them to be worth.

In any case, my philosophical investigation was driven by my desire to understand intellectually the truth that I felt was being revealed to me. I was enamored early on with the Tao Te Ching, which I continue to regard as a great fount of wisdom. Books that exercised additional early influence on me included Ram Dass' *Be Here Now*, Robert Anton Wilson's *Cosmic Trigger*, and Aleister Crowley's [Liber ABA](http://www.sacred-texts.com/oto/lib4.htm). The first and only mystical text that resonated deeply with me as being purely true was the Law of One. This is a subjective sense which stayed with me for many years and which I have now accepted. I don't think I know that the Law of One is true. But, it's my settled opinion that the philosophical principle called the Law of One is true, and that the Ra material is probably also true. I arrived at this opinion, as I said, as an outcome of my process of reflection on my mystical experiences and my attempts to seek philosophical truths that would make sense of all my observations and experiences, and of my own conflicting principles.

Overall, the years since I began having mystical experiences have been characterized by cognitive dissonance caused by my difficulties in reconciling rationality and mysticism. I wrote [a book called *Eh na?*](https://www.amazon.com/Eh-Na-Relationship-Rationality-Mysticism-ebook/dp/B0080ID3DK/ref=sr_1_1?ie=UTF8&qid=1495335293&sr=8-1&keywords=eh+na%3F+rationality+and+mysticism) on the subject in college, which I regard as an immature effort. I have not yet gone over again the ground I went over in that book, so it remains (together with this book) the best source of information on my views on how to reconcile rationality and mysticism. One of my major personal goals in writing Winning Arguments has been to crack some tough philosophical nuts that were left uncracked in *Eh na?*, for my own satisfaction and for the enlightenment of the planet Earth. Chief among these nuts is the problem of reconciling the Law of One with logic. Let's get cracking.

The conflicts between the Law of One and logic are obvious. Suppose the Law of One is true. Then everything is everything else. Then my left arm is my left foot. Then I walk on my left arm. This contradicts the fact that I don't walk on my left arm.

In general, the Law of One entails that every statement is both true and false. There are a number of ways of arriving at this conclusion.

For one, according to the rules of classical logic, [any contradiction entails every statement](https://en.wikipedia.org/wiki/Principle_of_explosion), so that the contradiction articulated in the preceding paragraph allows us to prove every statement (including the negation of every statement).

There are many other ways to arrive at the conclusion that the Law of One entails that every statement is both true and false, relying on much more modest logical machinery. For example, the Law of One entails that the property of truth and the property of falsehood are the same property, from which it follows that if every statement is either true or false, then every statement is both true and false. Similarly, if I let **P** be any true statement, and **Q** be any statement, from the consequence of the Law of One that **Q** is identical to **P**, I can infer than **Q** is true. If I do the same trick letting **P** be any false statement, I can infer that the arbitrary statement **Q** is false (as well as true).

There are various objections to the Law of One along the lines of: it is unthinkable, it is unintuitive, etc. These objections are without justification, because I can provide counterexample. After working for many years, I have an intuitive understanding of the Law of One which renders it not only thinkable to me but even common sense to me. It is the heart of my intellectual foundations, the thing upon which logic, mathematics, and all the rest are founded. To me it is intuitive. I expect that others can obtain to the same state of understanding as mine with much greater ease via the assistance of this text. However, I would not be surprised if it took years of patient and dedicated contemplation anyway. Others' experiences learning the Law of One with my help are at this point still wholly uncharted territory for me.

From the point of view of the title of this section, my contention is that accepting the Law of One as true is helpful in resolving paradoxes in general, including logical paradoxes. I'm not sure how many would be motivated by this consideration alone to accept the Law of One, but demonstrating the helpfulness of the Law of One in resolving logical paradoxes is a primary goal of this section.

I think there are additional benefits to accepting the Law of One. I have found my humble understanding of the Law of One to be an unbelievably productive cognitive tool. My attempts to understand the Law of One and to reconcile it with logic were the germ of this book. I would say that wrestling with the unthinkable paradox at the heart of the Law of One has done more than anything else to deepen my philosophical thinking.

My understanding of the Law of One also helps me to heal my emotional pains by loving and accepting everything as the one infinite creator. I am able to do this more fully because of my intellectual understanding of how everything is one with the infinite creator.

The main of this section will be the development of a system of metaphysics founded on the Law of One. This system of metaphysics has logical contradiction built in. It is intended to be a lens for viewing existence which allows us to understand the basic logical paradox created by the Law of One: how can the Law of One be reconciled with anything we ordinarily say or think about the world, all of which is based on separations and distinctions? How can we render the Law of One thinkable, intelligible, as best as we can? Answering this question is the basic goal of the system of metaphysics I will lay out in this section.

This section follows after the Kantian metaphysical tradition in certain ways. Like Kant, I assume that there is a reality of things in themselves external to my consciousness; and, I hold that I can't observe that reality or know things about it. My goal in doing metaphysics is to understand the principles underlying the proper functioning of my consciousness, including the processes of cognition, conceptualization, and intuition. In particular, my goal is to organize my cognition and conceptualization, from which an organization of intuition naturally follows. And, my goal is to build this organization around true principles as revealed to me by reasoning, intuition, and faith.

This metaphysics is based on one premise: the Law of One. This is the only premise we need assume, because it entails that every statement is true. From this point to the end of this section, our norms of discourse have changed completely compared to the rest of the book. Having made an assumption which entails every conclusion, there is no longer any sense in trying to prove anything or to construct any arguments. The goal now is simply to *describe* how things are given the Law of One, in a way that is intended to be as understandable as possible.

The first notion is the notion of **object**. We have been using this notion all along. The best explanation of the notion of object which we have gotten so far was the explanation provided by [model theory](https://github.com/sharedassumptions/winning-arguments/blob/master/winning-arguments.md#model-theory). Model theory takes objects as a given, and it does not inquire into their internal characteristics, but treats them merely as blank objects, different only in that they are distinct, and in what predicates are true of them and what relationships are true of them (or in other words what pairs in the interpretation of a copula they belong to).

Everything is an object. Among the objects there are all the objects we are familiar with from everyday life. There are sets and all other mathematical objects. There are also many objects we are unfamiliar with. Every conceivable object exists. All possible worlds exist.

There are many ways of slicing the world up into objects. For example, I regard the keyboard I am typing on as an object. I can also look at it as an assembly of objects: the keys, the case, etc. I can also consider it as two objects: the top half of my keyboard, and the bottom half of my keyboard. Or the left half and the right half of my keyboard. I can count the air inside the keyboard case as part of the keyboard; or I can exclude the air. I can count or I can exclude the empty space inside the keyboard case as part of the keyboard. I might even count the empty space as part of the keyboard without counting the air as part of the keyboard; one possible rationale would be that the empty space is a necessary part of the functional mechanisms of the keyboard, whereas the air is not.

I can consider myself and my keyboard as being a single object. I can consider my keyboard and all other components and accessories of my computer as being a single object. I can consider all of them as merely being a part of the planet Earth, or a part of the United States, or a part of my apartment.

I can consider my keyboard and the Dalai Lama as together forming exactly one object. I can call such an object "the mereological sum of my keyboard and the Dalai Lama." For any set of objects, the mereological sum of those objects exists. The merological sum of the set of all objects is a single object which is everything. This we can call The Whole.

All the objects mentioned so far are concrete objects. An object *x* is **concrete** iff *x* has a part *y* such that *y* is part of the physical world. In particular, since (by terminological stipulation) every object *x* is a part of itself, if *x* is part of the physical world then *x* is concrete. All objects mentioned so far are concrete.

An **abstract** object is any object which is not concrete. Tautologically, every object is either concrete or abstract. Some types of abstract objects are: sets, predicates, numbers, and any kind of mathematical object.

So far the following classifications of objects have been introduced. We have physical objects, i.e. parts of the physical world. We have concrete objects, which by definition are any objects with physical parts. And we have abstract objects, which by definition are non-concrete objects. Thus all objects are either abstract or concrete. The Whole is concrete, since it has physical parts. By The Law of One, the One is both abstract and concrete: its nature is therefore paradoxically both physical and non-physical, and the same comment may be extended to all objects.

A print edition of a book, for example, is a concrete object which has both physical and abstract parts. A print edition of a book can be thought of as the mereological sum of the copies that form parts of the print edition. One can also subdivide the print edition of the book into the chapters, sections, or pages of the edition.

If one considers a single page of the print edition, one can regard it as the mereological sum of all the copies of that page in all the copies of the edition. Or, on the other hand, one can consider it as the sequence of all words or all letters and spaces on the page.

A letter can be considered as an abstract object, which is instantiated by every physical phenomenon which represents that letter. Or, one can consider it as the mereological sum of all instantiations of the letter: either all instantiations in the current world or all instantiations in all possible worlds.

* If one equates a page of a print edition with the sequence of letters and spaces on the page, and one considers the letters as abstract objects, then from that point of view the page is an abstract object.
* On the other hand, if one equates a page of a print edition with the mereological sum of the copies of the page, then from that point of view the page is a concerete object.
* Another point of view on the page is to regard the page as the mereological sum of the sequence of abstract-object letters and spaces in the page, and the physical copies of the page. This perspective unites the preceding two perspectives, and regards the page as a concrete object with abstract parts.
* Another point of view on the page is to regard the page as the mereological sum of all objects with which the page can reasonably be equated. This is a (formally, theoretically) all-encompassing point of view on the page.

So far we have worked within a division of objects into physical objects, abstract objects (which lack physical parts), and objects which have both physical and abstract parts. The Law of One entails that physical objects and abstract objects are of the same nature: physical objects are abstract, and abstract objects are physical. More generally, the Law of One entails that there is only one type of substance. Consciousness and physical material are of the same nature. Consciousness is physical material, and physical material is consciousness.

I'll add some more meat to the concepts that have been laid out so far. The most important metaphysical concepts invoked so far are "object," "part," and "set." Everything is an object, and objects can be divided into parts, which are also objects. Objects can be assembled into sets, and one can produce new objects by the "mereological sum" operation applied to a set. The mereological sum operation, applied to a set, produces the smallest possible object of which all elements of the set are parts.

There is no purpose I can yet see in offering a rigorous axiomatization of the mereology that follows from the Law of One. I think that an intuitive understanding of the metaphysics of parthood according to the Law of One is very instructive for understanding the Law of One. I will not offer any axioms for mereology, or attempt to prove any results. I will merely attempt to illustrate true and general intuitions regarding the metaphysics of parthood under the Law of One.

Everything that exists is an object. Every object is part of the one infinite creator, because the one infinite creator is The Whole, the mereological sum of all objects. Every object is a slice of existence taken out of the one infinite creator. At the same time, the one infinite creator is contained at the bottom of everything; it is the atom.

These conclusions can be reached in intuition in many ways. I encourage the reader to try, as an exercise, reaching these conclusions in intuition, and to try with patience and creativity.

These conclusions can also be reached by many different arguments, some of which are instructive. It is merely true by definition that every object is part of The Whole, so this is not interesting to argue for. What is more interesting to argue for is the conclusion that the one infinite creator is the atom at the bottom of everything. Such arguments are not needed for justificatory purposes in the present context. However, they are useful for illustration purposes.

TODO

## Faith
